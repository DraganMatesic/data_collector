[← Back to index](1.%20index.md)

# Data Collector Framework — Product Overview

## What is Data Collector?

Data Collector is an enterprise-grade Python framework for building, orchestrating, and monitoring data ingestion pipelines. It provides a unified architecture for collecting data from web sources, REST/SOAP APIs, files (PDF, Excel, CSV), and OCR pipelines — then transforming and loading it into relational databases.

**The problem it solves:** Organizations that collect data from dozens of external sources end up with fragmented, unmaintainable code — different patterns per source, no centralized logging, no dependency tracking, and no visibility into what's running. Data Collector eliminates this by providing a single framework with standardized patterns for every stage of the pipeline.

## Product Vision

**Long-term vision:** A self-contained platform where organizations install Data Collector, define their data sources, build apps using framework patterns, and get orchestration, logging, monitoring, and dependency tracking out of the box — without stitching together Airflow, custom scripts, and monitoring tools.

### Design Philosophy

- **Convention over configuration** — Sensible defaults for database connections, logging, hashing, and table deployment. Override only what you need.
- **Database-agnostic** — Write once, run on PostgreSQL or MSSQL. The framework abstracts connection strings, auto-increment columns, schema resolution, and driver quirks. The connector architecture is pluggable — additional databases can be added by implementing the `BaseDBConnector` interface.
- **SHA-based change detection** — Every data record gets a deterministic hash. The `merge()` pattern compares hashes to detect inserts, updates, and removals without full row scans.
- **Observability-first** — Every app execution is tracked (runtime, exceptions, exit codes). Every database object touched is mapped. Logs flow to DB and Splunk via async queues.
- **Fail-safe defaults** — Logging handlers catch errors so apps never crash from a logging failure. When a sink (database, Splunk) fails, the error and original log record are written to a local fallback file, ensuring no log data is ever lost. Archive by default (not delete) so data history is preserved.
- **Cross-platform** — Developed and tested primarily on Windows, but fully deployable on Linux including Docker containers and cloud environments.

### Core Principles

1. **Modularity** — Each capability (scraping, OCR, captcha solving, proxy management, notifications) is a separate, pluggable module. Use only what you need.
2. **Auditability** — Full audit trail: what ran, when, what it touched, what changed, what failed.
3. **Resilience** — Queue-decoupled logging, retry patterns, graceful degradation, process isolation.
4. **Developer productivity** — Build a new data collection app in hours, not days. Framework handles sessions, hashing, logging, and deployment.

## Target Audience

| Audience | What they need |
|----------|---------------|
| **Internal developers** | API reference, getting-started guide, app patterns, code examples |
| **Enterprise clients** | Product overview, architecture docs, feature catalog, licensing terms |
| **DevOps / Infrastructure** | Deployment guide, Docker support, monitoring setup, configuration reference |
| **Contributors** | Contributing guide, architecture decisions, code standards |

## Feature Scope

### Implemented (v0.1.0)

| Feature | Description |
|---------|-------------|
| Multi-database support | PostgreSQL, MSSQL via unified `Database` class (pluggable connector pattern) |
| SHA-based merge | Deterministic change detection with `make_hash()` and `obj_diff()` |
| ORM table management | SQLAlchemy 2.x models with cross-database auto-increment, naming conventions |
| App lifecycle tables | `Apps`, `AppGroups`, `AppParents` with scheduling, commands, fatal error tracking |
| Database object tracking | Automatic mapping of tables/views/procedures used per app via `AppDbObjects` |
| Async logging | Queue-based logging with `DatabaseHandler`, `SplunkHECHandler`, `RouterHandler` |
| Settings management | Pydantic-based configuration with environment variable aliases |
| Secret management | AES-256-CBC encrypted secrets with PBKDF2 key derivation |
| Table deployment | `Deploy` class for schema creation, seeding codebook data |
| Runtime tracking | Per-execution metrics: duration, exception count, exit codes |

### In Development

| Feature | Description |
|---------|-------------|
| App orchestration manager | Central scheduler, command dispatch, process lifecycle management |
| Web scraping engine | Centralized `Request` class for all HTTP operations, base scraper, session management, retry logic |
| Captcha solving | AntiCaptcha API integration for automated CAPTCHA resolution |
| SOAP/XML integration | WSDL-based client for government/enterprise web services |
| PDF & OCR processing | Text extraction (pdfplumber), image OCR (Tesseract, PaddleOCR) via adapter pattern |
| Proxy management | Proxy pool, rotation strategies, ban detection — integrated with Request class |
| Threading & concurrency | ThreadPoolExecutor for parallel processing, async HTTP via httpx for concurrent I/O |
| RabbitMQ messaging | Distributed command dispatch and task routing via message queues |
| Notification system | Pluggable alert channels — Telegram, Microsoft Teams, Slack, email, webhooks |
| Data quality & auditing | Change tracking, validation rules, quality metrics |
| Storage management | File retention, directory organization, download management |
| Function profiler | `@fun_watch` decorator for per-function timing, stored in `FunctionLog` table |

### Planned

| Feature | Description |
|---------|-------------|
| Deep Learning / NER | Named Entity Recognition for extracting structured data from OCR text and HTML |
| Document processing pipelines | Infrastructure for building OCR/NER pipelines: base worker class, hot folder entry points, RabbitMQ topic routing, state tracking. Developers build specific workers per document type. |
| Task queue (Dramatiq) | Long-running task management via Dramatiq + RabbitMQ for OCR, NER, and batch processing |
| Event-driven architecture | Application event bus, lifecycle events, distributed event processing |
| REST API & dashboard | FastAPI-based management API with real-time WebSocket monitoring |

### Out of Scope

This section defines what the framework intentionally does **not** try to be, to keep the product focused and prevent scope creep.

- **Not a general-purpose workflow engine** — Data Collector is purpose-built for data ingestion pipelines. For generic workflow orchestration (DAGs, complex dependencies), use Airflow or Prefect.
- **Not a BI/analytics tool** — Data Collector collects, transforms, and loads data. Visualization, reporting, and analytics are downstream concerns.
- **Not a web application framework** — The REST API is strictly for framework management and monitoring, not for building user-facing web applications.

## Licensing

Data Collector is released under a **proprietary license**:

- **Evaluation license**: Free time-limited trial for evaluation purposes
- **Commercial license**: Required for all production use, client work, freelancing, internal company use, or any use that directly or indirectly generates revenue
- **Personal learning**: Permitted for individual educational use only (not for building deliverables for others)

> All dependencies use permissive licenses (MIT, BSD, Apache 2.0) or LGPL. A `THIRD-PARTY-NOTICES` file is included listing all dependency licenses.

See [LICENSE.txt](../LICENSE.txt) for full terms.

## Background

Data Collector is built from the ground up as an enterprise-grade framework, drawing on over 10 years of professional experience in data engineering — building and operating large-scale data collection systems, ETL pipelines, and integration platforms across multiple European markets.

The framework is designed to solve real problems encountered in production environments: managing dozens of concurrent data sources, ensuring reliable change detection, maintaining audit trails, handling failures gracefully, and providing complete operational visibility.

| Period | Milestone |
|--------|-----------|
| 2025 | Framework development started. Architecture designed based on years of enterprise data engineering experience. |
| Current | v0.1.0 — Core framework (database, logging, settings, tables, deployment). Active development. |

## Technology Stack

| Layer | Technology |
|-------|-----------|
| Language | Python 3.13+ |
| ORM | SQLAlchemy 2.x |
| Configuration | Pydantic / pydantic-settings |
| Databases | PostgreSQL (psycopg2), MSSQL (pyodbc) |
| HTTP | httpx (sync + async), BeautifulSoup4, lxml, html5lib |
| Encryption | cryptography (AES-256-CBC, PBKDF2) |
| Data processing | pandas, python-dateutil |
| Messaging / Task queue | pika (RabbitMQ), Dramatiq |
| PDF / OCR | pdfplumber, pytesseract, PaddleOCR (optional) |
| Notifications | Telegram Bot API, MS Teams webhooks, Slack API |
| Logging | structlog (structured logging with context binding) |
| Profiling | `@fun_watch` decorator (function-level timing) |
| Build | setuptools, wheel |
