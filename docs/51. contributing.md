[← Back to index](1.%20index.md)

# Contributing Guide

## Overview

Guidelines for contributing to the Data Collector framework. This covers code standards, git workflow, testing requirements, and architectural rules.

## Documentation Governance

Documentation quality is mandatory and is enforced through local checks and CI.

### Mandatory rules

- Use formal technical language
- Do not use emojis or decorative symbols
- Use only these documentation status values: `IMPLEMENTED`, `IN DEVELOPMENT`, `PLANNED`
- If a referenced path does not exist yet, use `Planned module path: data_collector/...` or `Planned file path: data_collector/...` / `.github/...`
- App modules must be documented under `data_collector/<country>/<parent>/<app>/` and executed as `python -m data_collector.<country>.<parent>.<app>.main`
- Country namespaces must be lowercase slugs and cannot use reserved names: `settings`, `utilities`, `tables`, `enums`, `pipeline`, `orchestration`, `api`, `notifications`, `constants`
- Do not skip heading levels (for example `##` directly to `####`)
- Ensure links, anchors, and local asset references resolve correctly

### Required checks for documentation changes

- Run local validation before opening a PR:

```bash
python data_collector/utilities/validate_docs.py
```

- Documentation changes must pass the `docs-quality` CI workflow

## Development Setup

### Prerequisites

- Python 3.13+
- PostgreSQL 14+ (for integration tests)
- Git
- **Platform**: Developed and tested primarily on Windows, but fully deployable on Linux including Docker containers and cloud environments. CI runs on Linux (GitHub Actions).

### Environment Setup

```bash
# Clone the repository
git clone https://github.com/DraganMatesic/data_collector.git
cd data_collector

# Create virtual environment
python -m venv .venv
source .venv/bin/activate      # Linux/macOS
.venv\Scripts\activate         # Windows

# Install with dev dependencies
pip install -e ".[dev]"

# Configure database for tests
export DC_DB_MAIN_USERNAME=postgres
export DC_DB_MAIN_PASSWORD=postgres
export DC_DB_MAIN_DATABASENAME=data_collector_test
export DC_DB_MAIN_IP=127.0.0.1
export DC_DB_MAIN_PORT=5432
```

## Code Standards

### Style

- **Formatter:** Ruff (replaces Black + isort)
- **Linter:** Ruff
- **Type checker:** mypy (strict mode)
- **Line length:** 120 characters
- **Quotes:** Double quotes for strings

```toml
# pyproject.toml
[tool.ruff]
line-length = 120
target-version = "py313"

[tool.ruff.lint]
select = ["E", "F", "I", "UP", "B", "SIM"]

[tool.mypy]
strict = true
```

### Naming Conventions

| Element | Convention | Example |
|---------|-----------|---------|
| Modules | `snake_case` | `log_settings.py` |
| Classes | `PascalCase` | `DatabaseSettings` |
| Functions | `snake_case` | `make_hash()` |
| Constants | `UPPER_SNAKE` | `NAMING_CONVENTION` |
| ORM tables | `PascalCase` class, `snake_case` `__tablename__` | `class Apps`, `__tablename__ = "apps"` |
| Database columns | `snake_case` | `next_run`, `date_created` |
| Enum members | `UPPER_SNAKE` | `RunStatus.RUNNING` |

### Import Order

```python
# 1. Standard library
import os
from pathlib import Path

# 2. Third-party
from sqlalchemy import Column, String
from pydantic_settings import BaseSettings

# 3. Framework (data_collector)
from data_collector.settings.main import DatabaseSettings
from data_collector.utilities.database.main import Database
```

### Docstrings

Use Google-style docstrings for public APIs:

```python
def make_hash(record: dict, columns: list[str]) -> str:
    """Generate SHA-256 hash from selected column values.

    Args:
        record: Dictionary of column name → value pairs.
        columns: Column names to include in the hash.

    Returns:
        Hexadecimal SHA-256 hash string.
    """
```

## Git Workflow

### Branch Naming

```
feat/short-description     # New features
fix/short-description      # Bug fixes
docs/short-description     # Documentation changes
refactor/short-description # Code restructuring
test/short-description     # Test additions
```

### Commit Messages

Follow [Conventional Commits](https://www.conventionalcommits.org/):

```
feat: add proxy rotation support
fix: handle null values in make_hash
docs: add orchestration specification
refactor: extract connector factory from Database class
test: add merge() edge case tests
chore: update dependencies
```

### Pull Request Process

1. Create a feature branch from `master`
2. Make changes with clear, focused commits
3. Ensure mandatory tests pass: `pytest tests/unit tests/quality`
4. Ensure linting passes: `ruff check data_collector/`
5. Open PR with description following the template
6. Address review feedback
7. Squash-merge into `master`
8. For docs changes, run `python data_collector/utilities/validate_docs.py` before opening the PR
9. Run optional integration suite when PR changes database/infrastructure logic: `pytest tests/integration -m integration`

### PR Template

```markdown
## Summary
Brief description of what this PR does.

## Changes
- Bullet list of specific changes

## Testing
- How was this tested?
- Any new tests added?

## Related
- Links to related issues or docs
```

## Testing

### Test Structure

```
tests/
├── unit/                       # Mandatory deterministic unit tests
│   └── functions/
│       ├── test_runtime.py
│       ├── test_converters.py
│       └── test_math.py
├── quality/                    # Mandatory quality contract tests
│   └── test_validate_docs.py
├── integration/                # Optional infrastructure-backed tests
│   └── database/
│       └── test_database_smoke.py
└── conftest.py                 # Shared bootstrap and fixtures
```

### Running Tests

```bash
# Mandatory local gate (same contract as required CI gate `tests-unit`)
pytest tests/unit tests/quality --cov=data_collector --cov-report=term-missing --cov-report=xml

# Optional infrastructure tests
pytest tests/integration -m integration
```

### CI Gates

- Required: `tests-unit` (unit + quality tests)
- Required: `docs-quality` (documentation validator)
- Optional/manual: `tests-integration` (`workflow_dispatch`)

### Documentation Validation

For any documentation change (`docs/*.md` or `README.md`), run:

```bash
python data_collector/utilities/validate_docs.py
```

The command must pass locally before submitting a pull request.

### Test Guidelines

1. **Unit tests** should be fast, isolated, and require no external services
2. **Quality tests** enforce governance contracts (for example validator behavior)
3. **Integration tests** are optional and may skip when infrastructure is unavailable
4. Use `pytest` fixtures for setup/teardown and environment bootstrap
5. Name tests descriptively: `test_make_hash_handles_none_values`
6. Test edge cases: empty inputs, null values, Unicode, large datasets
7. Coverage is currently report-only (no fail threshold in this pass)

## Architecture Rules

These rules preserve framework integrity. All contributions must follow them.

### 1. ORM Models Extend `Base`

All table models must inherit from `Base` (which combines `DeclarativeBase` + `BaseModel`):

```python
from data_collector.tables.shared import Base

class MyTable(Base):
    __tablename__ = "my_table"
    # ...
```

### 2. SHA Hashing for Change Detection

Use `SHAHashableMixin` or `bulk_hash()` for all data that goes through `merge()`:

```python
class MyTable(Base, SHAHashableMixin):
    __hash_columns__ = ["name", "value", "source"]
```

### 3. Settings via Pydantic

All configuration must go through Pydantic settings classes — never hardcode connection strings, paths, or credentials:

```python
from pydantic_settings import BaseSettings

class MyFeatureSettings(BaseSettings):
    model_config = SettingsConfigDict(env_prefix="DC_MY_FEATURE_")
    enabled: bool = True
```

### 4. Logging via Framework Logger

Use the framework's `LoggingService` — never use `print()` or bare `logging.getLogger()`:

```python
logger = logging_service.configure_logger()
logger.info("Processing complete", record_count=42)
```

### 5. No Direct SQL in Application Code

Use SQLAlchemy ORM and the `Database` class methods. Raw SQL is acceptable only in `Database` internals or migration scripts.

### 6. Feature Modules Are Optional

New features must not break the core if their dependencies are missing. Use `is_module_available()` for optional imports:

```python
from data_collector.utilities.functions.runtime import is_module_available

if is_module_available("pika"):
    from pika import BlockingConnection
```

## Release Process

1. Update version in `pyproject.toml`
2. Update [52. changelog.md](52.%20changelog.md) with release notes
3. Create PR: `release/vX.Y.Z`
4. After merge, tag: `git tag vX.Y.Z`
5. Push tag: `git push origin vX.Y.Z`
6. CI builds and publishes to PyPI

## Questions?

Open an issue on GitHub or reach out to the maintainer.
