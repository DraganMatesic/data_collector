[← Back to index](1.%20index.md)

# Web Scraping Engine

**Implementation Status:** IN DEVELOPMENT | **Work Package:** WP-04 | **Milestone:** v0.3.0

## Overview

The scraping engine provides standardized patterns for building web scraping and API integration applications. It abstracts HTTP communication, session management, retry logic, error handling, progress monitoring, and rate limiting so developers focus on data extraction.

Built from the ground up based on enterprise experience with 40+ scrapers across government, financial, and legal data sources in 5 countries.

Developed and tested primarily on Windows, but fully deployable on Linux.

All HTTP operations use the centralized `Request` class — see [4.4. request.md](4.4.%20request.md) for initialization, session management, error handling, retry strategy, and response helpers.

## App Structure

Every scraper application follows a standard 3-file structure:

```
data_collector/{group}/{parent}/{app_name}/
├── main.py        # App class + init() entry point
├── parser.py      # Parser class (BeautifulSoup, regex, XML)
└── tables.py      # SQLAlchemy ORM models
```

| File | Responsibility |
|------|---------------|
| `main.py` | Contains the app class (extends `BaseScraper`) and a module-level `init(runtime)` function as the entry point. The class holds all collection logic. |
| `parser.py` | `Parser` class — takes HTML/JSON/XML content, returns dicts or ORM objects. Stores intermediate data (tokens, lookup tables) as instance attributes. Supports inheritance for NER/DL parsing extensions. |
| `tables.py` | ORM models with standard columns (`id`, `sha`, `archive`, `date_created`, `date_modified`). One schema per app or shared per parent. |

Some apps have additional files (e.g., `soap_client.py`, `constants.py`) but `main.py`, `parser.py`, and `tables.py` form the core split.

## App Scaffold

The framework provides a CLI scaffold command to generate new apps with the correct structure, boilerplate, and convention:

```bash
python -m data_collector scaffold \
    --group country \
    --parent financials \
    --name company_data \
    --type single          # or "threaded"
```

### What Gets Generated

```
data_collector/country/financials/company_data/
├── __init__.py    (empty)
├── main.py        (BaseScraper subclass + init() with argparse, get_app_info, lifecycle)
├── parser.py      (skeleton Parser class with example methods)
└── tables.py      (skeleton ORM model with standard columns)
```

The scaffold also **registers the app in the Apps table** — computes `app_id = sha256(group|parent|name)` and inserts a row with `run_status=NOT_RUNNING`.

### Template Types

| Type | Flag | Description |
|------|------|-------------|
| **Single-threaded** | `--type single` (default) | `prepare_list()`, `collect()`, `store()`, `set_next_run()` — sequential processing |
| **Multi-threaded** | `--type threaded` | Same + `ThreadPoolExecutor`, per-thread `Request`, shared `RequestMetrics`, `collect_detail()` worker |

### Example Output

```
$ python -m data_collector scaffold --group country --parent financials --name company_data

Created app structure:
  data_collector/country/financials/company_data/
  ├── __init__.py
  ├── main.py      (single-threaded BaseScraper)
  ├── parser.py
  └── tables.py

App registered in database:
  app_id:  a3f8c2...  (sha256 of country|financials|company_data)
  status:  NOT_RUNNING

Next steps:
  1. Edit tables.py — define your ORM models
  2. Edit parser.py — implement Parser class methods
  3. Edit main.py — set base_url, implement collect() logic
  4. Run: python -m data_collector.country.financials.company_data.main
```

Generated files contain `# TODO` markers at every customization point — `base_url`, table references, parsing logic, scheduling interval. All framework boilerplate (`init()`, `argparse`, `get_app_info`, `update_app_status`, `fatal_check`, `log_stats`) is pre-wired correctly.

## BaseScraper Architecture

BaseScraper provides **convention-based lifecycle hooks** — not a chaining or template framework. Every scraper's HTTP flow is unique (CSRF forms, SOAP envelopes, API pagination, authenticated sessions, CAPTCHA), so multi-step logic stays in the app's `collect()` method. BaseScraper gives infrastructure; apps implement business logic.

```
┌─────────────────────────────────────────────────────────┐
│                      BaseScraper                         │
│                                                           │
│  Provides:                                                │
│  ├── self.base_url     → Target site root URL (str)       │
│  ├── self.request      → Request instance                 │
│  ├── self.metrics      → RequestMetrics (shared)          │
│  ├── self.proxy_data   → ProxyData configuration          │
│  ├── self.args         → Runtime arguments (dict | None)  │
│  ├── self.database     → Database instance                │
│  ├── self.logger       → Bound structlog logger           │
│  ├── Error tracking    → fatal_flag, fatal_check()        │
│  ├── Progress          → solved, failed, list_size        │
│  └── Scheduling        → next_run, set_next_run()         │
│                                                           │
│  Lifecycle hooks (override what you need):                │
│  ├── prepare_list()    → Query DB for items to process    │
│  ├── collect()         → Main collection logic (HTTP)     │
│  ├── store()           → merge() / bulk_insert()          │
│  └── cleanup()         → Logout, release proxies          │
└─────────────────────────────────────────────────────────┘
```

### Lifecycle Flow

```
prepare_list()  → Query DB for items (registry numbers, IDs, dates)
       │
       ▼
  self.work_list = [ORM objects or dicts]
       │
       ▼
collect()       → For each item: HTTP requests → parse → store
       │            (multi-step logic lives here)
       ▼
fatal_check()   → Evaluate error ratios, trigger alerts if needed
       │
       ▼
set_next_run()  → Schedule next execution based on results
       │
       ▼
cleanup()       → Logout, release proxies, close sessions
```

### What BaseScraper Provides

| Feature | Details |
|---------|---------|
| **Base URL** | `self.base_url` — target site root URL including protocol (e.g., `https://api.example.com`) |
| **HTTP client** | `self.request` — `Request` instance with transport config (timeout, retries, backoff) |
| **Shared metrics** | `self.metrics` — `RequestMetrics` instance aggregating data across threads |
| **Proxy config** | `self.proxy_data` — `ProxyData` dataclass with proxy pool configuration |
| **Arguments** | `self.args` — optional `dict` of runtime arguments for targeted collection, monitoring, date overrides |
| **Database** | `self.database` — `Database` instance for queries and merges |
| **Logging** | `self.logger` — structlog `BoundLogger` with `app_id`, `runtime` bound |
| **Error tracking** | `fatal_flag`, `fatal_msg`, `fatal_time`, `fatal_check()` — evaluates error ratios |
| **Progress** | `solved`, `failed`, `list_size`, `progress` — updates `Apps` table during execution |
| **Scheduling** | `next_run`, `set_next_run()` — dynamic scheduling based on runtime conditions |
| **Alert threshold** | `alert_threshold` (default 20%) — triggers fatal when error % exceeds threshold |

### Key Design Decisions

**Why convention-based, not chaining/templates:**
- Every scraper has a unique HTTP flow — chain abstractions end up as complex as raw code when every step is different
- The base class provides infrastructure (error tracking, progress, proxy), not HTTP orchestration
- Apps implement business logic in `collect()` — that's where the HTTP steps live

**Why `prepare_list()` returns DB items, not URLs:**
- In production, scrapers process lists of registry numbers, company IDs, dates, or case numbers fetched from the database
- URLs are built inside `collect()` because URL construction often depends on runtime state (tokens, session IDs, dynamic parameters)

## Examples

### Simple API Scraper

```python
# Planned file path: data_collector/country/financials/company_data/main.py
from data_collector.scraping import BaseScraper
from data_collector.utilities.hashing import bulk_hash
from data_collector.country.financials.company_data.parser import Parser
from data_collector.country.financials.company_data.tables import Companies

class CompanyApi(BaseScraper):
    """Fetch company details from the business registry API."""

    base_url = "https://api.registry.example.com"

    def __init__(self, database, **kwargs):
        super().__init__(database, **kwargs)
        self.parser = Parser()

    def prepare_list(self):
        """Fetch registry numbers from DB that need updating."""
        with self.database.create_session() as session:
            self.work_list = self.database.query(
                session, Companies,
                filters=and_(
                    Companies.archive == None,
                    Companies.date_checked < self.threshold
                )
            )

    def collect(self):
        """Fetch company details from API."""
        for company in self.work_list:
            response = self.request.get(
                f"{self.base_url}/company/{company.reg_number}"
            )

            if self.request.should_abort(self.logger, proxy_on=False):
                break

            data = self.parser.parse_company(response.json())
            self.store([data])
            self.solved += 1

    def store(self, records):
        bulk_hash(records)
        with self.database.create_session() as session:
            self.database.merge(
                records, session,
                filters=and_(Companies.archive == None)
            )
```

### Multi-Step Authenticated Scraper

Real scrapers often require multiple sequential HTTP steps — login, CSRF token extraction, session dismissal, search, pagination, logout. All of this lives in `collect()`:

```python
# Planned file path: data_collector/country/registries/registry_records/main.py
from data_collector.scraping import BaseScraper
from data_collector.utilities.hashing import bulk_hash
from data_collector.country.registries.registry_records.parser import Parser
from data_collector.country.registries.registry_records.tables import RegistryRecord, FetchQueue

class RegistryPortal(BaseScraper):
    """Multi-step authenticated scraper with CSRF protection."""

    base_url = "https://portal.registry.example.com"

    def __init__(self, database, **kwargs):
        super().__init__(database, **kwargs)
        self.parser = Parser()
        self.login_url = f"{self.base_url}/account/login"
        self.dismiss_url = f"{self.base_url}/account/dismiss"
        self.search_url = f"{self.base_url}/search"
        self.query_url = f"{self.base_url}/api/query"
        self.logout_url = f"{self.base_url}/account/logout"
        self.request.set_headers({
            "User-Agent": "Mozilla/5.0 ...",
            "Accept": "text/html,application/xhtml+xml"
        })

    def prepare_list(self):
        """Fetch unprocessed dates from database."""
        with self.database.create_session() as session:
            self.work_list = self.database.query(
                session, FetchQueue,
                filters=and_(FetchQueue.processed == None)
            )

    def collect(self):
        """Multi-step authenticated collection flow."""
        for item in self.work_list:
            # Step 1: Get login page, extract CSRF token
            response = self.request.get(self.base_url)
            token = self.parser.get_request_token(response.content)

            # Step 2: POST login with credentials + CSRF
            response = self.request.post(self.login_url, data={
                "Username": self.username,
                "Password": self.password,
                "__RequestVerificationToken": token
            })

            # Step 3: Handle existing session (dismiss if active)
            if self.parser.check_signin(response.content):
                token = self.parser.get_request_token(response.content)
                guid = self.parser.get_guid(response.content)
                self.request.post(self.dismiss_url, data={
                    "guid": guid,
                    "__RequestVerificationToken": token
                })

            # Step 4: Navigate to search, extract report ID
            response = self.request.get(self.search_url)
            report_id = self.parser.parse_report_id(response.content)

            # Step 5: POST search filters, get results with pagination
            page = 1
            while True:
                response = self.request.post(self.query_url, json={
                    "report_id": report_id,
                    "filters": {"date": item.target_date},
                    "page": page
                })
                records = self.parser.parse_results(response.json())
                if not records:
                    break

                self.store(records)
                self.solved += len(records)
                page += 1

            # Step 6: Logout
            self.request.get(self.logout_url)

    def store(self, records):
        bulk_hash(records)
        with self.database.create_session() as session:
            self.database.merge(records, session)

    def set_next_run(self):
        """Annual data — next run in 365 days."""
        self.next_run = datetime.now() + timedelta(days=365)
```

### Multi-Threaded Scraper with Proxy

For high-volume collection, use `ThreadPoolExecutor` with per-thread `Request` instances sharing a common `RequestMetrics`:

```python
# Planned file path: data_collector/country/legal/court_cases/main.py
from concurrent.futures import ThreadPoolExecutor, as_completed
from data_collector.scraping import BaseScraper
from data_collector.utilities.request import Request, RequestMetrics
from data_collector.utilities.hashing import bulk_hash
from data_collector.country.legal.court_cases.parser import Parser
from data_collector.country.legal.court_cases.tables import CaseList, CaseDetail

class CourtCases(BaseScraper):
    """Multi-threaded detail page scraper with proxy rotation."""

    base_url = "https://example.com"

    def __init__(self, database, **kwargs):
        super().__init__(database, **kwargs)
        self.parser = Parser()

    def prepare_list(self):
        """Fetch case IDs that need detail pages scraped."""
        with self.database.create_session() as session:
            self.work_list = self.database.query(
                session, CaseList,
                filters=and_(CaseList.details_fetched == None)
            )

    def collect(self):
        """Multi-threaded detail page collection."""
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(self.collect_detail, record, idx): record
                for idx, record in enumerate(self.work_list)
            }
            for future in as_completed(futures):
                if self.fatal_flag:
                    break
                future.result()

    def collect_detail(self, record, instance_id):
        """Per-thread: fetch + parse + store one record."""
        # Each thread gets its own Request — metrics aggregate in shared self.metrics
        req = Request(timeout=120, retries=3, metrics=self.metrics)
        req.set_headers({"User-Agent": "Mozilla/5.0 ..."})
        req.set_proxy(self.get_proxy())

        worker_logger = self.logger.bind(instance_id=instance_id)

        response = req.get(
            f"{self.base_url}/case/{record.case_id}"
        )
        if req.should_abort(worker_logger, proxy_on=True):
            return

        data = self.parser.parse_case_detail(response.content)
        self.store([data])
        self.solved += 1

    def store(self, records):
        bulk_hash(records)
        with self.database.create_session() as session:
            self.database.merge(records, session)
```

> **Thread safety:** Each thread creates its own `Request` instance with its own HTTP session and `ExceptionDescriptor`. The shared `RequestMetrics` aggregates counters, timing, and circuit breaker data across all threads using `threading.Lock`. See [4.4. request.md — Thread Safety](4.4.%20request.md#thread-safety) for details.

## `init()` Entry Point

Every app has a module-level `init(runtime)` function that the Manager calls via subprocess:

```python
# Planned file path: data_collector/country/registries/registry_records/main.py (bottom of file)
import argparse
import json
import uuid

def init(runtime, args=None):
    """Entry point called by the Manager process.

    Args:
        runtime: Unique execution identifier (UUID4 hex string)
        args: Optional dict of runtime arguments for targeted collection
    """
    database = Database(MainDatabaseSettings())
    service = LoggingService("country.registries.registry_records", db_engine=database.engine)
    logger = service.configure_logger()

    # app_id derived automatically from file path — convention over configuration
    app_id = get_app_info(__file__, only_id=True)
    logger = logger.bind(app_id=app_id, runtime=runtime)

    # Update Apps table — running
    update_app_status(database, app_id,
                      run_status=RunStatus.RUNNING, runtime_id=runtime)

    # Run scraper lifecycle — pass args to scraper
    scraper = RegistryPortal(database, logger=logger, runtime=runtime, args=args)
    scraper.prepare_list()
    scraper.collect()
    scraper.fatal_check()
    scraper.set_next_run()

    # Update Apps table — done
    update_app_status(database, app_id,
                      run_status=RunStatus.NOT_RUNNING,
                      next_run=scraper.next_run,
                      solved=scraper.solved,
                      failed=scraper.failed)

    # Persist request metrics to Runtime record
    stats = scraper.metrics.log_stats(logger)

    service.stop()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Data Collector App")
    parser.add_argument("--args", type=json.loads, default=None,
                        help="JSON string of runtime arguments")
    parsed = parser.parse_args()

    runtime = uuid.uuid4().hex
    init(runtime, args=parsed.args)
```

The `init()` function:
1. Creates `Database` and `LoggingService` instances
2. Derives `app_id` from `__file__` via `get_app_info()` — no hardcoded strings
3. Binds `app_id` and `runtime` to the logger
4. Updates the `Apps` table to `RUNNING`
5. Instantiates the scraper class with optional `args` and calls lifecycle methods
6. Updates the `Apps` table with results (`solved`, `failed`, `next_run`)
7. Persists request metrics to the `Runtime` record
8. Stops the logging service (flushes queue)

**`get_app_info(__file__)`** extracts `app_group`, `app_parent`, and `app_name` from the file path (last 4 path components), then computes `app_id = sha256(app_group|app_parent|app_name)`. Convention over configuration — the directory structure determines app identity. See [4.3. functions.md](4.3.%20functions.md) for the full utility reference.

Running directly (`python -m data_collector.country.registries.registry_records.main`) generates its own `runtime` UUID — useful for development and debugging. Pass `--args` with a JSON string for targeted runs (e.g., `--args '{"company_id":"123"}'`). Use `--help` to see available options.

## App Arguments

Apps accept optional runtime arguments for targeted collection, monitoring, and ad-hoc runs. Arguments arrive as a `dict | None` and are stored on `self.args`.

### Argument Sources

| Source | Mechanism | Example |
|--------|-----------|---------|
| **Manager subprocess** | `--args` CLI flag with JSON string | `python -m data_collector.country.registries.registry_records.main --args '{"company_id":"123"}'` |
| **RabbitMQ command** | Args dict in command message payload | `{"cmd": "start", "app": "...", "args": {"monitoring": true}}` |
| **Direct execution** | `argparse` CLI parsing | `python -m data_collector.country.registries.registry_records.main --args '{"dry_run": true}'` |

CLI argument parsing uses `argparse` (stdlib) — provides `--help`, input validation, and clear error messages out of the box.

### Common Argument Patterns

| Pattern | Argument | Effect on `prepare_list()` |
|---------|----------|---------------------------|
| Targeted collection | `company_id`, `reg_number` | Filters to single entity |
| Monitoring mode | `monitoring: true` | Filters to monitored entities only |
| Date override | `target_date: "2025-01-15"` | Processes specific date instead of auto-calculated |
| Dry run | `dry_run: true` | Validates without storing (skip `store()`) |
| Batch limit | `limit: 100` | Processes only N items from work_list |

### Usage in `prepare_list()`

```python
def prepare_list(self):
    """Fetch registry numbers — with optional args filtering."""
    filters = [Companies.archive == None]

    # Targeted collection: single company
    if self.args and self.args.get("company_id"):
        filters.append(Companies.reg_number == self.args["company_id"])
    # Monitoring mode: only monitored companies
    elif self.args and self.args.get("monitoring"):
        filters.append(Companies.monitored == True)
    else:
        filters.append(Companies.date_checked < self.threshold)

    # Batch limit
    limit = self.args.get("limit") if self.args else None

    with self.database.create_session() as session:
        self.work_list = self.database.query(
            session, Companies,
            filters=and_(*filters),
            limit=limit
        )
```

When no arguments are passed (`args=None`), the app runs its default full collection — backward compatible with existing behavior.

## Parser Module

Each app has a `Parser` class in `parser.py`. This separation keeps parsing logic testable, reusable, and independent of HTTP/database concerns. Instance attributes store intermediate data (tokens, lookup tables, extracted IDs) that multiple methods need:

```python
# Planned file path: data_collector/country/registries/registry_records/parser.py
from bs4 import BeautifulSoup
from data_collector.country.registries.registry_records.tables import RegistryRecord


class Parser:
    """Parse registry portal HTML responses."""

    def get_request_token(self, content):
        """Extract CSRF token from login page."""
        soup = BeautifulSoup(content, 'lxml')
        token = soup.find('input', {'name': '__RequestVerificationToken'})
        return token['value'] if token else None

    def check_signin(self, content):
        """Check if another session is already active."""
        soup = BeautifulSoup(content, 'lxml')
        return soup.find('div', {'class': 'confirm-dismiss'}) is not None

    def get_guid(self, content):
        """Extract session GUID for dismissal."""
        soup = BeautifulSoup(content, 'lxml')
        guid_input = soup.find('input', {'name': 'guid'})
        return guid_input['value'] if guid_input else None

    def parse_results(self, json_data):
        """Parse API response into ORM objects."""
        records = []
        for item in json_data.get('data', []):
            records.append(RegistryRecord(
                name=item[1],
                tax_id=item[2],
                status=item[5]
            ))
        return records
```

**Parser guidelines:**
- One `Parser` class per `parser.py` — instantiated in the scraper's `__init__()`
- Takes raw content (HTML bytes, JSON dict, XML string), returns structured data (dicts or ORM objects)
- Uses BeautifulSoup, `re`, `json`, or `xml.etree` — never `Request` or `Database`
- Store intermediate data as instance attributes when multiple methods need it
- Easily unit-testable with saved response fixtures
- Supports inheritance — `Parser` can extend a base parser for NER or deep learning extraction (see [21. deep-learning-ner.md](21.%20deep-learning-ner.md))

### Parser Selection

| Parser | Best for | Performance |
|--------|----------|-------------|
| `lxml` | Well-formed HTML, speed-critical | Fastest |
| `html5lib` | Broken/malformed HTML | Slowest, most lenient |
| `html.parser` | Simple pages, no extra dependencies | Medium |

## Error Handling and Fatal Check

### Error Tracking in BaseScraper

BaseScraper inherits error tracking as built-in infrastructure:

| Attribute | Type | Description |
|-----------|------|-------------|
| `fatal_flag` | bool | Set to `True` when error threshold exceeded |
| `fatal_msg` | str | Description of the fatal condition |
| `fatal_time` | datetime | When the fatal condition was detected |
| `alert_threshold` | float | Error percentage that triggers fatal (default: 0.20 = 20%) |

### `fatal_check()`

Called after `collect()` completes, `fatal_check()` evaluates error ratios and triggers alerts if thresholds are exceeded:

```python
# Inherited from BaseScraper — called after collect()
scraper.fatal_check()

# Evaluates (via RequestMetrics):
# - timeout_err / total_requests > alert_threshold → fatal
# - bad_status_code_err / total_requests > alert_threshold → fatal
# - proxy_errors / total_requests > alert_threshold → fatal
#
# If fatal:
#   scraper.fatal_flag = True
#   Updates Apps table: fatal_flag=FAILED_TO_START, fatal_msg=description
#   Sends notification via configured channel (Telegram, Teams, etc.)
```

### Abort Logic in `collect()`

The `Request` class provides `should_abort()` for per-request error introspection — see [4.4. request.md — Error Handling](4.4.%20request.md#error-handling):

```python
for item in self.work_list:
    response = self.request.get(url)
    if self.request.should_abort(self.logger, proxy_on=True):
        break  # Stop processing — proxy or target is broken

    data = self.parser.parse_item(response.content)
    self.store([data])
    self.solved += 1
```

For multi-threaded scrapers, combine `should_abort()` with target health from `RequestMetrics`:

```python
def collect_detail(self, item, instance_id):
    req = Request(timeout=30, retries=3, metrics=self.metrics)
    url = f"{self.base_url}/{item.id}"

    # Check circuit breaker before wasting a request
    if req.is_target_unhealthy(url):
        self.failed += 1
        return

    response = req.get(url)
    if req.should_abort(self.logger, proxy_on=True):
        return
```

## Rate Limiting

- Configurable delay between requests (default: 1-3 seconds)
- Random jitter to avoid detection patterns
- Per-domain rate limiting for multi-source scrapers

## Data Source Categories

| Category | Examples | Patterns |
|----------|---------|----------|
| Government registries | Business registries, court records | Multi-page search + detail pages |
| Financial data | Stock exchanges, financial reports | API + periodic scraping |
| Legal/court records | Court decisions, insolvency notices | RSS feeds + document download |
| Public procurement | Tender portals | Search + filter + pagination |
| Company registries | National registries | SOAP API or web scraping |

## Integration with Framework

- **Change detection:** `bulk_hash()` + `merge()` for incremental updates
- **Logging:** All scraper activity logged via `LoggingService` with structlog bound context
- **Runtime tracking:** Each scraper run creates a `Runtime` record with request metrics
- **Progress monitoring:** `solved`, `failed`, `list_size`, `progress` updated in `Apps` table during execution
- **Dependency mapping:** Database objects used during scraping are tracked via `AppDbObjects`
- **Request metrics:** `RequestMetrics` aggregates timing, status codes, and circuit breaker data — persisted to `Runtime` table

## Dependencies

- **Request class** ([4.4. request.md](4.4.%20request.md)) — HTTP client, sessions, retries, error handling, metrics
- **BeautifulSoup4** (`bs4`) — HTML parsing
- **lxml** — Fast parser
- **html5lib** — Lenient parser
- Proxy management ([15. proxy.md](15.%20proxy.md))
- Captcha solving ([12. captcha.md](12.%20captcha.md))
- Concurrency ([16. concurrency.md](16.%20concurrency.md))


