[← Back to index](1.%20index.md)

# Centralized Request Class

**Source:** `data_collector/utilities/request.py` | **Phase:** 2 (v0.2.0)

## Overview

All HTTP operations in the framework go through a single `Request` class built on **httpx**. This class provides both synchronous and asynchronous methods in one unified interface, with built-in error tracking, retry logic, and session management.

Used across the entire framework — scraping, CAPTCHA solving, SOAP/XML, proxy validation, notifications, and any other HTTP communication.

Built from the ground up based on enterprise experience with 40+ data collection apps processing millions of requests.

## Architecture Positioning

The framework uses a custom `Request` class rather than a full crawling framework like Scrapy. This is a deliberate architectural choice:

| Capability | Request Class | Scrapy |
|-----------|---------------|--------|
| **Data collection pattern** | Targeted — known URLs, paginated searches, APIs | Web crawling — link-following discovery |
| **Orchestration** | Manager controls lifecycle (start/stop/PID) | Scrapy engine controls its own lifecycle |
| **Data pipeline** | Framework DB layer — `merge()`, `bulk_hash()`, ORM | Scrapy Item Pipeline — feed exports |
| **Async runtime** | httpx (asyncio-native) | Twisted reactor |
| **Scope** | HTTP, SOAP, API, file downloads — one class | Web scraping only |
| **Framework integration** | Native — logging, runtime tracking, progress | Requires adapters to bridge back to framework |

Framework apps are **targeted data collectors** (government registries, financial APIs, court records) — not web crawlers that discover pages by following links.

## Initialization

The constructor takes only transport configuration. Session state (headers, cookies, auth, proxy) is managed via setter methods — enabling runtime flexibility when headers or cookies change mid-operation (after login, CAPTCHA solve, token refresh).

```python
from data_collector.request import Request

# Constructor — transport config only
req = Request(
    timeout=30,
    retries=3,
    backoff_factor=2,
    retry_on_status=[429, 500, 502, 503, 504],
    save_responses=True,             # optional — save HTML/JSON to disk
    save_dir="output/responses"      # optional — directory for saved responses
)

# Session state — setter methods
req.set_headers({
    "User-Agent": "Mozilla/5.0 ...",
    "Accept": "text/html,application/xhtml+xml",
    "Accept-Language": "en-US,en;q=0.9"
})
req.set_cookies({"session_id": "abc123"})
req.set_auth("username", "password")
req.set_proxy(proxy_config)          # optional — see 15. proxy.md

# --- Synchronous usage ---
response = req.get("https://example.com/page")
response = req.post("https://example.com/api", json={"query": "test"})

# --- Asynchronous usage ---
response = await req.async_get("https://example.com/page")
response = await req.async_post("https://example.com/api", json={"query": "test"})
```

### Constructor Parameters (Transport)

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `timeout` | int | 30 | Request timeout in seconds |
| `retries` | int | 3 | Maximum retry attempts |
| `backoff_factor` | int | 2 | Exponential backoff multiplier |
| `retry_on_status` | list[int] | [429, 500, 502, 503, 504] | HTTP status codes that trigger retry |
| `save_responses` | bool | False | Save raw HTML/JSON responses to disk |
| `save_dir` | str | None | Directory for saved responses |
| `metrics` | RequestMetrics \| None | None | Shared metrics collector for multi-threaded aggregation (see [RequestMetrics](#requestmetrics)) |

### Setter Methods (Session State)

| Method | Description |
|--------|-------------|
| `set_headers(headers: dict)` | Set default headers for all requests |
| `reset_headers()` | Clear all default headers |
| `set_cookies(cookies: dict)` | Set cookies for session persistence |
| `reset_cookies()` | Clear all cookies |
| `set_auth(username: str, password: str)` | Set HTTP basic authentication |
| `set_proxy(proxy_config)` | Set proxy configuration (see [15. proxy.md](15.%20proxy.md)) |

Setters are plain methods (return `None`, no chaining). Headers and cookies persist across requests within the same `Request` instance until explicitly reset.

### Request Methods

| Method | Description |
|--------|-------------|
| `get(url, **kwargs)` | Synchronous GET via `httpx.Client` |
| `post(url, **kwargs)` | Synchronous POST via `httpx.Client` |
| `async_get(url, **kwargs)` | Asynchronous GET via `httpx.AsyncClient` |
| `async_post(url, **kwargs)` | Asynchronous POST via `httpx.AsyncClient` |

## Session Management

```python
# Headers and cookies persist across requests — useful for authenticated sessions
req = Request(timeout=30, retries=3)

# Step 1: Login
req.set_headers({"User-Agent": "Mozilla/5.0 ...", "Accept": "text/html"})
login_response = req.post("https://example.com/login", data={"user": "admin", "pass": "secret"})

# Step 2: Cookies are now set from login response — scrape authenticated pages
response = req.get("https://example.com/protected/data")

# Step 3: Switch headers for API endpoint
req.set_headers({"User-Agent": "Mozilla/5.0 ...", "Accept": "application/json"})
api_response = req.get("https://example.com/api/records")

# Step 4: Reset for a different source
req.reset_headers()
req.reset_cookies()
```

## Retry Strategy

- **Exponential backoff:** 1s, 2s, 4s, 8s between retries (configurable `backoff_factor`)
- **Max retries:** configurable per source (default: 3)
- **Retry on:** connection errors, HTTP 429/500/502/503/504 (configurable `retry_on_status`)
- **Fail on:** HTTP 401/403/404 (no point retrying)

## Response Saving

```python
req = Request(save_responses=True, save_dir="output/responses")

# Responses are saved as:
#   output/responses/2024-01-15_143022_example.com_page.html
#   output/responses/2024-01-15_143025_api.example.com_data.json
```

## Error Handling

### Grouped Exception Handling

The `make_request()` method categorizes exceptions into groups, incrementing specific counters and recording errors with timestamps. This classification drives the error introspection methods below.

| Group | Exceptions | Counter | Retry? |
|-------|-----------|---------|--------|
| **Timeout** | `TimeoutException`, `ConnectTimeout`, `ReadTimeout` | `timeout_err` | Yes |
| **Connection/Proxy** | `ConnectionError`, `ProxyError`, `SSLError` | `proxy_err` | Depends on proxy config |
| **HTTP Error** | Non-2xx status codes (extracted from response) | `bad_status_code_err` | Yes for 429/5xx, No for 4xx |
| **Too Many Redirects** | Excessive 3xx chain | `redirect_err` | No |
| **General Request** | Other request library errors | `request_err` | No |
| **Other** | Catch-all for unexpected exceptions | `other_err` | No |

Every error is recorded in the `ExceptionDescriptor` with a timestamp, error type, message, and URL — enabling time-based queries and post-mortem analysis.

### ExceptionDescriptor

Timestamped error tracking — each error is stored with its occurrence time, enabling queries like "did any errors occur after the last successful request?"

```python
class ExceptionDescriptor:
    """Tracks errors with timestamps for time-based analysis."""

    def __init__(self):
        # key: datetime, value: dict with 'type', 'message', 'url'
        self.errors: dict[datetime, dict[str, str]] = {}

    def add_error(self, error_type: str, message: str, url: str | None = None):
        self.errors[datetime.now()] = {
            "type": error_type,
            "message": message,
            "url": url or ""
        }

    def get_last_error(self) -> dict[str, str] | None:
        return next(reversed(self.errors.values()), None)

    def get_errors_by_type(self, error_type: str) -> list[dict[str, str]]:
        return [v for v in self.errors.values() if v["type"] == error_type]

    def has_errors_after(self, timestamp: datetime) -> bool:
        return any(err_time > timestamp for err_time in self.errors)

    def clear(self):
        self.errors.clear()
```

### Error Introspection Methods

The Request class provides boolean methods to inspect the last error. Callers use these to decide what action to take — without parsing error messages themselves.

| Method | Returns `True` when | Use case |
|--------|-------------------|----------|
| `has_errors()` | Any error occurred after the last request timestamp | General error check |
| `is_blocked()` | HTTP 401/403/429 or "forcibly closed by remote host" | IP banned, need proxy rotation |
| `is_proxy_error()` | 502 proxy error, tunnel failed, SSL/protocol errors | Proxy is broken, get new one |
| `is_timeout()` | Last error type is Timeout | Page slow or proxy slow |
| `is_server_down()` | 5xx status codes, "internal server error" | Target site is down |

```python
# After a request, check what happened
response = req.get("https://registry.example.com/search")

if req.is_blocked():
    logger.warning("IP blocked, rotating proxy")
    req.set_proxy(next_proxy)
elif req.is_timeout():
    logger.warning("Request timed out, will retry")
elif req.is_server_down():
    logger.error("Target server is down, aborting")
```

### Abort Decision — `should_abort()`

Centralized method that checks all error conditions and returns `True` if the caller should stop the current operation. Logs the abort reason automatically. This standardizes error handling across all apps — developers don't reimplement abort logic.

```python
def should_abort(self, logger, proxy_on: bool = False) -> bool:
    """Returns True if a critical error occurred and the caller should stop."""
    if not self.has_errors():
        return False

    # If using proxy, check for proxy-specific fatal errors
    if proxy_on and (self.is_blocked() or self.is_proxy_error()):
        logger.debug("Aborting: proxy is blocked or not working")
        return True

    # Page timeout
    if self.is_timeout():
        logger.debug("Aborting: page or proxy timeout")
        return True

    # Server down
    if self.is_server_down():
        logger.debug("Aborting: server page is down")
        return True

    # Any other error — log and abort
    logger.error(str(self.get_last_error()))
    return True
```

Usage:
```python
for url in urls:
    response = self.request.get(url)
    if self.request.should_abort(logger, proxy_on=True):
        break  # Stop processing, current proxy/page is broken

    data = self.parse_data(response)
    self.store_data(data)
```

## Error Counters and Statistics

Each Request instance tracks cumulative error counts. When a `RequestMetrics` instance is provided (see [RequestMetrics](#requestmetrics)), counters aggregate in the shared collector — enabling accurate totals across all threads. When no `RequestMetrics` is provided, counters stay on the individual `Request` instance (single-threaded mode).

Error introspection methods (`is_blocked()`, `is_timeout()`, `should_abort()`) always use the per-thread `ExceptionDescriptor` — they reflect the last error on *this* Request instance, not the aggregate.

```python
# Single-threaded — counters on Request
stats = req.log_stats(logger)

# Multi-threaded — counters on shared RequestMetrics
stats = metrics.log_stats(logger)
```

The error breakdown is built dynamically — it discovers all attributes ending in `_err` with values > 0. New error counters are automatically included without code changes.

| Counter | Tracks |
|---------|--------|
| `request_count` | Total requests made |
| `timeout_err` | Timeout errors |
| `proxy_err` | Proxy/connection/SSL errors |
| `bad_status_code_err` | HTTP 4xx/5xx responses |
| `redirect_err` | Too many redirects |
| `request_err` | General request library errors |
| `other_err` | Unexpected exceptions |

## Request Metrics

Performance metrics are tracked in memory during each run — no per-request database writes. At millions of requests per week, storing individual request rows is unsustainable. Instead, the framework collects **aggregate summaries** per runtime and persists only the summary.

### RequestMetrics <a id="requestmetrics"></a>

`RequestMetrics` is a **thread-safe shared metrics collector** — created once per runtime, passed to every `Request` instance. It aggregates error counters, timing data, status codes, per-proxy stats, and circuit breaker state across all threads.

```python
from data_collector.request import Request, RequestMetrics

# Create shared metrics once per runtime
metrics = RequestMetrics(
    max_target_failures=3,     # Circuit breaker: consecutive failures before marking unhealthy
    min_distinct_proxies=2     # Circuit breaker: min proxies that must fail
)

# Each thread creates its own Request with the shared metrics
req = Request(timeout=30, retries=3, metrics=metrics)
req.set_headers({"User-Agent": "Mozilla/5.0 ..."})
response = req.get("https://registry.example.com/search")
```

**Separation of concerns:**

| Concern | `Request` (per-thread) | `RequestMetrics` (shared) |
|---------|----------------------|--------------------------|
| HTTP response state | `response`, `last_request_url` | — |
| Session state | headers, cookies, auth, proxy | — |
| Error introspection | `ExceptionDescriptor`, `is_blocked()`, `is_timeout()`, `should_abort()` | — |
| Error counters | Basic counters when no metrics provided | `timeout_err`, `proxy_err`, `bad_status_code_err`, etc. |
| Timing / reservoir sampling | — | Per-domain reservoir, P50/P95/P99 |
| Status code distribution | — | Per-domain status code counts |
| Per-proxy breakdown | — | Per-proxy timing and success/failure |
| Circuit breaker | — | `is_target_unhealthy()` |
| Statistics output | — | `log_stats()` returns aggregated dict |

**Key methods:**

| Method | Description |
|--------|-------------|
| `record_request(domain, proxy, status_code, response_time_ms)` | Called by `Request.make_request()` after each response |
| `record_error(domain, proxy, error_type)` | Called by `Request.make_request()` on exception |
| `is_target_unhealthy(url)` | Circuit breaker check — see [Target Health](#target-health) |
| `log_stats(logger)` | Returns aggregated stats dict and logs summary |

**Thread safety:** All methods acquire an internal `threading.Lock`. Mutating methods (`record_request`, `record_error`) and read methods (`log_stats`, `is_target_unhealthy`) are all lock-protected. The lock scope is narrow — each call holds the lock for microseconds.

**Async safety:** asyncio runs on a single thread. The lock is never contested, so there's zero overhead.

**Internal flow — `Request.make_request()` calls `RequestMetrics` automatically:**
```
Request.get(url)
    └── make_request(url)
            ├── response = httpx.get(url)
            ├── self.metrics.record_request(domain, proxy, status_code, response_time_ms)
            └── on exception: self.metrics.record_error(domain, proxy, error_type)
```

Developers never call `record_request()` or `record_error()` directly — `Request` handles delegation internally.

**Single-threaded backward compatibility:**

```python
# Still works — no RequestMetrics needed for simple apps
req = Request(timeout=30, retries=3)
response = req.get("https://example.com")
stats = req.log_stats(logger)  # Basic error counters only, no timing/circuit breaker
```

When no `RequestMetrics` is provided, `Request` tracks basic error counters internally (current behavior). Timing, reservoir sampling, and circuit breaker require `RequestMetrics`.

### Extended `log_stats()` with Timing

`make_request()` records `response_time_ms` after each request. Per-domain timing is tracked via **reservoir sampling** — a fixed-size sample (~1000 entries per domain, ~8KB memory) that produces accurate P50/P95/P99 percentiles regardless of total request volume.

```python
# Multi-threaded — call on shared RequestMetrics after all threads complete
stats = metrics.log_stats(logger)

# Single-threaded — delegate through Request
stats = req.log_stats(logger)
```

```python
# Logs structured summary:
# {
#     "total_requests": 4200,
#     "total_errors": 50,
#     "error_rate_percent": 1.2,
#     "error_breakdown": {"timeout": 30, "proxy": 15, "bad_status_code": 5},
#     "timing": {
#         "avg_ms": 320,
#         "p50_ms": 280,
#         "p95_ms": 520,
#         "p99_ms": 1200
#     },
#     "by_domain": {
#         "registry.example.com": {
#             "count": 3500, "success": 3470, "p95_ms": 480,
#             "status_codes": {"200": 3470, "429": 20, "503": 10}
#         },
#         "api.example.com": {
#             "count": 700, "success": 680, "p95_ms": 620,
#             "status_codes": {"200": 680, "500": 20}
#         }
#     },
#     "by_proxy": {
#         "DE:8080": {"count": 2100, "success": 2080, "p95_ms": 450},
#         "US:9090": {"count": 2100, "success": 2070, "p95_ms": 380}
#     }
# }
```

Proxy breakdown uses `country:port` only — no usernames or passwords in metrics output.

### Runtime Table Timing Columns

`log_stats()` returns a dict. The caller persists relevant fields to the `Runtime` record — the framework already creates a Runtime record at the end of each run.

| Column | Type | Description |
|--------|------|-------------|
| `request_count` | Integer | Total HTTP requests made during this runtime |
| `avg_response_ms` | Integer | Average response time in milliseconds |
| `p95_response_ms` | Integer | 95th percentile response time in milliseconds |
| `error_rate` | Float | Error rate as percentage (0.0–100.0) |

These columns enable historical trend analysis without keeping per-request data. Weekly P95 breach detection is a simple SQL query on the existing `Runtime` table:

```sql
-- Weekly P95 trend per app — detect performance degradation
SELECT
    app_id,
    AVG(p95_response_ms) AS avg_p95,
    MAX(p95_response_ms) AS max_p95,
    AVG(error_rate)       AS avg_error_rate,
    COUNT(*)              AS runs
FROM runtime
WHERE start_time > NOW() - INTERVAL '7 days'
GROUP BY app_id
ORDER BY avg_p95 DESC;

-- Flag apps where P95 exceeds historical baseline
SELECT app_id, p95_response_ms, start_time
FROM runtime
WHERE start_time > NOW() - INTERVAL '1 day'
  AND p95_response_ms > (
      SELECT AVG(p95_response_ms) * 1.5
      FROM runtime r2
      WHERE r2.app_id = runtime.app_id
        AND r2.start_time > NOW() - INTERVAL '30 days'
  );
```

No new tables, no in-memory history across runs, no external infrastructure. One row per runtime — a few hundred rows per week across 70+ apps.

### Target Health — Circuit Breaker <a id="target-health"></a>

Tracks per-target success/failure in `RequestMetrics` during a run. When a target (domain or URL pattern) fails consecutively across multiple different proxies, it's the **target** that's down — not the proxy. Continuing to retry wastes proxy credits.

```python
# RequestMetrics tracks success/failure per target internally.
# Request.make_request() updates it automatically after each response.

# Check before making a request — delegates to metrics.is_target_unhealthy()
if req.is_target_unhealthy("registry.example.com/company/12345"):
    logger.warning("Target consistently failing across proxies, skipping",
                   target="registry.example.com/company/12345")
    continue  # Don't waste another proxy on a dead target
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_target_failures` | 3 | Consecutive failures before marking target unhealthy |
| `min_distinct_proxies` | 2 | Minimum different proxies that must fail (prevents false positives from one bad proxy) |

The circuit breaker resets between runs — no persistence needed. It solves a specific, expensive problem: burning proxy credits on targets that are temporarily or permanently unavailable.

Usage alongside `should_abort()`:
```python
for url in urls:
    if self.request.is_target_unhealthy(url):
        logger.debug("Skipping unhealthy target", url=url)
        continue

    response = self.request.get(url)
    if self.request.should_abort(logger, proxy_on=True):
        break  # Proxy/connection is broken — stop entirely

    data = self.parse_data(response)
    self.store_data(data)
```

Note the distinction: `should_abort()` stops the entire loop (proxy is dead, server is down). `is_target_unhealthy()` skips one target and continues with the next (this specific URL is broken, but others may work).

When no `RequestMetrics` is provided, `is_target_unhealthy()` always returns `False` — circuit breaker requires the shared collector.

### Observability Scope

| Feature | v1.0 | Rationale |
|---------|------|-----------|
| `RequestMetrics` shared collector | Yes | Thread-safe aggregation across Request instances |
| Extended `log_stats()` with timing | Yes | Lightweight, extends existing pattern |
| Runtime table timing columns | Yes | Weekly P95 via SQL, one row per run |
| Target health circuit breaker | Yes | Direct proxy cost savings |

## SOAP Methods

The Request class provides SOAP support via Zeep, ensuring SOAP calls are tracked by the same `ExceptionDescriptor`, counters, and `RequestMetrics` as REST calls.

### `create_soap_client(wsdl_url, **kwargs)`

Creates a Zeep `Client` wired through the Request's session. Proxy, auth, and timeout configuration flow through automatically. Additional Zeep kwargs (e.g., `wsse`, `service_name`, `port_name`) are passed to the `Client` constructor.

```python
# Basic SOAP client
client = req.create_soap_client("https://api.example.com/service?wsdl")

# With WS-Security
from zeep.wsse.username import UsernameToken
client = req.create_soap_client(wsdl_url, wsse=UsernameToken("user", "pass"))

# With WSDL caching (for apps that create Client per iteration)
client = req.create_soap_client(wsdl_url, cache=True)
```

### `soap_call(service_method, **params)`

Wraps a Zeep service call with the same error handling as `get()` and `post()`:

- Catches `Fault`, `Timeout`, `ConnectionError`, `TransportError`
- Records errors in `ExceptionDescriptor`
- Increments counters in `RequestMetrics`
- Returns `None` on error (or raises if `raise_faults=True`)

```python
# Standard call — errors handled, counters tracked
result = req.soap_call(client.service.GetCompanyInfo, registration_number="12345678")
if result is None:
    # Error already logged in ExceptionDescriptor
    pass

# With raise_faults — for app-specific fault handling
from zeep.exceptions import Fault
try:
    result = req.soap_call(client.service.GetAccountsByOIB, raise_faults=True, oib="12345")
except Fault as e:
    if e.code == "INVALID_OIB":
        # App-specific handling
        pass
```

After `soap_call()`, `should_abort()` works exactly as after `get()`/`post()`:

```python
result = req.soap_call(client.service.GetData, id=item.id)
if req.should_abort(logger, proxy_on=False):
    break
```

See [13. soap-xml.md](13.%20soap-xml.md) for full SOAP integration patterns, authentication methods, and BaseScraper examples.

## Response Helpers

| Method | Description |
|--------|-------------|
| `save_html(save_path)` | Save response content to file (binary write) |
| `get_content()` | Return raw response content |
| `get_content_length()` | Return `Content-Length` header as int |
| `get_json()` | Parse response as JSON, return error string on failure |

## Thread Safety

Each `Request` instance stores per-thread state (`response`, `last_request_url`, `ExceptionDescriptor`). In multi-threaded scenarios (e.g., `ThreadPoolExecutor`), create **one Request instance per thread** — do not share instances across threads. Use a shared `RequestMetrics` to aggregate metrics across all threads.

```python
from data_collector.request import Request, RequestMetrics

# Create shared metrics once per runtime — thread-safe
metrics = RequestMetrics(max_target_failures=3, min_distinct_proxies=2)

def worker(url, instance_id):
    # Each thread gets its own Request with the shared metrics
    req = Request(timeout=30, retries=3, metrics=metrics)
    req.set_headers({"User-Agent": "Mozilla/5.0 ..."})

    if req.is_target_unhealthy(url):        # Delegates to shared metrics
        return

    response = req.get(url)
    if req.should_abort(logger, proxy_on=True):  # Per-thread error introspection
        return

# After all threads complete — aggregated stats from all threads
stats = metrics.log_stats(logger)
# Persist summary to Runtime record
runtime.request_count = stats["total_requests"]
runtime.avg_response_ms = stats["timing"]["avg_ms"]
runtime.p95_response_ms = stats["timing"]["p95_ms"]
runtime.error_rate = stats["error_rate_percent"]
```

**What's per-thread vs shared:**
- `Request` (per-thread): HTTP response, session state, `ExceptionDescriptor`, `should_abort()`
- `RequestMetrics` (shared): error counters, timing, status codes, circuit breaker, `log_stats()`

## Dependencies

- **httpx** — HTTP client (sync and async in one library)
- Proxy management ([15. proxy.md](15.%20proxy.md))
- Logging ([5. logging.md](5.%20logging.md))
