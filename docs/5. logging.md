[← Back to index](1.%20index.md)

# Logging System

## Overview

Data Collector uses [structlog](https://www.structlog.org/) for structured logging with a queue-based pipeline that decouples application code from log I/O. Every log event is a structured dict — not a formatted string — enabling machine-readable output, bound context, and automatic column mapping to the `Logs` database table.

**Source:** `data_collector/utilities/log/`

## Architecture

```
Application Code
    │
logger.info("message", key=value)     ← structlog BoundLogger
    │
    ▼
structlog processor chain             ← normalize, enrich, extract caller info
    │
    ▼
QueueHandler → Queue (max 10,000)     ← app thread never blocks on I/O
    │
    ▼
QueueListener (background thread)     ← dedicated thread for all log I/O
    │
    ▼
RouterHandler                          ← broadcasts to all sinks
    │
┌───┼───────┐
▼   ▼       ▼
Console  Database  Splunk HEC
```

**Why queue-based:** The application thread never blocks on log writes. Database inserts, HTTP requests to Splunk — all happen in a dedicated background thread. If a sink fails, the application continues unaffected.

**Why structlog:** Bound context eliminates repetitive `extra={}` dicts. Bind `app_id` and `runtime` once — every subsequent log call inherits them automatically. Output is structured (key=value for dev, JSON for production), making logs queryable without parsing.

## LoggingService <a id="logging-service"></a>

The main entry point for configuring logging.

**Source:** `data_collector/utilities/log/main.py`

### Constructor

```python
from data_collector.utilities.log.main import LoggingService
from data_collector.settings.main import LogSettings

service = LoggingService(
    logger_name="my_app",
    settings=LogSettings(),      # optional, defaults to LogSettings()
    db_engine=database.engine,   # optional, enables DB logging
    log_level=logging.DEBUG      # optional, overrides settings.log_level
)
```

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `logger_name` | str | required | Name for the logger instance |
| `settings` | LogSettings | `LogSettings()` | Logging configuration |
| `db_engine` | Engine | None | SQLAlchemy engine for database logging |
| `log_level` | int | `settings.log_level` | Minimum log level to capture |

### configure_logger()

Sets up the full logging pipeline and returns a `structlog.stdlib.BoundLogger`:

```python
logger = service.configure_logger()
logger.info("Application started")
logger.warning("Something unusual", threshold=0.95)
logger.error("Record failed", record_id=42, source="api")
```

**What it does internally:**

1. Configures structlog processor chain (see [Processor Chain](#processor-chain))
2. Sets up stdlib logger with level and propagation
3. **Idempotence guard** — if handlers already exist, returns immediately
4. Appends configured sinks based on settings:
   - `DatabaseHandler` if `log_to_db=True` and `db_engine` provided
   - `SplunkHECHandler` if `log_to_splunk=True` with URL and token
5. Always appends console sink with `ProcessorFormatter` (renderer based on `log_format`)
6. Wraps all sinks in `RouterHandler`
7. Creates `Queue` → `QueueHandler` → `QueueListener(RouterHandler)`
8. Starts the listener thread
9. Returns `structlog.get_logger(logger_name)`

### start() / stop()

```python
service.start()   # Starts the QueueListener thread (called automatically by configure_logger)
service.stop()    # Stops the QueueListener thread (call on shutdown)
```

### append_sink()

Register a custom handler before calling `configure_logger()`:

```python
service.append_sink(my_custom_handler)
logger = service.configure_logger()
```

### Debug Mode

Set `service.debug = True` before `configure_logger()` to disable DB and Splunk sinks — console only:

```python
service.debug = True
logger = service.configure_logger()  # Console only
```

## Context Binding <a id="context-binding"></a>

structlog's key feature: bind context to a logger, and every subsequent log call inherits it. Context is bound in **layers** that match the application lifecycle — not all at once.

### Binding Layers

Each identifier has a different lifecycle and gets bound at the appropriate point:

| Layer | Identifier | Lifecycle | Bound when |
|-------|-----------|-----------|------------|
| 1 | `app_id` | Static — never changes | Once at app init |
| 2 | `runtime` | Per execution cycle | Once per runtime cycle |
| 3 | `function_id` | Per function | Per function call (automatic via `@fun_watch`) |
| 4 | `instance_id` | Per thread/worker | Per thread in ThreadPoolExecutor |

#### Layer 1 — App Identity

`app_id` is a hash of `app_group|app_parent|app_name`. It identifies the application and never changes. Bind it once after configuring the logger:

```python
service = LoggingService("my_app", db_engine=database.engine)
logger = service.configure_logger()

# app_id is static — bind once, never changes
app_hash = make_hash(f"{app_group}|{app_parent}|{app_name}")
logger = logger.bind(app_id=app_hash)
```

#### Layer 2 — Runtime Cycle

`runtime` is a UUID4 identifier representing a single execution cycle. Each time the app runs (scheduled or manual), it gets a new runtime ID. Bind it at the start of each execution:

```python
import uuid

# New runtime per execution cycle
runtime_id = uuid.uuid4().hex   # 32-char unique ID
logger = logger.bind(runtime=runtime_id)

# Now all logs carry both app_id and runtime
logger.info("Pipeline started")
# → app_id=abc123 runtime=rt456 event="Pipeline started"
```

#### Layer 3 — Function Scope

`function_id` is a hash of `app_group|app_parent|app_name|function_name`. It changes with every function, so it cannot be bound at startup. The `@fun_watch` decorator handles this automatically — developers rarely bind it manually:

```python
# @fun_watch computes function_id and binds it automatically
@fun_watch
def collect_data(self):
    # self.logger already has function_id bound by the decorator
    self.logger.info("Collecting data")
    # → app_id=abc123 runtime=rt456 function_id=fn789 event="Collecting data"

# Manual binding (rare — only when not using @fun_watch)
func_hash = make_hash(f"{app_group}|{app_parent}|{app_name}|collect_data")
func_logger = logger.bind(function_id=func_hash)
```

#### Layer 4 — Thread/Worker Scope

`instance_id` differentiates parallel workers. Each thread creates a child logger with its own instance_id:

```python
def worker(self, record, instance_id):
    worker_logger = self.logger.bind(instance_id=instance_id)
    worker_logger.info("Processing record", record_id=record.id)
    # → app_id=abc123 runtime=rt456 function_id=fn789 instance_id=3 event="Processing record"
```

### Temporary Context

`logger.bind()` returns a **new logger instance** — the original is never mutated. Use this for sub-scopes:

```python
# Main logger has app_id + runtime bound
batch_logger = logger.bind(batch_id="batch_001", batch_size=500)
batch_logger.info("Processing batch")
# → app_id=abc123 runtime=rt456 batch_id=batch_001 batch_size=500 event="Processing batch"

# Original logger is unaffected
logger.info("Continuing main flow")
# → app_id=abc123 runtime=rt456 event="Continuing main flow" (no batch_id)
```

### Threading Patterns <a id="threading-patterns"></a>

#### Pattern A — Instance-Bound Logger (recommended for ThreadPoolExecutor)

Store the bound logger on `self`. Each worker thread creates a child logger with its own `instance_id`. Since `bind()` returns a new instance (no mutation), this is thread-safe:

```python
class MyApp(Required):
    @fun_watch
    def run(self):
        # self.logger already has app_id, runtime, function_id bound
        with ThreadPoolExecutor(max_workers=self.no_workers) as executor:
            futures = {
                executor.submit(self.worker, record, idx): record
                for idx, record in enumerate(self.task_list)
            }
            for future in as_completed(futures):
                future.result()

    def worker(self, record, instance_id):
        # Create child logger for this thread — parent logger is not affected
        thread_logger = self.logger.bind(instance_id=instance_id)
        thread_logger.info("Processing", record_id=record.id)
        # → app_id=... runtime=... function_id=... instance_id=3 record_id=42
```

#### Pattern B — contextvars (for asyncio or cross-function context)

`structlog.contextvars` uses Python's `contextvars` module. Context is automatically isolated per asyncio task and inherited (but isolated) per thread:

```python
import structlog

# Bind context that spans function calls within the same thread/task
structlog.contextvars.bind_contextvars(request_id="req_123")

# Any logger in this thread/task picks it up automatically
logger.info("Handling request")
# → request_id=req_123 event="Handling request"

# Clear when done
structlog.contextvars.unbind_contextvars("request_id")
```

**When to use which:**

| Pattern | Use when |
|---------|----------|
| `logger.bind()` on `self` | ThreadPoolExecutor workers — logger passed via class instance |
| `contextvars` | Asyncio tasks, or when logger can't be passed through method signatures |

## Processor Chain <a id="processor-chain"></a>

**Planned module path:** `data_collector/utilities/log/processors.py`

structlog processes each log event through an ordered chain of processors before output:

```python
[
    structlog.contextvars.merge_contextvars,     # 1. Merge thread-local context
    structlog.processors.add_log_level,           # 2. Add log_level key
    extract_caller_info,                           # 3. Extract module, function, lineno, call_chain
    structlog.processors.TimeStamper(fmt="iso"),  # 4. ISO-8601 timestamp
    structlog.stdlib.add_logger_name,              # 5. Add logger name
    limit_context_size,                            # 6. Cap arbitrary keys (settings.log_context_max_keys)
    structlog.processors.StackInfoRenderer(),     # 7. Render stack traces
    structlog.processors.format_exc_info,          # 8. Format exception info
    structlog.stdlib.ProcessorFormatter.wrap_for_formatter,  # 9. Bridge to stdlib
]
```

### Custom Processors

#### extract_caller_info

Walks the call stack to populate `module_name`, `module_path`, `function_name`, `lineno`, and `call_chain`. Skips structlog/logging/queue internals to find the actual application code that emitted the log.

- `call_chain`: trace from root caller to actual log emitter (e.g., `"main → run_app → collect_data"`)
- Configurable via `settings.log_call_chain` — disable for performance if not needed

#### limit_context_size

Prevents `context_json` explosion by capping the number of arbitrary context keys. Fixed keys (app_id, runtime, etc.) are exempt. Controlled by `settings.log_context_max_keys` (default: 50).

#### separate_fixed_context

Splits the event dict into two parts for `DatabaseHandler`:
- **Fixed keys** → mapped directly to Logs table columns (app_id, runtime, function_id, etc.)
- **Remaining keys** → serialized as JSON into `context_json`

## Log Sinks

### Console (StreamHandler) <a id="console-handler"></a>

Always active. Output format depends on `settings.log_format`:

**`"console"` (default)** — colored key=value output for development:
```
2025-01-15T10:30:00Z [info     ] Pipeline started               app_id=abc123 runtime=rt456
2025-01-15T10:30:01Z [error    ] Record failed                  app_id=abc123 record_id=42
```

**`"json"` — JSON lines for production / log aggregators:**
```json
{"event": "Pipeline started", "level": "info", "timestamp": "2025-01-15T10:30:00Z", "app_id": "abc123", "runtime": "rt456"}
{"event": "Record failed", "level": "error", "timestamp": "2025-01-15T10:30:01Z", "app_id": "abc123", "record_id": 42}
```

### DatabaseHandler <a id="database-handler"></a>

**Source:** `data_collector/utilities/log/handlers.py`

Writes structured log records to the `Logs` table using the ORM. The `separate_fixed_context` processor splits the event dict — fixed keys map to indexed columns, everything else goes to `context_json`.

#### Event → Column Mapping

| structlog key | Logs column | Source |
|--------------|-------------|--------|
| `app_id` | `app_id` | Bound once at app init via `logger.bind()` |
| `runtime` | `runtime` | Bound once per execution cycle via `logger.bind()` |
| `function_id` | `function_id` | Bound per function by `@fun_watch` or `logger.bind()` |
| `module_name` | `module_name` | Auto-extracted by `extract_caller_info` |
| `module_path` | `module_path` | Auto-extracted by `extract_caller_info` |
| `function_name` | `function_name` | Auto-extracted by `extract_caller_info` |
| `lineno` | `lineno` | Auto-extracted by `extract_caller_info` |
| `call_chain` | `call_chain` | Auto-extracted by `extract_caller_info` |
| `instance_id` | `instance_id` | Bound per thread/worker via `logger.bind()` |
| `event` | `msg` | The log message |
| Level name | `log_level` | Mapped to `LogLevel` enum integer (INFO→20) |
| Everything else | `context_json` | `json.dumps()` of remaining keys |
| — | `date_created` | Server default (`NOW()`) |

**Implementation:**
- Uses Logs ORM model for inserts (not raw SQL)
- Creates a `Session(engine)` per insert
- Maps level name to `LogLevel` IntEnum value (DEBUG=10, INFO=20, WARNING=30, ERROR=40, CRITICAL=50)
- **No try/except in `emit()`** — exceptions propagate to `RouterHandler`, which logs sink failures to the fallback file. Individual handlers must never swallow their own exceptions.

### SplunkHECHandler <a id="splunk-handler"></a>

**Source:** `data_collector/utilities/log/handlers.py`

Sends structured log events to Splunk via HTTP Event Collector (HEC).

| Setting | Description |
|---------|-------------|
| `hec_url` | Splunk HEC endpoint URL |
| `token` | HEC authentication token |

**Request format:**
```json
{
    "time": 1706000000.0,
    "event": {
        "level": "ERROR",
        "logger": "my_app",
        "message": "Record failed",
        "app_id": "abc123",
        "runtime": "rt456",
        "record_id": 42
    }
}
```

Sends the full structured event (fixed keys + arbitrary context) as JSON. Timeout set to 2.5 seconds. On failure, the exception propagates to `RouterHandler` which logs it to the fallback file.

### RouterHandler <a id="router-handler"></a>

**Source:** `data_collector/utilities/log/router.py`

Observer pattern — broadcasts each log record to all registered handlers. When a sink fails, the failure is logged to a dedicated fallback file (`error.log`) so sink failures are never silent:

```python
import sys
import logging
import traceback
from logging.handlers import RotatingFileHandler

class RouterHandler(logging.Handler):
    def __init__(self, handlers: list[logging.Handler],
                 swallow_errors: bool = True,
                 error_file: str = "error.log",
                 error_max_bytes: int = 5_242_880,
                 error_backup_count: int = 3):
        super().__init__()
        self.handlers = handlers
        self.swallow_errors = swallow_errors

        # Fallback file handler — independent of main pipeline (no DB, no network)
        self._fallback = RotatingFileHandler(
            error_file, maxBytes=error_max_bytes, backupCount=error_backup_count
        )
        self._fallback.setFormatter(logging.Formatter(
            '%(asctime)s SINK_FAILURE %(message)s'
        ))

    def emit(self, record: logging.LogRecord):
        for h in self.handlers:
            try:
                h.handle(record)
            except Exception:
                if not self.swallow_errors:
                    raise
                self._log_sink_failure(h, record, sys.exc_info())

    def _log_sink_failure(self, handler, record, exc_info):
        """Log sink failure to fallback file with handler name, exception, traceback, and original data."""
        handler_name = type(handler).__name__
        original_msg = record.getMessage()
        tb = ''.join(traceback.format_exception(*exc_info))
        failure_record = logging.LogRecord(
            name="sink_fallback", level=logging.ERROR,
            pathname="", lineno=0, msg="",
            args=None, exc_info=None
        )
        failure_record.msg = (
            f"handler={handler_name} | "
            f"error={exc_info[1]!r} | "
            f"original_level={record.levelname} | "
            f"original_msg={original_msg}\n"
            f"Traceback:\n{tb}"
        )
        self._fallback.emit(failure_record)
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `swallow_errors` | `True` | If True, a failing handler doesn't affect others. Set False for testing. |
| `error_file` | `"error.log"` | Path for sink failure fallback log |
| `error_max_bytes` | `5242880` | Max file size before rotation (5 MB) |
| `error_backup_count` | `3` | Number of rotated backup files to keep |

**Why fallback file logging:** In production, `logging.raiseExceptions` is False — the default `handleError()` does nothing. When a DB connection drops or Splunk times out, logs silently disappear with no trace. The fallback file is independent of the main pipeline (pure file I/O, no network) and captures exactly what failed and why.

**Example `error.log` output:**
```
2025-01-15 10:30:01,234 SINK_FAILURE handler=DatabaseHandler | error=OperationalError('connection refused') | original_level=ERROR | original_msg=Record failed
Traceback:
Traceback (most recent call last):
  File "data_collector/utilities/log/handlers.py", line 15, in emit
    session.add(log_entry)
sqlalchemy.exc.OperationalError: connection refused

2025-01-15 10:30:01,235 SINK_FAILURE handler=SplunkHECHandler | error=ConnectionError('timeout') | original_level=INFO | original_msg=Pipeline started
Traceback:
Traceback (most recent call last):
  File "data_collector/utilities/log/handlers.py", line 38, in emit
    requests.post(self.url, headers=self.headers, json=event, timeout=2.5)
requests.exceptions.ConnectionError: HTTPSConnectionPool: Read timed out
```

## Logs Table Schema <a id="logs-table"></a>

See [1.2. data-model.md](1.2.%20data-model.md#logs) for the full column reference.

Key fields for connecting logs to the application lifecycle:
- `app_id` → links to `apps.app` (identifies which application emitted the log)
- `runtime` → links to `runtime.runtime` (identifies which execution cycle)
- `function_id` → hash of `app_group + app_parent + app_name + function_name` (identifies the code location)
- `call_chain` → trace from root caller to actual log emitter
- `log_level` → FK to `c_log_level` (integer, maps from `LogLevel` enum)
- `context_json` → arbitrary structured context as JSON text

## Configuration <a id="configuration"></a>

### LogSettings

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `log_to_db` | bool | `True` | Enable database logging |
| `log_to_splunk` | bool | `False` | Enable Splunk HEC logging |
| `splunk_hec_url` | str \| None | `None` | Splunk HEC endpoint URL |
| `splunk_token` | str \| None | `None` | Splunk HEC auth token |
| `log_max_queue` | int | `10000` | Maximum queue size before blocking |
| `log_format` | `Literal["console", "json"]` | `"console"` | Console: key=value (dev), JSON: machine-readable (prod) |
| `log_level` | int | `10` | Minimum log level (10=DEBUG, 20=INFO, 30=WARNING, 40=ERROR, 50=CRITICAL) |
| `log_call_chain` | bool | `True` | Enable call_chain extraction via stack introspection |
| `log_context_max_keys` | int | `50` | Maximum arbitrary context keys per log event |
| `log_error_file` | str | `"error.log"` | Path for sink failure fallback log |
| `log_error_max_bytes` | int | `5242880` | Max file size before rotation (5 MB) |
| `log_error_backup_count` | int | `3` | Number of rotated backup files to keep |

## Usage Examples

### Basic Setup with Database Logging

```python
from data_collector.utilities.database.main import Database
from data_collector.utilities.log.main import LoggingService
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())

service = LoggingService(logger_name="my_app", db_engine=database.engine)
logger = service.configure_logger()

# Bind app context once
logger = logger.bind(app_id=app_hash, runtime=runtime_id)

logger.info("Pipeline started")
logger.error("Record parsing failed", record_id=42, source="api")
# → DB: app_id, runtime auto-populated; context_json={"record_id": 42, "source": "api"}

# On shutdown
service.stop()
```

### Console-Only (Debug Mode)

```python
service = LoggingService(logger_name="my_app")
service.debug = True
logger = service.configure_logger()

logger.info("This only goes to console")
```

### JSON Output for Production

```python
from data_collector.settings.main import LogSettings

settings = LogSettings(log_format="json", log_level=20)  # JSON, INFO+
service = LoggingService(logger_name="my_app", settings=settings, db_engine=database.engine)
logger = service.configure_logger()

logger.info("Request processed", duration_ms=42, status=200)
# Console: {"event": "Request processed", "level": "info", "duration_ms": 42, "status": 200, ...}
# DB: msg="Request processed", context_json={"duration_ms": 42, "status": 200}
```

### Adding a Custom Sink

```python
import logging

class FileHandler(logging.FileHandler):
    pass

service = LoggingService(logger_name="my_app")
service.append_sink(FileHandler("app.log"))
logger = service.configure_logger()
```

### Structured Exception Logging

```python
try:
    process_record(record)
except Exception:
    logger.exception("Processing failed", record_id=record.id)
    # Automatically includes formatted traceback in the log event
```

## Fixed Columns vs context_json

The Logs table has two types of data storage:

| Type | Columns | Purpose | Queryable |
|------|---------|---------|-----------|
| **Fixed** | `app_id`, `runtime`, `function_id`, `log_level`, `module_name`, `function_name`, `lineno`, `call_chain`, `instance_id`, `msg` | Indexed, frequently queried fields | Direct SQL/ORM filters |
| **Flexible** | `context_json` | Arbitrary key-value pairs that vary per log call | JSON text search |

**Rule:** Bind framework-level identifiers (app_id, runtime, function_id) — they map to indexed columns. Pass application-specific data as keyword arguments — they go to `context_json`.

```python
# Framework context → fixed columns (indexed, queryable)
logger = logger.bind(app_id=app_hash, runtime=runtime_id, function_id=fn_hash)

# Application data → context_json (flexible, per-event)
logger.info("Batch complete", batch_size=500, duration_s=12.3, errors=2)
```
