[← Back to index](1.%20index.md)

# Data Quality & Auditing

**Implementation Status:** In Development | **Phase:** 4 (v0.4.0)

## Overview

Automated data validation, change tracking, and quality metrics. Ensures collected data meets business rules and provides traceability through standard columns, Logs, Runtime records, and the archive pattern.

## Validation Rules

### Schema Validation
- Column type enforcement (string length, numeric ranges)
- Nullability checks (required fields present)
- Format validation (dates, phone numbers, registration numbers)

### Business Rule Validation
- Cross-field validation (e.g., end_date > start_date)
- Referential integrity (foreign key existence)
- Value range checks (e.g., financial amounts within expected bounds)

### Statistical Anomaly Detection
- Row count drift (alert if record count changes >X% between runs)
- Value distribution changes (detect when data patterns shift)
- Null rate monitoring (alert on increasing null percentages)

## Audit Trail

### SHA-Based Change Detection
`Database.merge()` ([4.1. database.md](4.1.%20database.md#database-merge)) inherently provides audit capabilities:
- **Active records:** `archive IS NULL` — current source data
- **Historical records:** `archive IS NOT NULL` — previous versions with timestamp

### Change Tracking
```python
# Track what changed between runs:
# 1. New records (inserted) — didn't exist before
# 2. Removed records (archived) — no longer in source
# 3. Modified records — same entity, different hash (archive old + insert new)
```

### Standard Column Audit Trail
Every data record carries audit context via standard columns ([1.2. data-model.md](1.2.%20data-model.md)):
- `date_created` — when the record was created
- `date_modified` — when the record was last updated
- `archive` — when the record was superseded (soft-deleted by `Database.merge()` or `Database.archive()`)
- `sha` — content hash for change detection

Application identity is tracked in the `Runtime` and `Logs` tables via `app_id` and `runtime` — see [5. logging.md](5.%20logging.md#context-binding) for how these identifiers are bound to every log entry.

> **Note:** A dedicated audit trail subsystem (row-level change journals, etc.) is deferred beyond v1.0. For v1.0, the combination of standard columns (`sha`, `date_created`, `date_modified`, `archive`), the Logs table ([5. logging.md](5.%20logging.md)), Runtime records ([1.2. data-model.md](1.2.%20data-model.md)), and `Database.merge()` ([4.1. database.md](4.1.%20database.md#database-merge)) provide sufficient auditability.

## Quality Metrics

| Metric | Description | Calculation |
|--------|-------------|-------------|
| **Completeness** | % of required fields populated | `non_null_count / total_count` |
| **Freshness** | Time since last successful run | `now() - last_run` |
| **Consistency** | Cross-field rule violations | `violations / total_records` |
| **Volume stability** | Record count vs expected range | `abs(current - avg) / stddev` |

## Integration with Framework

Quality checks run as post-merge validation:
```python
stats = database.merge(records, session, stats=True)

# Post-merge quality checks
if stats.number_of_records < expected_minimum:
    logger.warning("Low record count", count=stats.number_of_records, expected=expected_minimum)

if stats.archived > stats.number_of_records * 0.5:
    logger.error("High archive rate", archived=stats.archived, total=stats.number_of_records)
```

## Dependencies

- `Database.merge()` — SHA-based change detection and archive pattern ([4.1. database.md](4.1.%20database.md#database-merge))
- Logging system — structured logging with audit context ([5. logging.md](5.%20logging.md))
- Data model — standard columns and table definitions ([1.2. data-model.md](1.2.%20data-model.md))
