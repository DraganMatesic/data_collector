[← Back to index](1.%20index.md)

# Concurrency & Async HTTP

**Implementation Status:** IN DEVELOPMENT | **Phase:** 3 (v0.3.0)

## Overview

Parallel and asynchronous processing for scraping and data processing workloads. Combines Python's `concurrent.futures.ThreadPoolExecutor` for CPU-bound or mixed workloads with **async HTTP via httpx** for I/O-bound operations (web scraping, API calls).

Database operations remain synchronous (sync SQLAlchemy for v1.0).

Developed and tested primarily on Windows, but fully deployable on Linux.

## When to Use Async vs Threads

| Pattern | Best for | Example |
|---------|----------|---------|
| **Async HTTP** (`Request.async_get`) | I/O-bound: many concurrent requests | Fetching 100 URLs concurrently |
| **ThreadPoolExecutor** | CPU-bound or mixed workloads | Parsing + scraping combined |
| **Sync HTTP** (`Request.get`) | Simple sequential scrapers | Single-source, iterate-and-fetch |

## Threading Model

```
Main Thread
    |
    v
Create shared RequestMetrics (thread-safe)
    |
    v
ThreadPoolExecutor(max_workers=N)
    |
    +-- Worker 1: own Request + shared metrics + proxy → items batch 1
    +-- Worker 2: own Request + shared metrics + proxy → items batch 2
    +-- Worker 3: own Request + shared metrics + proxy → items batch 3
    +-- ...
    |
    v
metrics.log_stats(logger)  →  aggregated stats from all threads
```

### Per-Thread State

Each worker thread creates its own:
- **`Request` instance** — connection pooling and cookies are not thread-safe
- **Database session** — SQLAlchemy sessions are not thread-safe
- **Proxy assignment** — different proxy per thread avoids IP blocking

Shared across threads (thread-safe):
- **`RequestMetrics`** — error counters, timing, circuit breaker (protected by `threading.Lock`)
- **`Database` engine** — SQLAlchemy engine manages its own connection pool

## Concurrent Scraping Pattern (Threads)

```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from data_collector.utilities.request import Request, RequestMetrics

# Shared metrics — thread-safe, aggregates data from all workers
metrics = RequestMetrics()

def collect_detail(item, proxy_config, instance_id):
    """Worker function — runs in its own thread with its own Request."""
    req = Request(timeout=120, retries=3, metrics=metrics)
    req.set_headers({"User-Agent": "Mozilla/5.0 ..."})
    req.set_proxy(proxy_config)

    response = req.get(f"https://portal.example.com/detail/{item.reg_number}")

    if req.should_abort(logger, proxy_on=True):
        return None

    return parse_detail(response.content)

# Distribute work across threads
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = {
        executor.submit(collect_detail, item, get_proxy(), idx): item
        for idx, item in enumerate(work_list)
    }

    results = []
    for future in as_completed(futures):
        data = future.result()
        if data:
            results.append(data)

# Aggregated stats from all threads
stats = metrics.log_stats(logger)
```

### Integration with BaseScraper

Multi-threaded scrapers use the same pattern inside `collect()`. See [11. scraping.md](11.%20scraping.md) for the full multi-threaded BaseScraper example.

```python
class CourtCases(BaseScraper):
    def collect(self):
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = {
                executor.submit(self.collect_detail, case): case
                for case in self.work_list
            }
            for future in as_completed(futures):
                if self.fatal_flag:
                    break
                future.result()

    def collect_detail(self, case):
        """Per-thread: own Request, shared self.metrics."""
        req = Request(timeout=120, retries=3, metrics=self.metrics)
        req.set_proxy(self.get_proxy())
        # ...
```

## Async HTTP Patterns (httpx)

The centralized `Request` class ([4.4. request.md](4.4.%20request.md)) provides both sync and async methods. For I/O-bound operations like web scraping and API calls, async HTTP delivers significantly better throughput.

### Async Scraping with Request Class

```python
import asyncio
from data_collector.utilities.request import Request

req = Request(retries=3)
req.set_proxy(proxy_config)

async def scrape_all(items: list):
    """Fetch multiple items concurrently using async HTTP."""
    tasks = [req.async_get(f"https://api.example.com/{item.id}") for item in items]
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    results = []
    for item, response in zip(items, responses):
        if isinstance(response, Exception):
            logger.warning("Failed to scrape", item_id=item.id, error=str(response))
        else:
            results.extend(parse_response(response))
    return results

results = asyncio.run(scrape_all(work_list))
```

### httpx.AsyncClient for Concurrent Requests

```python
import httpx

async def fetch_batch(urls: list[str]):
    async with httpx.AsyncClient(timeout=30) as client:
        tasks = [client.get(url) for url in urls]
        return await asyncio.gather(*tasks, return_exceptions=True)
```

## Database Thread Safety

Database operations remain synchronous (sync SQLAlchemy for v1.0).

### Session-Per-Thread Pattern

```python
def worker(records_batch, database):
    with database.create_session() as session:
        database.bulk_insert(records_batch, session)
```

### Connection Pool Sizing

Default: `pool_size=20`, `max_overflow=0`

Rule of thumb: `pool_size >= max_workers` to avoid connection starvation.

```python
database = Database(settings, pool_size=30, max_overflow=10)
```

## Configuration

| Setting | Default | Description |
|---------|---------|-------------|
| `max_workers` | 5 | Number of concurrent threads |
| `queue_size` | 1000 | Max items in work queue |
| `timeout_per_worker` | 300s | Max time per worker task |

## Dependencies

- Python standard library: `concurrent.futures`, `threading`, `asyncio`
- **Request class** ([4.4. request.md](4.4.%20request.md)) — sync and async HTTP
- **RequestMetrics** ([4.4. request.md](4.4.%20request.md)) — thread-safe shared metrics
- Scraping engine ([11. scraping.md](11.%20scraping.md))
- Proxy management ([15. proxy.md](15.%20proxy.md))

