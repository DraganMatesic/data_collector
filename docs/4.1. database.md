[← Back to index](1.%20index.md) | [← Back to utilities](4.%20utilities.md)

# Database Package

Database package contains modules that handle all database-related operations: connection management, DML operations with built-in change detection, real-time dependency tracking, and structured routine execution.

Current modules:
- `main.py` — Database class, connectors, dependency tracking, SHAHashableMixin, BaseModel
- `loaders.py` — Driver pre-flight checks

## main.py Module

Content:
- [Connectors](#connectors)
- [Database class](#database-class)
  - [Session helper](#session-helper)
  - [Object dependency tracking](#object-dependency-tracking)
  - [DML methods](#database-dml-methods)
    - [query](#database-query)
    - [execute](#database-execute)
    - [merge](#database-merge)
    - [delete](#database-delete)
    - [archive](#database-archive)
    - [bulk_insert](#database-bulk-insert)
    - [update_insert](#database-update-insert)
- [SHAHashableMixin](#sha-hashable-mixin)

## Connectors <a id="connectors"></a>

Database connections are built through the pluggable `BaseDBConnector` pattern. Each connector subclass knows how to construct the correct connection string for its database type and authentication method.

| Connector | Database | Driver | Auth Methods |
|-----------|----------|--------|-------------|
| `Postgres` | PostgreSQL | psycopg2 | SQL (username/password), Windows (GSSAPI), Kerberos |
| `MsSQL` | SQL Server | pyodbc (ODBC) | SQL (username/password), Windows (trusted_connection) |

**Adding a new database backend:** Subclass `BaseDBConnector` and implement `build_conn_string()`:

```python
class MySQL(BaseDBConnector):
    def build_conn_string(self):
        host = self.get_host()
        return f"mysql+pymysql://{self.settings.username}:{self.settings.password}@{host}/{self.database_name}"
```

Then register in the `database_classes()` factory function.

### Database class <a id="database-class"></a>

Initializing this class creates an interface for communication with the database specified in the settings argument.

> Check [3. settings.md](3.%20settings.md) to see how settings classes should be configured.

```python
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
```

Constructor parameters:

| Parameter | Type | Description |
|-----------|------|-------------|
| `settings` | `DatabaseSettings` | Configuration instance with connection parameters |
| `app_id` | `str` (optional) | Hashed app identifier for dependency tracking. Required when `map_objects=True` |
| `**kwargs` | | Additional arguments passed to `create_engine()` |

### Session helper <a id="session-helper"></a>

| Helper | What it does |
| ------ | ------------ |
| `create_session()` | Returns a SQLAlchemy `Session`. Use as context manager for automatic cleanup. |

```python
database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # database operations here
    pass
```

### Object dependency tracking <a id="object-dependency-tracking"></a>

When `settings.map_objects` is `True`, the Database class automatically records every table, view, or routine touched during the current run into the `AppDbObjects` table.

| Helper | Purpose | Typical caller |
| ------ | ------- | -------------- |
| `register_models(db_objects)` | Persist metadata rows for each SQLAlchemy model | High-level methods (`bulk_insert`, `merge`, …) |
| `register_sql_objects(objects)` | Persist arbitrary object names (from raw SQL) | `execute()` |
| `_track_models_from_objects(objects)` | Collects distinct ORM model classes from object list | Internal |

### Database class DML methods <a id="database-dml-methods"></a>

#### query <a id="database-query"></a>

```python
query(self, statement: Select, session: Session, map_objects: bool = True) -> Result
```

Executes a SQLAlchemy 2.x `select()` statement with optional dependency tracking. The caller constructs the full `select()` statement — `database.query()` handles execution and model registration.

Returns a `Result` object — use `.scalars().all()` for ORM objects or `.all()` for rows.

| Parameter | Type | Description |
|-----------|------|-------------|
| `statement` | `Select` | A SQLAlchemy 2.x `select()` statement |
| `session` | `Session` | Active SQLAlchemy session |
| `map_objects` | `bool` | Override `settings.map_objects` for this query (default: `True`) |

```python
from sqlalchemy import select
from data_collector.enums import RunStatus
from data_collector.tables import Apps
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Build the statement — full SQLAlchemy 2.x API available
    stmt = select(Apps).where(Apps.last_run.isnot(None))
    records = database.query(stmt, session).scalars().all()

    # Filter with enum — no magic numbers
    stmt = select(Apps).where(Apps.run_status == RunStatus.RUNNING)
    running = database.query(stmt, session).scalars().all()

    # Joins, aggregations, subqueries — all supported
    stmt = (
        select(Apps, Runtime)
        .join(Runtime, Apps.app == Runtime.app_id)
        .where(Apps.run_status == RunStatus.RUNNING)
    )
    results = database.query(stmt, session).all()

    # Skip dependency tracking for a specific query
    stmt = select(Apps)
    records = database.query(stmt, session, map_objects=False).scalars().all()
```

**How it works internally:**
1. Extracts ORM model classes from the `Select` statement
2. If `map_objects=True` and `app_id` is set, registers extracted models in `AppDbObjects`
3. Executes `session.execute(statement)` and returns the `Result`

#### execute <a id="database-execute"></a>

```python
execute(self, executable: DbExecutable, session: Session) -> Result
```

Executes a stored procedure or function using the structured `DbExecutable` dataclass. Automatically registers the routine in `AppDbObjects` when dependency tracking is enabled.

**DbExecutable dataclass:**

```python
from dataclasses import dataclass, field
from data_collector.enums import DbObjectType

@dataclass
class DbExecutable:
    object_name: str                          # Procedure or function name
    object_type: DbObjectType                 # DbObjectType.PROCEDURE or DbObjectType.FUNCTION
    schema: str = None                        # Defaults to public (Postgres) or dbo (MSSQL)
    params: dict = field(default_factory=dict) # Named parameters
```

**DbObjectType enum** (planned module path: `data_collector/enums/database.py`):

```python
class DbObjectType(IntEnum):
    PROCEDURE = 1
    FUNCTION = 2
```

The `execute()` method builds the correct SQL call syntax per database type:

| Database | Procedure | Function |
|----------|-----------|----------|
| PostgreSQL | `CALL schema.proc_name(...)` | `SELECT schema.func_name(...)` |
| MSSQL | `EXEC schema.proc_name ...` | `SELECT schema.func_name(...)` |

**Usage:**

```python
from data_collector.utilities.database.main import Database, DbExecutable
from data_collector.enums import DbObjectType
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Execute a stored procedure
    proc = DbExecutable(
        object_name="sp_refresh_data",
        object_type=DbObjectType.PROCEDURE,
        schema="dbo",
        params={"param1": "value"}
    )
    database.execute(proc, session)

    # Execute a function
    func = DbExecutable(
        object_name="fn_calculate_total",
        object_type=DbObjectType.FUNCTION,
        schema="public"
    )
    result = database.execute(func, session)
```

**Why DbExecutable instead of raw SQL?**
- Type-safe: `DbObjectType` enum eliminates string matching
- No regex parsing: schema, name, and type are already structured
- No SQL injection risk from malformed strings
- Consistent dependency tracking: schema + name + type available without parsing
- Database-agnostic: same dataclass works for PostgreSQL and MSSQL

#### merge <a id="database-merge"></a>

```python
merge(self, objs, session, filters, archive_col='archive', delete=False,
      update=True, stats=False, archive_date=None, logger=None, compare_key='sha') -> Optional[Stats]
```

Core data synchronization method. Compares new objects (from web/API/file) against existing database records using hash-based change detection, then inserts new and archives (or deletes) removed records — all in a single transaction.

**How it works:**
1. Hash each incoming record (via `sha` column or custom `compare_key`)
2. Fetch existing records matching `filters`
3. Compare hash sets via `obj_diff()` to identify inserts and removals
4. Insert new, archive (or delete) removed
5. Commit

**Parameters:**

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `objs` | `T \| List[T]` | — | ORM object(s) from external source |
| `session` | `Session` | — | Active SQLAlchemy session |
| `filters` | `TextClause` | `text('')` | Filter to narrow DB records for comparison |
| `archive_col` | `str` | `'archive'` | Column name for soft-delete timestamp |
| `delete` | `bool` | `False` | Hard-delete removed records instead of archiving |
| `update` | `bool` | `True` | Archive removed records (mutually exclusive with `delete`) |
| `stats` | `bool` | `False` | Return `Stats` dataclass with operation counts |
| `archive_date` | `datetime` | `None` | Custom archive timestamp (default: `datetime.now()`) |
| `logger` | `Logger` | `None` | Override logger to connect events to caller app |
| `compare_key` | `str \| List[str]` | `'sha'` | Column(s) used for uniqueness comparison |

```python
from datetime import datetime
from sqlalchemy import and_
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Data from external source
    new_data = [
        ExampleTable(company_id=100, person_id=123, name="John", surname="Doe", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=100, person_id=234, name="Mary", surname="Jaine", birth_date=datetime(2001, 8, 18)),
    ]

    # Hash all records
    new_data = bulk_hash(new_data)

    # Narrow comparison scope
    filters = and_(ExampleTable.company_id == 100)

    # Sync: inserts new, archives removed, skips unchanged
    stats = database.merge(objs=new_data, session=session, filters=filters, stats=True)
    print(f"Inserted: {stats.inserted}, Archived: {stats.archived}, Total: {stats.number_of_records}")
```

#### delete <a id="database-delete"></a>

```python
delete(self, object_list, session) -> None
```

Hard-deletes ORM objects from the database. Caller must commit afterward.

```python
from sqlalchemy import select
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    to_delete = session.execute(
        select(ExampleTable).where(ExampleTable.company_id == 100)
    ).scalars().all()
    database.delete(to_delete, session)
    session.commit()
```

#### archive <a id="database-archive"></a>

```python
archive(self, object_list, session, archive_col='archive', archive_date=None) -> None
```

Soft-deletes by setting the archive column to a timestamp. Archived records remain in the database for historical queries.

```python
from datetime import datetime
from sqlalchemy import select
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    to_archive = session.execute(
        select(ExampleTable).where(ExampleTable.company_id == 100)
    ).scalars().all()

    # Archive with default timestamp (now)
    database.archive(to_archive, session)

    # Archive with custom timestamp
    database.archive(to_archive, session, archive_date=datetime(2025, 1, 1))
    session.commit()
```

#### bulk_insert <a id="database-bulk-insert"></a>

```python
bulk_insert(self, object_list, session) -> None
```

Inserts ORM objects using `session.add_all()` with dependency tracking. No comparison — straight insert. Caller must commit afterward.

```python
from datetime import datetime
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    data = [
        ExampleTable(company_id=102, person_id=123, name="John", surname="Doe", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=102, person_id=234, name="Mary", surname="Jaine", birth_date=datetime(2001, 8, 18)),
    ]
    data = bulk_hash(data)
    database.bulk_insert(object_list=data, session=session)
    session.commit()
```

#### update_insert <a id="database-update-insert"></a>

```python
update_insert(self, objects, session, filter_cols: list, commit=True) -> Stats
```

Upsert operation: for each object, queries the database using `filter_cols` as the match key. If no match exists, inserts. If a match exists, compares all column values and updates only if something changed.

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `objects` | `T \| List[T]` | — | ORM object(s) to upsert |
| `session` | `Session` | — | Active SQLAlchemy session |
| `filter_cols` | `List[str]` | — | Column names to match on (business key) |
| `commit` | `bool` | `True` | Auto-commit after processing |

```python
from datetime import datetime
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    data = [
        ExampleTable(company_id=100, person_id=123, name="John", surname="Doe-Updated", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=100, person_id=999, name="New", surname="Person", birth_date=datetime(1995, 3, 15)),
    ]
    data = bulk_hash(data)

    # Match on business key — not primary key
    stats = database.update_insert(objects=data, session=session, filter_cols=["company_id", "person_id"])
    print(f"Inserted: {stats.inserted}, Updated: {stats.updated}")
```

**update_insert vs SQLAlchemy's session.merge():**

| Concern | `update_insert` | SQLAlchemy `session.merge()` |
|---------|-----------------|------------------------------|
| Match key | Arbitrary business columns (`filter_cols`) | Primary key only |
| Batch support | Processes lists in one call | Single object at a time |
| Change detection | Only updates if values actually differ | Always copies all attributes |
| Stats | Returns `Stats` with insert/update counts | Returns merged ORM instance |
| Dependency tracking | Integrates with `AppDbObjects` | None |

Use `update_insert` when you need to upsert based on natural/business keys (e.g., "same company + person ID") where the auto-increment PK is unknown. Use SQLAlchemy's `session.merge()` when you already have the primary key.

## DML Method Summary

| Method | Purpose | Compare Key | Commit |
|--------|---------|-------------|--------|
| `merge()` | Hash-based sync: insert new, archive/delete removed | `sha` (default) or custom | Auto |
| `update_insert()` | Upsert: insert or update based on business key match | `filter_cols` | Auto (configurable) |
| `bulk_insert()` | Straight insert, no comparison | None | Caller |
| `delete()` | Hard-delete ORM objects | None | Caller |
| `archive()` | Soft-delete via timestamp column | None | Caller |
| `execute()` | Run stored procedures/functions via `DbExecutable` | N/A | Depends on routine |
| `query()` | Execute `select()` statement with dependency tracking | N/A | Read-only |

## SHAHashableMixin <a id="sha-hashable-mixin"></a>

Mixin class for ORM models that need automatic SHA3-256 hash computation on creation. Models that inherit from `SHAHashableMixin` define which columns to hash via `get_hash_keys()`, and the `sha` column is automatically populated on object construction.

```python
from data_collector.utilities.database.main import SHAHashableMixin

class MyTable(Base, SHAHashableMixin):
    __tablename__ = 'my_table'
    id = auto_increment_column()
    name = mapped_column(String(256))
    value = mapped_column(Integer)

    @staticmethod
    def get_hash_keys() -> list:
        return ['name', 'value']  # Only these columns are hashed

# sha is automatically computed on creation
obj = MyTable(name="test", value=42)
print(obj.sha)  # SHA3-256 hash of name + value
```

## loaders.py — Driver Pre-flight Helpers <a id="loaders-module"></a>

Runtime checks that connector subclasses call before opening a connection. You normally don't call them directly, but knowing what they do helps when deploying to CI/CD or Docker.

| Helper | What it does | Raises on failure |
| ------ | ------------ | ----------------- |
| `check_pyodbc()` | Verifies `pyodbc` is importable before MSSQL connection is attempted | `ImportError` if absent |

> Additional databases can be added by implementing the `BaseDBConnector` interface. The pluggable connector pattern allows extending database support without modifying core framework code.

### When would I import these helpers myself?

* **CI pipeline** — Add a quick `python -c "from data_collector.database.loaders import check_pyodbc; check_pyodbc()"` step to fail fast if drivers are missing.
