[← Back to index](1.%20index.md) | [← Back to utilities](4.%20utilities.md)

# Database package

Database package contains modules that handle any database related action from checking dependencies, 
mapping database objects in real time that framework apps use to the different DML operations.

Current modules are:
- main.py
- loaders.py


## main.py module
Content:
- [Database class](#database-class)
  - [Session helper](#session-helper)
  - [Database class DML methods](#database-dml-methods)
    - [query](#database-query)
    - [execute](#database-execute)
    - [merge](#database-merge)
    - [delete](#database-delete)
    - [archive](#database-archive)
    - [bulk_insert](#database-bulk-insert)
    - [update_insert](#database-update-insert)

## loaders.py module
- [driver pre-flight helper](#loaders-module)

### Database class <a id="database-class"></a>
Initializing this class you will create and instance for communication with database that you 
forwarded to settings argument. 

> Check [3. settings.md](3.%20settings.md) to see how settings class should be configured.

Recommended initialization of Database class 
```python
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
```

Now that we have instance of Database we can now create session with database

### Session helper <a id="session-helper"></a>
| Helper | What it does |
| ------ | ------------ |
| `create_session(name: str \| None = None)` | Context-manager friendly wrapper around `sessionmaker`. Commits on success, rolls back on exception. Optional **name** shows up in connection-pool stats & debug logs. |


```python
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # do things with session
    pass
```

Note: if you work with multiple sessions you should name session with appropriate name to 
distinguish them easier
### Object dependency tracking  <a id="object-dependency-tracking"></a>
When `settings.map_objects` is **True** the helper stack records every
table/view or routine touched during the current run.  

| Helper | Purpose | Typical caller |
| ------ | ------- | -------------- |
| `register_models(*models, session)` | Persist metadata rows for each SQLAlchemy model. | High-level helpers (`bulk_insert`, `merge`, …) |
| `register_sql_objects(obj_names, session)` | Persist arbitrary object names (views, materialised views). | Direct SQL executed with `execute()` |
| `_map_objects(query, models, seen=set())` | Internal inspector that registers tables once per runtime. | `query()` |


### Database class DML methods <a id="database-dml-methods"></a>
##### query (self, session, *models, map_objects=True, track_models=None): <a id="database-query"></a>

Wrapper around session.query() method that enables mapping ORM objects in
realtime if enabled and in same time user performs same operations like with 
original .query() method except forwarding session is needed.

Arguments:
- map_objects - when database.settings.map_objects is True, you can set for this 
specific query to False if you don't want to map the database object
- track_models - is used usually in batch by other database class methods 
preventing mapping same models multiple times reducing unnecessary calls to the database.

Regular use
```python
from sqlalchemy import and_
from data_collector.tables import Apps
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings


database = Database(MainDatabaseSettings())
with database.create_session() as session:
    app_filter = and_(Apps.last_run.isnot(None))
    records = database.query(session, Apps).filter(app_filter)
```

##### execute (self, sql_text, session): <a id="database-execute"></a>
Unlike session.execute(), this class method is intentionally limited to execute
only stored procedures, functions, or package procedures.
Automatically registers used routine in AppDbObjects if database.app_id is specified and 
database.settings.map_objects is set to True.

```python
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    database.execute('call public.sp_testit()', session)
```


##### merge (objs, session, filters, archive_col='archive', delete=False, update=True, stats=False, archive_date=None, logger=None, compare_key='sha') <a id="database-merge"></a>
Merge class method is used for comparing new objects and old objects. 
New objects represent data that is fetched in current runtime and old objects are those that are already existing in database.

**Old objects are separated into two states:**
- active state (archive is null, represents currently last up-to date version of the record/s dataset)
- historical objects (archive is not null, represents previous versions of the records enabling to have history of changes)

**Arguments:**
- **objs** - A single ORM object or list of ORM objects (typically from an external source like an WEB, API, File).
Each object must have a unique field (default: 'sha') for comparison.
- **session** - it is mandatory to provide The SQLAlchemy session to use
- **filters** - SQLAlchemy filter to narrow the selection of records from the database. If not provided, all records from the inferred table will be fetched.
- **archive_col** - The name of the column used to mark a record as archived.
Archived record = history record no longer active on source. Default is 'archive'. Only relevant if `update=True`.
- **delete** - If True, records missing from `objs` will be permanently deleted from the database. Mutually exclusive with `update`.
- **update** - If True, records missing from `objs` will have their `archive_col` updated with the current time or `archive_date`. Mutually exclusive with `delete`.
- **stats** - If True returns Stats class with statistics of how many objects have been inserted, archive or deleted and also inserted objects that now contain id of row in database
- **archive_date** - A specific datetime to use when marking records as archived. If not provided, the current time is used.
- **logger** - Option to forward app logger instead using Database logger. This connects logging events to the caller (the app). Database default loging is sufficient when:
  - We don't need events to be connected to specific app
  - For debugging purposes
- **compare_key** - One or more attribute names (str or list/tuple of str) used to uniquely identify each object. 
The field is used to determine uniqueness (default is 'sha') for comparison between new and existing objects.

```python
from datetime import datetime
from sqlalchemy import (and_)
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Generating some initial data
    initial_data = [
        ExampleTable(company_id=100, person_id=123, name="John", surname="Doe", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=100, person_id=234, name="Mary", surname="Jaine", birth_date=datetime(2001, 8, 18)),
    ]

    # Adding hashing to the list objects
    initial_data = bulk_hash(initial_data)

    # Create filter to narrow down records you want to compare
    filters = and_(ExampleTable.company_id == 100)

    # Using merge to avoid insertion of same records if they already exist
    stats = database.merge(objs=initial_data, session=session, filters=filters, stats=True)

    # Showing results of merge
    database.logger.info(
        f"RESULTS: inserted:{stats.inserted}, archived:{stats.archived}, total records: {stats.number_of_records}")
```

##### delete (object_list, session) <a id="database-delete"></a>
Deletes a list of ORM objects from the database. Caller must commit afterward.

```python
from sqlalchemy import (and_)
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    delete_objs = database.query(session).filter(and_(ExampleTable.company_id == 100))
    database.delete(delete_objs, session)
    session.commit()
```

##### archive (object_list, session, archive_col='archive', archive_date=None) <a id="database-archive"></a>
Updates archive_col on existing DB records with a default timestamp or wit the one defined in archive_date.

```python
from datetime import datetime
from sqlalchemy import (and_)
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    archive_objs = database.query(session).filter(and_(ExampleTable.company_id == 100))
    
    # archiving with default archive timestamp
    database.archive(archive_objs, session)
    
    # archiving with custom timestamp
    my_timestamp = datetime.now()
    database.archive(archive_objs, session, archive_date=my_timestamp)
    session.commit()
```

##### bulk_insert (object_list, session) <a id="database-bulk-insert"></a>
Performs a bulk insert of ORM objects using add_all and enables mapping database table dependencies at the same time.
It doesn't compare data it just performs straight forward insert of new rows.

```python
from datetime import datetime
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Generating some initial data
    initial_data = [
        ExampleTable(company_id=102, person_id=123, name="John", surname="Doe", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=102, person_id=234, name="Mary", surname="Jaine", birth_date=datetime(2001, 8, 18)),
    ]

    # Adding hashing to the list objects
    initial_data = bulk_hash(initial_data)
    
    # Inserting records
    database.bulk_insert(object_list=initial_data, session=session)
```

##### update_insert (objects, session, filter_cols:list) <a id="database-update-insert"></a>
Performs update or insert of row based on whether filter columns values are matched or not.
```python
from datetime import datetime
from sqlalchemy import (and_)
from data_collector.tables import ExampleTable
from data_collector.utilities.database.main import Database
from data_collector.settings.main import MainDatabaseSettings
from data_collector.utilities.functions.runtime import bulk_hash

database = Database(MainDatabaseSettings())
with database.create_session() as session:
    # Generating some initial data
    initial_data = [
        ExampleTable(company_id=100, person_id=123, name="John", surname="Doe-New", birth_date=datetime(1987, 9, 23)),
        ExampleTable(company_id=100, person_id=234, name="Mary", surname="Jaine", birth_date=datetime(2001, 8, 18)),
    ]

    # Adding hashing to the list objects
    initial_data = bulk_hash(initial_data)
    
    # update or insert new row depending on if filtered columns are matched or not
    filter_cols = ['company_id', 'person_id']
    database.update_insert(objects=initial_data, session=session, filter_cols=filter_cols)
```

## loaders.py – driver pre-flight helpers <a id="loaders-module"></a>
It bundles **runtime checks and one-shot client initialisers** that the
engine subclasses call before opening a connection.  
You normally **don’t** call them directly, but knowing what they do helps when
deploying to CI/CD or Docker.

| Helper | What it does | Raises on failure |
| ------ | ------------ | ----------------- |
| `initialize_oracle_client(logger=None) -> bool` | Imports `cx_Oracle`, then calls `cx_Oracle.init_oracle_client(lib_dir= $ORACLE_CLIENT )`. Returns **True** if the client is ready, **False** if the env-var is missing. Logs details via the optional `logger`. | - |
| `check_oracle(logger=None)` | Guarantees that **cx_Oracle is installed** *and* the Instant Client is initialised. Internally calls `initialize_oracle_client`. | `ImportError` if the module is missing; `RuntimeError` if the client cannot be initialised. :contentReference[oaicite:0]{index=0} |
| `check_pyodbc()` | Simple guard that `pyodbc` is import-able before a SQL Server connection is attempted. | `ImportError` if the library is absent. :contentReference[oaicite:1]{index=1} |

### Environment variables

| Variable | Purpose | Example |
| -------- | ------- | ------- |
| `oracle_client` | Absolute path to the Instant Client libraries (`libclntsh.so.*`, `oci.dll`, …). Only needed for Oracle connections. | `/opt/oracle/instantclient_21_13` |

> **Heads-up**: On Alpine-based containers you need the *glibc-compatible*  
> Oracle Instant Client—otherwise `initialize_oracle_client()` fails at load time.

### When would I import these helpers myself?

* **CI pipeline** – Add a quick `python -c "from data_collector.database.loaders import check_pyodbc, check_oracle; check_pyodbc(); check_oracle()"` step to fail fast if drivers are missing.  
* **Custom health-check endpoint** – Call `check_oracle()` to verify Instant Client availability before advertising “ready”.
