[← Back to index](1.%20index.md)

# App Orchestration Manager

**Phase:** 2 (v0.2.0)

## Overview

The orchestration manager is the central process that schedules, monitors, and controls all data collection applications. It is the "brain" of the framework — continuously polling the `Apps` table, launching apps when their `next_run` time arrives, processing commands (start/stop/restart), and handling fatal errors.

Built from the ground up based on enterprise experience managing 70+ apps across 5 countries with 99% uptime.

Developed and tested primarily on Windows, but fully deployable on Linux.

## Architecture Positioning <a id="architecture-positioning"></a>

The Manager is a **process supervisor with dynamic scheduling** — not a workflow orchestrator like Apache Airflow. This is a deliberate architectural choice:

| Capability | Manager | Airflow |
|-----------|---------|---------|
| **App independence** | Each app runs independently — no dependency graph needed | Built around DAGs (directed acyclic graphs) with task dependencies |
| **Dynamic scheduling** | `next_run` set by the app at runtime based on conditions and results | Fixed schedule (cron/timetable) — dynamic rescheduling requires workarounds |
| **Real-time progress** | Apps update `progress%`, `solved`, `failed`, `ETA` in the Apps table during execution | No built-in per-task progress tracking |
| **Process control** | PID tracking, live stop/restart/disable via commands | No clean "kill running task" — executor abstracts processes away |
| **Command dispatch** | Real-time via RabbitMQ (fallback: DB polling) | Web UI or CLI — no real-time command injection |
| **Deployment** | Zero external dependencies — just Python + database | Requires webserver + scheduler + metadata DB + optional Celery/Redis |

**When to use what:**
- **Manager (default):** Independent apps with interval-based or condition-based scheduling, real-time monitoring, command dispatch
- **Airflow (optional integration):** When clients have existing Airflow infrastructure, or when apps require DAG dependency chains (e.g., "run transfer only after scraper finishes")

Apps are **orchestration-agnostic** — the same app code runs under either Manager or Airflow without modification. See [Airflow Integration](#airflow-integration) for the optional operator.

## Architecture

```
┌──────────────────────────────────────────────────────────┐
│                    Manager Process                         │
│                                                            │
│  ┌─────────────┐  ┌──────────────┐  ┌─────────────────┐ │
│  │  Scheduler   │  │  Command     │  │  Process        │ │
│  │  (main loop) │  │  Dispatcher  │  │  Monitor        │ │
│  │              │  │              │  │                 │ │
│  │  Poll apps   │  │  RabbitMQ or │  │  PID tracking   │ │
│  │  Check       │  │  DB polling  │  │  Crash detect   │ │
│  │  next_run    │  │              │  │  Auto-restart   │ │
│  └──────┬───────┘  └──────┬───────┘  └────────┬────────┘ │
│         │                  │                    │          │
│         └──────────────────┴────────────────────┘          │
│                            │                               │
│                    ┌───────┴───────┐                       │
│                    │   Apps Table  │                       │
│                    └───────────────┘                       │
└──────────────────────────────────────────────────────────┘
```

## Apps Table Integration

### Dashboard Columns

The `Apps` table includes columns for real-time progress monitoring:

| Column | Type | Description |
|--------|------|-------------|
| `task_size` | Integer | Total number of items to process in the current run |
| `progress` | Integer (0-100) | Percentage of task completed |
| `solved` | Integer | Number of items successfully processed |
| `failed` | Integer | Number of items that failed processing |
| `eta` | DateTime | Estimated time of completion |

### Run Status State Machine

```
                ┌──── ENABLE ────┐
                ▼                │
         ┌──────────┐    ┌──────┴────┐
  ──────►│NOT_RUNNING│───►│  RUNNING  │
         │   (0)     │    │    (1)    │
         └──────────┘    └─────┬─────┘
                ▲              │
                │         STOP/CRASH
                │              │
                │        ┌─────▼─────┐
                └────────│  STOPPED  │
                 RESTART │    (2)    │
                         └───────────┘
```

### Command Flow

Command dispatch uses `CmdName` IntEnum (values 1-5) and `CmdFlag` IntEnum (values 0-2). Enums are defined in `data_collector/enums/commands.py` (see [1.3. enums.md](1.3.%20enums.md)).

**CmdName (IntEnum):**

| Value | Name | Action |
|-------|------|--------|
| 1 | START | Start the application process |
| 2 | STOP | Gracefully stop the application |
| 3 | RESTART | Stop then start |
| 4 | ENABLE | Re-enable a disabled application |
| 5 | DISABLE | Disable the application (prevents scheduling) |

**CmdFlag (IntEnum):**

| Value | Name | Meaning |
|-------|------|---------|
| 0 | PENDING | Command issued, not yet processed |
| 1 | EXECUTED | Command processed successfully |
| 2 | NOT_EXECUTED | Command could not be executed |

**Flow:**

1. External system (API, dashboard) sets `cmd_name` and `cmd_flag=PENDING` on an app
2. Manager detects pending command during its polling loop
3. Manager executes the command (start process, kill process, etc.)
4. Manager updates `cmd_flag=EXECUTED` and `cmd_exec=datetime.now()`

### Fatal Error Handling

| Fatal Flag | Meaning | Manager Action |
|-----------|---------|---------------|
| 1 - FAILED_TO_START | App process couldn't start | Disable app, send alert |
| 2 - ALERT_SENT | Alert has been sent to operators | Wait for manual intervention |
| 3 - UNEXPECTED_BEHAVIOR | Runtime anomaly detected | Log details, notify |

## Scheduling

### Interval-Based Scheduling
```python
# After successful run:
app.next_run = datetime.now() + timedelta(minutes=app.interval)

# After failed run:
app.next_run = datetime.now() + timedelta(minutes=app.retry_interval)
```

### Cron Expression Support (planned)
```
next_run = cron_next("0 */2 * * *")  # Every 2 hours
```

## Process Management

### Subprocess Spawning
```python
import sys
import json
import subprocess

# Standard run — no arguments
cmd = [sys.executable, "-m", f"apps.{group}.{parent}.{app_name}.main"]

# Targeted run — with arguments (e.g., from RabbitMQ command)
if app_args:
    cmd.extend(["--args", json.dumps(app_args)])

process = subprocess.Popen(
    cmd,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)
app.app_pids = str(process.pid)
app.run_status = RunStatus.RUNNING
```

The app's `init(runtime, args=None)` function parses `--args` via `argparse` (stdlib). See [11. scraping.md — App Arguments](11.%20scraping.md#app-arguments) for the full argument pattern.

> **Security:** Never use `shell=True` — it enables shell injection if app names come from external input. The list form passes arguments directly to the OS without shell interpretation.

> **Virtual environment:** `sys.executable` returns the Python interpreter currently running the Manager process. If the Manager is started from a virtual environment, all child app processes automatically inherit the same venv and its installed dependencies. **Always start the Manager from the correct virtual environment** — this is enforced in the service deployment configurations below.

### Process Lifecycle
1. **Spawn** — start as subprocess, record PID in `Apps.app_pids`
2. **Monitor** — check PID alive periodically, read stdout/stderr
3. **Complete** — update `Runtime` table with execution metrics
4. **Crash** — detect unexpected termination, set `fatal_flag` if repeated

## Service Deployment <a id="service-deployment"></a>

The Manager runs as a long-lived background process. Deploy it as a native service on each platform:

### Windows — Native Windows Service

Use `pywin32` to register the Manager as a proper Windows Service (no third-party wrappers):

```python
import win32serviceutil
import win32service
import win32event

class DataCollectorService(win32serviceutil.ServiceFramework):
    _svc_name_ = "DataCollectorManager"
    _svc_display_name_ = "Data Collector Manager"
    _svc_description_ = "Orchestrates data collection applications"

    def SvcStop(self):
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        self.manager.stop = True
        win32event.SetEvent(self.hWaitStop)

    def SvcDoRun(self):
        from data_collector.manager import init
        init()

if __name__ == '__main__':
    win32serviceutil.HandleCommandLine(DataCollectorService)
```

```cmd
:: Install and manage
python service.py install
python service.py start
python service.py stop

:: Or via sc.exe
sc start DataCollectorManager
sc stop DataCollectorManager
```

### Linux — systemd

```ini
# /etc/systemd/system/data-collector-manager.service
[Unit]
Description=Data Collector Manager
After=network.target postgresql.service

[Service]
Type=simple
User=datacollector
WorkingDirectory=/opt/data-collector
ExecStart=/opt/data-collector/venv/bin/python -m data_collector.manager
Restart=on-failure
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
systemctl enable data-collector-manager
systemctl start data-collector-manager
systemctl status data-collector-manager
journalctl -u data-collector-manager -f    # Live logs
```

### Docker

```yaml
# docker-compose.yml
services:
  manager:
    build: .
    command: python -m data_collector.manager
    restart: unless-stopped
    environment:
      - DC_DB_MAIN_IP=db
      - DC_DB_MAIN_PORT=5432
    depends_on:
      - db
      - rabbitmq
```

## Dramatiq Integration

For long-running task queues (OCR, NER, batch processing), the orchestration layer integrates with **Dramatiq + RabbitMQ**. See [17. rabbitmq.md](17.%20rabbitmq.md) for details.

- Framework provides a **base worker class** for pipeline processing
- **Watched directory entry points** trigger pipeline ingestion
- **Topic-based RabbitMQ routing** dispatches documents to the appropriate Dramatiq worker
- **PipelineTask** table tracks state per document (stage, status, timestamps, errors)

Dramatiq workers run as a **separate service** alongside the Manager — both share the same virtual environment, framework codebase, and database. The Manager handles scheduled app lifecycle; Dramatiq handles asynchronous task queue processing. They communicate via RabbitMQ.

The orchestration manager coordinates both traditional scheduled apps and Dramatiq-based pipeline workers.

## App Groups and Parents

Hierarchical organization:
```
AppGroup: "croatia"
    └── AppParent: "fina" (financial agency)
        ├── App: "companies" (company data scraper)
        ├── App: "jrr_api" (business registry API)
        └── App: "koncesije" (concessions data)
```

Hash identifiers:
- `app_group | app_parent | app_name` → SHA3-256 → `app.app` (64-char hash)
- Used for unique identification across the system

## Runtime Tracking

Each execution creates a `Runtime` record:

```python
import uuid
from data_collector.utilities.functions.math import get_totals, get_totalm, get_totalh

runtime_record = Runtime(
    runtime=uuid.uuid4().hex,              # 32-char unique ID per execution
    app_id=app.app,
    start_time=start,
    end_time=end,
    totals=get_totals(start, end),
    totalm=get_totalm(start, end),
    totalh=get_totalh(start, end),
    except_cnt=exception_count,
    exit_code=process.returncode
)
```

## Airflow Integration (Phase 5) <a id="airflow-integration"></a>

**Phase:** 5 (optional)

For clients with existing Apache Airflow infrastructure, `DataCollectorOperator` (`data_collector/orchestration/airflow.py`) triggers any registered app as an Airflow task. Apps remain orchestration-agnostic — the same code runs under Manager or Airflow without modification.

### DataCollectorOperator

```python
from airflow.models import BaseOperator
from airflow.utils.context import Context

class DataCollectorOperator(BaseOperator):
    """Triggers a Data Collector framework app as an Airflow task."""

    def __init__(self, app_group: str, app_parent: str, app_name: str, **kwargs):
        super().__init__(**kwargs)
        self.app_group = app_group
        self.app_parent = app_parent
        self.app_name = app_name

    def execute(self, context: Context):
        import subprocess, sys
        result = subprocess.run(
            [sys.executable, "-m",
             f"apps.{self.app_group}.{self.app_parent}.{self.app_name}.main"],
            capture_output=True, text=True
        )
        if result.returncode != 0:
            raise Exception(f"App failed: {result.stderr}")
        self.log.info(f"App completed: {self.app_group}/{self.app_parent}/{self.app_name}")
```

### DAG Example

```python
from airflow import DAG
from datetime import datetime
from data_collector.airflow import DataCollectorOperator

with DAG("croatia_fina",
         schedule="*/30 * * * *",
         start_date=datetime(2025, 1, 1),
         catchup=False):

    companies = DataCollectorOperator(
        task_id="companies",
        app_group="croatia", app_parent="fina", app_name="companies"
    )

    jrr_api = DataCollectorOperator(
        task_id="jrr_api",
        app_group="croatia", app_parent="fina", app_name="jrr_api"
    )

    koncesije = DataCollectorOperator(
        task_id="koncesije",
        app_group="croatia", app_parent="fina", app_name="koncesije"
    )

    # Airflow adds DAG dependency management
    [companies, jrr_api] >> koncesije   # Run in parallel, then koncesije
```

### What the App Still Handles

When triggered by Airflow, the app still:
- Generates its own `runtime` UUID and creates a `Runtime` record
- Binds `app_id`, `runtime`, `function_id` to the structlog logger
- Logs to DB, Splunk, console via the logging pipeline
- Updates `progress`, `solved`, `failed`, `ETA` in the Apps table

Airflow provides scheduling and DAG dependency management. The framework handles everything else.

### When to Use Airflow

| Scenario | Recommendation |
|----------|---------------|
| Independent apps, simple interval scheduling | **Manager** — zero dependencies, dynamic `next_run` |
| Apps with conditional scheduling ("run only if data exists") | **Manager** — apps set their own `next_run` based on conditions |
| Client already has Airflow infrastructure | **Airflow** — plug in via `DataCollectorOperator` |
| Apps with strict dependency chains (A must finish before B) | **Airflow** — native DAG dependency support |
| Need web UI for DAG visualization | **Airflow** — built-in web dashboard |
| Real-time progress monitoring and command dispatch | **Manager** — Apps table + RabbitMQ commands |

## Dependencies

- **Core Framework** (Phase 1) — Database, Settings, Logging, Tables
- **RabbitMQ** (Phase 4) — for distributed command dispatch (optional, falls back to DB polling) and Dramatiq task queues
- **Notification System** (Phase 4) — for alert notifications via pluggable channels (Telegram, Teams, Slack, email, webhooks) — optional
- **Apache Airflow** (Phase 5, optional) — for `DataCollectorOperator` integration. Only needed when deploying into existing Airflow infrastructure. Apps run without Airflow by default.
