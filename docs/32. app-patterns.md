[← Back to index](1.%20index.md) | [← First App](31.%20first-app.md)

# App Patterns & Best Practices

## App Design Patterns

### Single-Source Scraper
Collects data from one external source and syncs to one table.

```
Source (website/API) → Parse → Hash → merge() → Database
```

### Multi-Source Aggregator
Collects from multiple sources into a single table, using filters to scope each source.

```python
# Each source syncs independently with its own filter
for source in sources:
    data = collect_from(source)
    bulk_hash(data)
    with db.create_session() as session:
        db.merge(data, session, filters=and_(Table.source == source.name))
```

### Transfer Pipeline
Reads from one database and writes to another (ETL transfer).

```python
source_db = Database(SourceSettings())
target_db = Database(TargetSettings())

with source_db.create_session() as src_session:
    records = source_db.query(src_session, SourceTable).all()
    # Transform records to target format
    target_records = [transform(r) for r in records]

bulk_hash(target_records)
with target_db.create_session() as tgt_session:
    target_db.merge(target_records, tgt_session)
```

## Database Best Practices

### Session Lifecycle

Always use sessions as context managers:

```python
# Good
with database.create_session() as session:
    database.merge(objects, session)

# Avoid
session = database.create_session()
database.merge(objects, session)
session.close()  # Easy to forget
```

### Choosing the Right Method

| Scenario | Method | Description |
|----------|--------|-------------|
| Full sync with source | `merge()` | Inserts new, archives/deletes removed |
| Insert only (no comparison) | `bulk_insert()` | Fast insert, no change detection |
| Update existing or insert new | `update_insert()` | Match by filter columns, update changed |
| Remove specific records | `delete()` | Permanent deletion |
| Mark records as historical | `archive()` | Set archive timestamp |

### Filter Design for merge()

**Be specific.** Filters define the comparison scope:

```python
# Good: compare against this company's data
filters = and_(Companies.country == "HR", Companies.source == "registry")

# Dangerous: compare against ALL companies in the table
# This will archive records from other countries!
filters = text("")  # Empty filter = full table
```

### Cross-Database Patterns

Access multiple databases in the same app:

```python
main_db = Database(MainDatabaseSettings())          # Framework DB
warehouse_db = Database(WarehouseSettings())         # Data warehouse

# Read from main, write to warehouse
with main_db.create_session() as src:
    data = main_db.query(src, SourceTable).all()

with warehouse_db.create_session() as tgt:
    warehouse_db.bulk_insert(transformed_data, tgt)
    tgt.commit()
```

## HTTP Request Patterns

### Centralized Request Class

The `Request` class (`data_collector/utilities/request.py`) provides centralized HTTP operations based on httpx, supporting both synchronous and asynchronous methods:

```python
from data_collector.request import Request

# Synchronous usage — timeout is a constructor parameter (transport config)
request = Request(timeout=30, retries=3)
response = request.get("https://api.example.com/data")
data = response.json()
```

### Async HTTP via httpx

For high-throughput API consumers, use async HTTP to perform concurrent requests:

```python
import asyncio
from data_collector.request import Request

async def fetch_all(urls: list[str]) -> list[dict]:
    request = Request(timeout=30, retries=3)
    tasks = [request.async_get(url) for url in urls]
    responses = await asyncio.gather(*tasks, return_exceptions=True)
    return [r.json() for r in responses if not isinstance(r, Exception)]

data = asyncio.run(fetch_all(urls))
```

### Sync vs Async Guidelines

| Operation | Approach | Reason |
|-----------|----------|--------|
| HTTP requests | Sync or Async (via `Request`) | httpx supports both; use async for concurrent I/O |
| Database operations | **Synchronous only** | SQLAlchemy sessions are sync for v1.0; keeps transaction logic simple |
| Mixed workloads | Async HTTP + sync DB | Fetch data asynchronously, then write to DB synchronously |

## Error Handling

### Retry Pattern

The `Request` class handles retries internally — configure via constructor:

```python
from data_collector.request import Request

# Retries, backoff, and retry-on-status are transport config
request = Request(timeout=30, retries=3, backoff_factor=2, retry_on_status=[429, 500, 502, 503, 504])
response = request.get(url)

# Check if we should stop after repeated failures
if request.should_abort(logger, proxy_on=False):
    logger.error("Aborting — too many failures")
```

### Fatal vs Recoverable Errors

| Error Type | Action | Example |
|-----------|--------|---------|
| **Recoverable** | Retry, then skip | Network timeout, HTTP 503 |
| **Data error** | Log and continue | Missing field, parse failure |
| **Fatal** | Stop execution | Authentication failure, schema mismatch |

```python
try:
    data = parse_page(html)
except ParseError as e:
    logger.warning("Skipping record", error=str(e))
    continue  # Recoverable — skip this record
except AuthenticationError as e:
    logger.error("Authentication failed", error=str(e))
    raise  # Fatal — stop execution
```

## Performance

### Connection Pool Tuning

Default: `pool_size=20`, `max_overflow=0`. Adjust for high-concurrency apps:

```python
database = Database(settings, pool_size=50, max_overflow=10)
```

### Batch Size for merge()

For large datasets, process in batches:

```python
BATCH_SIZE = 5000

for i in range(0, len(all_records), BATCH_SIZE):
    batch = all_records[i:i + BATCH_SIZE]
    bulk_hash(batch)
    with database.create_session() as session:
        database.merge(batch, session, filters=filters)
    logger.info("Processed batch", batch=i // BATCH_SIZE + 1)
```

### Hash Performance

- `make_hash()` is CPU-bound. For millions of records, consider pre-computing hashes during parsing.
- `sha3_256` (default) is secure but slower than `md5`. For internal deduplication where security isn't needed, use `constructor='md5'`.

## Logging Best Practices

### Log Level Guidelines

| Level | When to use |
|-------|------------|
| `DEBUG` | Detailed diagnostic info (record counts, timing, data samples) |
| `INFO` | Routine operations (start, complete, batch progress) |
| `WARNING` | Unexpected but handled situations (retry, duplicate key, missing optional field) |
| `ERROR` | Failures that affect a single record or operation |
| `CRITICAL` | Failures that stop the entire app |

### Structured Context

With structlog, pass key-value pairs directly — no `extra` dict needed:

```python
logger.info("Record processed", record_count=len(records), source="registry_api")

# Bind context once — all subsequent logs inherit it
logger = logger.bind(app_id=app_hash, runtime=runtime)
logger.info("Starting collection")  # app_id and runtime auto-included
```

## Testing

### Unit Testing ORM Models

```python
def test_company_hash():
    c1 = Companies(name="Acme", country="HR", reg_number="123")
    c2 = Companies(name="Acme", country="HR", reg_number="123")
    make_hash(c1, inplace=True)
    make_hash(c2, inplace=True)
    assert c1.sha == c2.sha  # Same data = same hash

    c3 = Companies(name="Different", country="HR", reg_number="123")
    make_hash(c3, inplace=True)
    assert c1.sha != c3.sha  # Different data = different hash
```

### Integration Testing with Test Database

Use a separate settings class pointing to a test database:

```python
class TestDatabaseSettings(DatabaseSettings):
    username: str = "test_user"
    password: str = "test_pass"
    database_name: str = "test_data_collector"
    ip: str = "localhost"
    port: int = 5432
    database_type: DatabaseType = DatabaseType.POSTGRES
    database_driver: DatabaseDriver = DatabaseDriver.POSTGRES

test_db = Database(TestDatabaseSettings())
```
