[‚Üê Back to index](1.%20index.md)

# Monitoring & Observability

## Overview

Observability strategy for the Data Collector framework, covering structured logging, metrics collection, health checks, alerting, and dashboards.

## Three Pillars

### 1. Logs

`LoggingService` ([5. logging.md](5.%20logging.md)) provides structured log output to multiple sinks:

| Sink | Purpose |
|------|---------|
| Console | Development debugging |
| File (rotating) | Local log retention |
| Database | Queryable log history |
| Splunk HEC | Centralized log aggregation |

**Structured Log Fields (fixed keys mapped to Logs table columns):**
```json
{
  "timestamp": "2025-01-15T07:30:00.123Z",
  "log_level": "ERROR",
  "app_id": "a1b2c3d4e5f6...",
  "runtime": "a3f8c2d1e5b647f0912345678abcdef0",
  "function_id": "f9e8d7c6b5a4...",
  "module_name": "scraping.base",
  "function_name": "fetch_page",
  "msg": "Connection timeout after 30s",
  "context_json": {"url": "https://example.com", "attempt": 3}
}
```

### 2. Metrics

Key metrics to collect per app and system-wide:

**App Metrics:**
| Metric | Type | Description |
|--------|------|-------------|
| `app.runtime_seconds` | histogram | Duration of each app run |
| `app.records_collected` | counter | Records fetched per run |
| `app.records_merged` | counter | Records written to database |
| `app.records_archived` | counter | Records archived per run |
| `app.errors_total` | counter | Error count per run |
| `app.fatal_count` | counter | Fatal events |

**System Metrics:**
| Metric | Type | Description |
|--------|------|-------------|
| `system.cpu_percent` | gauge | CPU utilization |
| `system.memory_percent` | gauge | Memory utilization |
| `system.disk_usage_percent` | gauge | Storage usage |
| `system.db_connections_active` | gauge | Open database connections |
| `system.queue_size` | gauge | Logging queue depth |
| `system.running_apps` | gauge | Currently executing apps |

### 3. Traces

For distributed tracing across multi-server deployments:

```
Manager ‚Üí RabbitMQ ‚Üí Worker ‚Üí Database
   ‚îÇ                    ‚îÇ         ‚îÇ
   trace_id‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Use `runtime` as the natural trace/correlation ID ‚Äî it's already unique per execution and propagated through all log entries.

## Splunk Integration

### Splunk HEC (HTTP Event Collector)

`SplunkHECHandler` (`data_collector/utilities/log/handlers.py`) sends log records to Splunk:

```python
# Configured via settings
DC_LOG_SPLUNK_ENABLED=true
DC_LOG_SPLUNK_URL=https://splunk.example.com:8088
DC_LOG_SPLUNK_TOKEN=your-hec-token
```

### Splunk Searches

**Find errors for a specific app:**
```spl
index=data_collector app="scraper_company_registry" level="ERROR"
| table _time, message, exc_type
```

**Runtime duration trends:**
```spl
index=data_collector source="runtime"
| stats avg(duration_seconds) as avg_runtime by app
| sort -avg_runtime
```

**Fatal event timeline:**
```spl
index=data_collector level="CRITICAL" OR fatal_flag="FATAL"
| timechart count by app
```

### Splunk Dashboards

| Dashboard | Panels |
|-----------|--------|
| **Operations Overview** | Running apps, recent failures, system health |
| **App Performance** | Runtime durations, record counts, error rates |
| **Data Quality** | Merge statistics, archive rates, anomalies |
| **Infrastructure** | CPU, memory, disk, DB connections |

## Health Checks

### Application Health

**Unauthenticated** (`GET /health`) ‚Äî minimal response, no version or internal details:
```json
{
    "status": "healthy"
}
```

**Authenticated** (`GET /api/v1/system/health`) ‚Äî full component breakdown:
```json
{
    "status": "healthy",
    "version": "0.5.0",
    "uptime_seconds": 86400,
    "checks": {
        "database": {
            "status": "up",
            "latency_ms": 12
        },
        "rabbitmq": {
            "status": "up",
            "latency_ms": 5
        },
        "disk": {
            "status": "warning",
            "usage_percent": 85.2
        }
    }
}
```

See [24. rest-api.md ‚Äî Security Hardening](24.%20rest-api.md#response-security) for rationale on splitting authenticated vs unauthenticated health endpoints.

### Health Check Thresholds

| Check | Healthy | Warning | Unhealthy |
|-------|---------|---------|-----------|
| DB latency | < 100ms | 100-500ms | > 500ms or unreachable |
| Disk usage | < 80% | 80-90% | > 90% |
| Queue depth | < 1000 | 1000-5000 | > 5000 |
| Memory | < 80% | 80-90% | > 90% |
| Failed apps (last hour) | 0 | 1-3 | > 3 |

## Alerting

### Alert Channels

The framework uses a pluggable notification system supporting multiple channels (Telegram, Teams, Slack, email, webhooks):

| Channel | Use Case |
|---------|----------|
| **Notification system** ([18. notifications.md](18.%20notifications.md)) | Real-time alerts for fatal events, system issues (Telegram, Teams, Slack, email, webhooks) |
| **Splunk alerts** | Scheduled searches triggering email/webhook |
| **API WebSocket** | Dashboard notifications |

### Alert Rules

| Alert | Condition | Severity | Channel |
|-------|-----------|----------|---------|
| App fatal | `fatal_flag = FATAL` | Critical | Notification system |
| Repeated failures | 3+ consecutive failed runs | High | Notification system |
| DB unreachable | Health check fails 3x | Critical | Notification system |
| High disk usage | > 90% | Warning | Notification system |
| Low record count | < 50% of average | Warning | Splunk |
| No runs in window | No successful run in 24h | High | Notification system |
| Queue backlog | Log queue > 5000 | Warning | Console |

### Alert Message Format

```
üî¥ FATAL: scraper_company_registry
Runtime: a3f8c2d1e5b647f0912345678abcdef0
Error: ConnectionError - Connection refused
Consecutive failures: 3
Action: App disabled, manual intervention required
```

## Key Queries

### Operational Queries

**Apps that haven't run in 24 hours:**
```sql
SELECT app, next_run, run_status
FROM apps
WHERE next_run < NOW() - INTERVAL '24 hours'
  AND command_flag != 'STOP';
```

**Top 10 slowest apps (last 7 days):**
```sql
SELECT app,
       AVG(EXTRACT(EPOCH FROM (ended - started))) as avg_seconds,
       COUNT(*) as run_count
FROM runtime
WHERE started > NOW() - INTERVAL '7 days'
GROUP BY app
ORDER BY avg_seconds DESC
LIMIT 10;
```

**Error rate by app (last 24 hours):**
```sql
SELECT r.app,
       COUNT(*) as total_runs,
       COUNT(*) FILTER (WHERE r.runtime_code = 'ERROR') as errors,
       ROUND(100.0 * COUNT(*) FILTER (WHERE r.runtime_code = 'ERROR') / COUNT(*), 1) as error_pct
FROM runtime r
WHERE r.started > NOW() - INTERVAL '24 hours'
GROUP BY r.app
ORDER BY error_pct DESC;
```

## Logging Best Practices

1. **Always include `app` and `runtime`** in log context for correlation
2. **Use appropriate log levels**: DEBUG for development, INFO for normal operations, WARNING for recoverable issues, ERROR for failures, CRITICAL for fatal events
3. **Structure log messages** ‚Äî include relevant IDs, counts, and durations
4. **Avoid logging sensitive data** ‚Äî no passwords, tokens, or PII in log messages
5. **Set log rotation** ‚Äî prevent disk exhaustion from verbose logging

## Pipeline Monitoring

### PipelineTask Table Queries

Monitor document processing pipelines by querying the `PipelineTask` table. See [1.2. data-model.md](1.2.%20data-model.md#pipelinetask) for column definitions and [17.1. dramatiq.md](17.1.%20dramatiq.md#pipelinetask-state-tracking) for pipeline architecture.

**Stuck documents (no progress for over 1 hour):**
```sql
SELECT id, current_stage, status, date_modified
FROM pipeline_task
WHERE status = 'in_progress'
  AND date_modified < NOW() - INTERVAL '1 hour';
```

**Throughput by stage (last 24 hours):**
```sql
SELECT current_stage,
       COUNT(*) as completed,
       AVG(EXTRACT(EPOCH FROM (end_time - start_time))) as avg_seconds
FROM pipeline_task
WHERE status = 'completed'
  AND end_time > NOW() - INTERVAL '24 hours'
GROUP BY current_stage
ORDER BY current_stage;
```

**Stage bottlenecks (longest average processing time):**
```sql
SELECT current_stage,
       COUNT(*) as total,
       AVG(EXTRACT(EPOCH FROM (end_time - start_time))) as avg_seconds,
       MAX(EXTRACT(EPOCH FROM (end_time - start_time))) as max_seconds
FROM pipeline_task
WHERE status = 'completed'
  AND end_time > NOW() - INTERVAL '7 days'
GROUP BY current_stage
ORDER BY avg_seconds DESC;
```

## FunctionLog Monitoring

The `@fun_watch` decorator logs per-function performance data to the `FunctionLog` table. Function metadata (name, filepath) is stored in the `AppFunctions` registry table and referenced via `function_hash`. See [25. fun-watch.md](25.%20fun-watch.md) for decorator details and [1.2. data-model.md](1.2.%20data-model.md#function-tracking) for table definitions.

**Registered function inventory:**
```sql
SELECT af.function_name, af.filepath, af.app_id, af.first_seen, af.last_seen
FROM app_functions af
ORDER BY af.last_seen DESC;
```

**Per-function performance trends (last 7 days):**
```sql
SELECT af.function_name,
       DATE(fl.start_time) as day,
       COUNT(*) as calls,
       AVG(fl.totals) as avg_seconds,
       SUM(fl.solved) as total_solved,
       SUM(fl.failed) as total_failed
FROM function_log fl
JOIN app_functions af ON fl.function_hash = af.function_hash
WHERE fl.start_time > NOW() - INTERVAL '7 days'
GROUP BY af.function_name, DATE(fl.start_time)
ORDER BY af.function_name, day;
```

**Failure ratios by function (last 30 days):**
```sql
SELECT af.function_name,
       COUNT(*) as total_calls,
       SUM(fl.solved) as total_solved,
       SUM(fl.failed) as total_failed,
       ROUND(100.0 * SUM(fl.failed) / NULLIF(SUM(fl.task_size), 0), 1) as failure_pct
FROM function_log fl
JOIN app_functions af ON fl.function_hash = af.function_hash
WHERE fl.start_time > NOW() - INTERVAL '30 days'
GROUP BY af.function_name
ORDER BY failure_pct DESC;
```

## Dependencies

- [5. logging.md](5.%20logging.md) ‚Äî Logging system architecture
- [18. notifications.md](18.%20notifications.md) ‚Äî Notification system (pluggable channels)
- [24. rest-api.md](24.%20rest-api.md) ‚Äî Health check endpoints
