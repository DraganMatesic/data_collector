[← Back to index](1.%20index.md)

# Frequently Asked Questions

## General

### What is Data Collector?

An enterprise Python framework for automated data collection, ETL, web scraping, API integration, OCR processing, and application orchestration. It provides a unified platform for managing dozens of data ingestion applications across multiple sources and databases.

See [0. product-overview.md](0.%20product-overview.md) for full details.

### Who is this for?

Data engineers and backend developers who need to build and manage multiple data collection applications. The framework handles the cross-cutting concerns (database connectivity, logging, scheduling, error handling) so developers can focus on business logic.

### What databases are supported?

PostgreSQL (via psycopg2) and Microsoft SQL Server (via pyodbc). PostgreSQL is the recommended database for new deployments. The `BaseDBConnector` is pluggable, so additional database backends (such as Oracle) can be added by implementing the connector interface.

### What Python version is required?

Python 3.13 or higher.

### Why no Oracle support?

PostgreSQL and MSSQL cover the primary use cases for the framework. The database layer uses a pluggable `BaseDBConnector` pattern, so Oracle support can be added in the future by implementing a new connector class without changes to application code. This keeps the core dependency footprint lean.

### What platforms are supported?

Developed and tested primarily on Windows, but fully deployable on Linux including Docker containers and cloud environments.

---

## Core Framework

### How does `merge()` work?

`merge()` synchronizes a list of ORM objects with the database using SHA-256 hashing:

1. Compute a hash for each record based on its business columns
2. Compare hashes against existing records in the database
3. **New hashes** → INSERT new records
4. **Missing hashes** → Archive or DELETE records no longer in source
5. **Matching hashes** → No action (data unchanged)

This provides idempotent, change-detection-based data synchronization. See [4.1. database.md](4.1.%20database.md).

### What is the difference between `archive` and `delete` mode in merge?

- **Archive mode** (`method="archive"`): Sets the `archive` timestamp on removed records, preserving history. Records with `archive IS NULL` are current; records with a timestamp are historical.
- **Delete mode** (`method="delete"`): Permanently removes records no longer in the source data.

Use archive mode when you need audit trails. Use delete mode for transient data where history isn't needed.

### How do I add a new database table?

1. Create a model class extending `Base` from `data_collector.tables.shared`
2. Define columns using SQLAlchemy 2.x `mapped_column()` syntax
3. Add `SHAHashableMixin` if the table will use `merge()`
4. Register it with `Deploy.create_tables()`

See [6. tables.md](6.%20tables.md) for a complete guide.

### Why Pydantic for settings instead of a config file?

Pydantic settings provide:
- **Type validation** at startup (fail fast on misconfiguration)
- **Environment variable** support out of the box (12-factor app)
- **IDE autocompletion** and type checking
- **Default values** with override capability

See [3. settings.md](3.%20settings.md).

---

## Logging

### Why structlog?

The framework uses structlog as the logging frontend (wrapping stdlib `logging` via `structlog.stdlib.BoundLogger`). Key benefits:
- **Bound context** — `app_id`, `runtime`, `function_id` are attached once and auto-included in every log entry
- **Structured output** — key-value pairs in development (`console` format), JSON in production (`json` format)
- **Layered binding** — identifiers bound at the correct lifecycle point (app_id = static, runtime = per cycle, function_id = per function, instance_id = per thread)

The queue-based architecture is preserved: structlog → `QueueHandler` → `QueueListener` (background thread) → `RouterHandler` → sinks. Application code never blocks on log writes.

See [5. logging.md](5.%20logging.md).

### How do I add a custom log handler?

Create a class extending `logging.Handler` and pass it to `RouterHandler`. **Critical rule:** handlers must NOT catch their own exceptions — let them propagate to `RouterHandler`, which logs failures to a fallback `error.log` file.

```python
class MyHandler(logging.Handler):
    def emit(self, record):
        # Your custom logic — NO try/except here
        # RouterHandler handles failures and writes to error.log
        pass
```

See [5. logging.md](5.%20logging.md) for the full handler contract and `RouterHandler` fallback mechanism.

---

## Data Collection

### How are HTTP requests handled?

The framework provides a centralized `Request` class (`data_collector/utilities/request.py`) based on httpx, supporting both synchronous and asynchronous HTTP operations. The constructor takes transport config only (timeout, retries, backoff_factor). Session state is managed via setter methods: `set_headers()`, `set_cookies()`, `set_auth()`, `set_proxy()`, `reset_headers()`, `reset_cookies()`. This enables runtime flexibility — headers/cookies often change mid-scraper (after login, CAPTCHA solve, token refresh). See [4.4. request.md](4.4.%20request.md) for the full API and [11. scraping.md](11.%20scraping.md) for usage patterns.

### How do I handle websites that require login?

Use session-based scraping with the `Request` class to maintain cookies across requests. The scraping framework ([11. scraping.md](11.%20scraping.md)) provides session management with automatic cookie persistence.

### How do I deal with CAPTCHAs?

The framework integrates with AntiCaptcha for automated captcha resolution. You send the CAPTCHA image or site key, and receive the solution. See [12. captcha.md](12.%20captcha.md).

### Can I run multiple scrapers concurrently?

Yes. The concurrency module ([16. concurrency.md](16.%20concurrency.md)) provides `ThreadPoolExecutor`-based patterns for parallel data collection within a single app. The orchestration system ([10. orchestration.md](10.%20orchestration.md)) runs multiple apps as separate processes.

### Why not use Scrapy?

Framework scrapers are **targeted data collectors** — they query known registry numbers/IDs, fetch specific pages, and store structured results. Scrapy is designed for **web crawling** (link-following discovery), uses Twisted (not asyncio/httpx), has its own data pipeline that bypasses the framework's database layer, and conflicts with Manager subprocess orchestration. The custom `Request` class + `BaseScraper` provides framework integration (error tracking, progress monitoring, `RequestMetrics`, proxy management) that Scrapy can't offer without extensive customization. See [11. scraping.md](11.%20scraping.md).

### How do I create a new app?

Use the scaffold CLI to generate the standard 3-file app structure:

```bash
python -m data_collector scaffold --group germany --parent bundesbank --name interest_rates --type single
```

This creates `main.py` (BaseScraper class + `init()` entry point), `parser.py` (parsing functions), `tables.py` (ORM models), and auto-registers the app in the Apps table. Use `--type threaded` for multi-threaded apps. See [11. scraping.md](11.%20scraping.md).

### How do runtime arguments work?

Apps accept optional arguments via `init(runtime, args=None)` for targeted collection, monitoring mode, or ad-hoc runs. Arguments arrive as a `dict | None` from three sources:

| Source | Mechanism |
|--------|-----------|
| Manager subprocess | `--args '{"company_id":"123"}'` CLI flag |
| RabbitMQ command | Args dict in message payload |
| Direct execution | `sys.argv` parsing |

`BaseScraper` stores `self.args` and `prepare_list()` uses it to filter queries. See [11. scraping.md](11.%20scraping.md).

---

## Hashing & Change Detection

### Why SHA-256 for change detection?

SHA-256 provides:
- **Deterministic** — same input always produces same hash
- **Collision-resistant** — practically impossible for different data to produce the same hash
- **Fixed-size** — 64-character hex string regardless of input size
- **Fast** — native Python `hashlib` implementation

This makes it ideal for detecting whether source data has changed between collection runs.

### Which columns should I include in the hash?

Include all **business data columns** — the fields that, if changed, mean the record has been updated. Exclude:
- Primary keys / surrogate keys
- Timestamps (`date_created`, `archive`)
- The hash column itself
- Any framework-managed columns

See [4.2. hashing.md](4.2.%20hashing.md).

---

## Deployment

### How do I set up the database schema?

Use the `Deploy` class:

```python
from data_collector.tables.deploy import Deploy

deploy = Deploy(database)
deploy.create_tables()      # Create all registered tables
deploy.populate_tables()    # Seed codebook data
```

See [7. deployment.md](7.%20deployment.md).

### Can I use Docker?

Yes. See [40. deployment-infra.md](40.%20deployment-infra.md) for Dockerfiles, Docker Compose configurations, and production deployment patterns.

### How do I manage secrets in production?

Options ranked by complexity:
1. **Environment variables** — simplest, suitable for Docker/Kubernetes
2. **`.env` files** — local development (never commit to git)
3. **`SecretLoader`** — AES-256-CBC encrypted file for deployments without a vault
4. **HashiCorp Vault / Cloud KMS** — enterprise secret management

See [2. secrets.md](2.%20secrets.md) and [2.1. secret_loader.md](2.1.%20secret_loader.md).

---

## Architecture

### Why is `Database` a single class instead of separate repositories?

The `Database` class acts as a **Unit of Work** — it wraps SQLAlchemy session management and provides consistent patterns (merge, bulk_insert, query) that all applications use. This ensures:
- Consistent error handling and session cleanup
- Automatic dependency tracking via `AppDbObjects`
- Unified merge/hash/archive pattern across all apps

Future versions may extract some concerns (see [1.1. architecture.md](1.1.%20architecture.md) for planned decoupling).

### How is `app_id` determined?

`app_id` is derived automatically from the file path using `get_app_info(__file__, only_id=True)`. It extracts the last 4 path components (`app_group`, `app_parent`, `app_name`, `module_name`) and hashes `app_group|app_parent|app_name` via SHA-256. Convention over configuration — the same file always produces the same `app_id`. Never hardcode `make_hash("group|parent|name")` directly. See [4.3. functions.md](4.3.%20functions.md).

### What is the relationship between Apps, Runtime, and Logs?

```
Apps (registry)
 └── Runtime (one per execution)
      └── Logs (many per runtime)
```

- **Apps**: Static registry of all applications with scheduling config
- **Runtime**: One record per app execution, tracking start/end time and status
- **Logs**: Individual log entries linked to a specific runtime

See [1.2. data-model.md](1.2.%20data-model.md).

### Is `@fun_watch` thread-safe?

Yes. Each invocation creates its own closure-local counters (`solved`, `failed`, `start_time`, `end_time`, `task_size`) — not stored on `self` or shared state. Multiple threads calling the same decorated method get independent counters. Each thread writes its own `FunctionLog` row with `thread_id = threading.get_ident()`. The `Apps` table update (live dashboard progress) is performed only by the main thread to avoid race conditions. See [25. fun-watch.md](25.%20fun-watch.md).

### Why not use Celery or Airflow for task orchestration?

The framework uses a custom Manager/Scheduler because:
- **Subprocess isolation** — each app runs in its own process, preventing memory leaks from affecting other apps
- **Database-driven scheduling** — `next_run` in the Apps table allows dynamic schedule changes without redeployment
- **Command dispatch** — the `command_flag` pattern enables start/stop/reset without a message broker dependency
- **Real-time progress** — live tracking of task_size, solved, failed, ETA per app
- **Simplicity** — no additional infrastructure required for single-server deployments

**Why not Celery?** Dramatiq is used instead for pipeline task processing — simpler API, no global state, built-in middleware, better error handling. Celery's complexity isn't justified for the framework's use case.

**Why not Airflow?** Apps are independent (no DAG dependencies needed), and the Manager handles dynamic `next_run` scheduling based on runtime conditions — something Airflow doesn't do natively. For clients with existing Airflow infrastructure, an optional `DataCollectorOperator` is planned for Phase 5. Apps are orchestration-agnostic — same code runs under Manager or Airflow.

See [10. orchestration.md](10.%20orchestration.md).

### How do pipelines work?

Pipelines use a two-layer dispatch model: event producers (WatchService, scrapers, API endpoints) write to the `Events` table in the database. The `TaskDispatcher` polls the Events table for unprocessed events and dispatches them to Dramatiq workers via RabbitMQ with topic-based routing. Each document is tracked via a `PipelineTask` record (stage, status, timestamps, errors). Developers build domain-specific Dramatiq actors (PDF extraction, OCR, NER) following the worker patterns in [17.1. dramatiq.md](17.1.%20dramatiq.md#worker-patterns). See also [17.2. watchdog.md](17.2.%20watchdog.md) for file system monitoring and [17. rabbitmq.md](17.%20rabbitmq.md) for message broker infrastructure.

---

## Troubleshooting

### My app keeps going into FATAL state

The fatal detection system disables apps after repeated consecutive failures. To recover:
1. Check logs for the root cause: `SELECT * FROM logs WHERE app_id = 'your_app_hash' AND log_level >= 40 ORDER BY date_created DESC`
2. Fix the underlying issue
3. Reset the fatal flag: update `fatal_flag` to `NO_FATAL` in the Apps table
4. Restart the app

### merge() is slow with large datasets

Tips for optimizing merge performance:
- Use `bulk_hash()` before merge to pre-compute hashes in memory
- Increase database `work_mem` for large sort operations
- Consider partitioning large tables by date
- Use `delete` mode instead of `archive` if history isn't needed (avoids UPDATE overhead)

### Logs aren't appearing in Splunk

Check:
1. `DC_LOG_SPLUNK_ENABLED=true` is set
2. `DC_LOG_SPLUNK_URL` points to the correct HEC endpoint (including port 8088)
3. `DC_LOG_SPLUNK_TOKEN` is a valid HEC token
4. Network allows outbound HTTPS to the Splunk server
5. The QueueListener is running (check for "Logging service started" in console output)
