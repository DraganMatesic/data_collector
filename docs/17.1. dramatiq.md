[← Back to index](1.%20index.md)

# Dramatiq Task Processing

**Phase:** 4 (v0.4.0)

## Overview

Dramatiq is a task processing framework used for long-running operations that must run outside the main application process — OCR extraction, NER entity recognition, batch document processing, and other pipeline tasks. RabbitMQ serves as the message broker (no Redis option — RabbitMQ is already in the stack and provides dead-letter queues, topic routing, and message durability).

**Relationship to RabbitMQ:** RabbitMQ ([17. rabbitmq.md](17.%20rabbitmq.md)) provides the message infrastructure. Dramatiq is a consumer framework that uses RabbitMQ as its broker to route and execute tasks.

**Dependencies:** `dramatiq[rabbitmq]`, `pika`

Developed and tested primarily on Windows, but fully deployable on Linux.

## Architecture

The system uses a two-layer dispatch model. Event producers (WatchService, scrapers, API endpoints, manual inserts) write events to a database table. The TaskDispatcher polls that table and publishes messages to RabbitMQ, where Dramatiq workers consume and process them:

```
  Event Producers              Events Table           TaskDispatcher          RabbitMQ / Dramatiq
  (WatchService [17.2],        (database)             (polling loop)          (workers)
   API, manual INSERT)
       │                           │                       │                       │
       ▼                           ▼                       ▼                       ▼
  Write event ──► INSERT ──► Unprocessed ──► Poll + ──► Create message
  to Events       row with   events with    batch      and publish to
  table           app_path   no dispatch    dispatch   correct exchange
                                status                  via routing key
                                                             │
                                                             ▼
                                                        Dramatiq Workers
                                                        process documents
                                                        through pipeline
                                                        stages (PREPARE →
                                                        EXTRACT → PROCESS)
```

### Service Separation

Dramatiq workers run as a **separate service** alongside the Manager process. Both share the same virtual environment, framework code, and database. Communication flows through RabbitMQ:

```
┌──────────────────┐          ┌──────────────────┐
│  Manager Process  │          │  Dramatiq Workers │
│  (orchestration)  │          │  (task processing) │
│                    │          │                    │
│  Spawns apps      │  ──────► │  Consumes tasks    │
│  Handles commands │ RabbitMQ │  Runs pipelines    │
│  Monitors health  │  ◄────── │  Updates state     │
└──────────────────┘          └──────────────────┘
         │                              │
         └──────────┬───────────────────┘
                    │
              ┌─────▼─────┐
              │  Database   │
              │  (shared)   │
              └─────────────┘
```

## Why Dramatiq over Celery

| Criteria | Dramatiq | Celery |
|----------|----------|--------|
| API simplicity | Simple decorator-based actors | Complex configuration, global state |
| Error handling | Built-in retry middleware with backoff | Manual retry configuration |
| Broker support | RabbitMQ native (no Redis needed) | RabbitMQ or Redis |
| Dead-letter queues | Automatic DLQ per queue | Requires manual setup |
| Middleware | Composable middleware pipeline | Signal-based hooks |
| Global state | No global state — actors are stateless | Global `app` object |
| Dependencies | Minimal | Heavy dependency tree |

## Declarative Queue Topology

Queue topology is defined declaratively via dataclasses in `topic/` modules. This provides a single source of truth for all exchanges, queues, and routing keys — code drives RabbitMQ state, not the other way around.

### Dataclasses

```python
from dataclasses import dataclass, field
from pika.exchange_type import ExchangeType

@dataclass(frozen=True)
class TopicExchange:
    """RabbitMQ topic exchange definition."""
    name: str
    durable: bool = True
    exchange_type: ExchangeType = ExchangeType.topic
    arguments: dict = field(default_factory=dict)

@dataclass(frozen=True)
class TopicExchangeQueue:
    """Queue bound to a topic exchange with a routing key."""
    name: str
    actor_name: str
    durable: bool = True
    exchange_name: str = ""
    routing_key: str = ""
    actor_path: str = ""       # Module path for AST validation

@dataclass(frozen=True)
class RegularQueue:
    """Standard queue without exchange routing."""
    name: str
    durable: bool = True
    actor_name: str = ""
    actor_path: str = ""       # Module path for AST validation
```

### Pre-Defined Exchanges and Queues

```python
# topic/base.py — shared infrastructure

UNROUTABLE_EXCHANGE = TopicExchange(
    name="dc_unroutable",
    exchange_type=ExchangeType.fanout
)

OCR_TOPIC_EXCHANGE = TopicExchange(
    name="dc_ocr_topic",
    exchange_type=ExchangeType.topic,
    arguments={"alternate-exchange": UNROUTABLE_EXCHANGE.name}
)

PDF_EXTRACT = TopicExchangeQueue(
    name="dc_pdf_extract",
    actor_name="process_pdf",
    exchange_name=OCR_TOPIC_EXCHANGE.name,
    routing_key="ocr.pdf.extract",
    actor_path="data_collector.pipeline.workers.pdf_processor"
)

MANAGER_QUEUE = RegularQueue(
    name="dc_manager_commands",
    actor_name="process_command",
    actor_path="data_collector.pipeline.workers.commands"
)

DEAD_LETTERS_QUEUE = RegularQueue(
    name="dc_dead_letters",
    actor_name="log_dead_letter",
    actor_path="data_collector.pipeline.workers.dead_letters"
)
```

The `alternate-exchange` on `OCR_TOPIC_EXCHANGE` ensures no message is silently lost — messages with no matching routing key are forwarded to `UNROUTABLE_EXCHANGE` for inspection.

### Country/Domain-Specific Topics

Each document type or country gets its own topic module defining pipeline-specific queues:

```python
# topic/croatia.py — Croatian document pipelines

from .base import OCR_TOPIC_EXCHANGE, TopicExchangeQueue

EOGLASNA_PREPARE = TopicExchangeQueue(
    name="dc_eoglasna_prepare",
    actor_name="eoglasna_prepare",
    exchange_name=OCR_TOPIC_EXCHANGE.name,
    routing_key="ocr.new.eoglasna",
    actor_path="data_collector.country.processing.document_ocr.workers.ocr"
)

EOGLASNA_PROCESS = TopicExchangeQueue(
    name="dc_eoglasna_process",
    actor_name="eoglasna_process",
    exchange_name=OCR_TOPIC_EXCHANGE.name,
    routing_key="ocr.extracted.eoglasna",
    actor_path="data_collector.country.processing.document_ocr.workers.ocr"
)
```

**Adding a new pipeline:** Create a new topic file (e.g., `topic/germany.py`), define `TopicExchangeQueue` constants with unique routing keys, and implement the corresponding Dramatiq actors. The broker management class auto-discovers and applies the topology.

## Dramatiq Broker Management

The `DramatiqBroker` class manages the full broker lifecycle — initialization, exchange/queue declaration, binding synchronization, and topology self-healing:

### Broker Initialization

```python
from dramatiq.brokers.rabbitmq import RabbitmqBroker
import dramatiq

class DramatiqBroker:
    """Manages Dramatiq broker lifecycle and RabbitMQ topology."""

    def __init__(self, settings: RabbitMQSettings, load: bool = False):
        self.settings = settings
        self.broker = self._create_broker()
        dramatiq.set_broker(self.broker)

        if load:
            self._declare_exchanges()
            self._sync_bindings()
            self._clean_dead_queues()

    def _create_broker(self) -> RabbitmqBroker:
        credentials = pika.PlainCredentials(
            self.settings.rabbit_username,
            self.settings.rabbit_password
        )
        params = pika.ConnectionParameters(
            host=self.settings.rabbit_host,
            port=self.settings.rabbit_port,
            credentials=credentials
        )
        return RabbitmqBroker(parameters=[params])
```

When `load=True`, the broker declares all exchanges, synchronizes bindings (adding missing, removing stale), and removes orphan queues. This is run on startup to ensure RabbitMQ topology matches code.

### AST-Based Actor Validation

Before binding a queue, the broker validates that the referenced actor actually exists in the target module — without importing it (avoids circular imports and side effects):

```python
import ast
import importlib.util

def _actor_exists(self, actor_path: str, actor_name: str) -> bool:
    """Validate that an actor function exists via AST inspection."""
    spec = importlib.util.find_spec(actor_path)
    if spec is None or spec.origin is None:
        return False

    with open(spec.origin, "r") as f:
        tree = ast.parse(f.read())

    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef) and node.name == actor_name:
            return True
    return False
```

This prevents broken queue bindings pointing to non-existent actors — a common source of silent message loss in production.

### Topology Self-Healing

On startup, the broker compares code-defined queues against RabbitMQ broker state and removes orphan queues (queues in RabbitMQ that are no longer defined in code):

```python
def _clean_dead_queues(self):
    """Remove queues from RabbitMQ that no longer exist in code."""
    active_queues = {q.name for q in self._get_all_queues()}
    broker_queues = self._get_broker_queues()  # RabbitMQ Management API

    for queue_name in broker_queues:
        if queue_name.startswith("dc_") and queue_name not in active_queues:
            self._delete_queue(queue_name)
            logger.warning("Removed orphan queue", queue=queue_name)
```

Only queues with the `dc_` prefix are managed — this prevents accidental deletion of queues belonging to other applications.

### Binding Synchronization

Bindings are synchronized per exchange — stale bindings are removed and missing bindings are created:

```python
def _sync_bindings(self):
    """Ensure RabbitMQ bindings match code definitions."""
    queues = self._get_all_queues()
    by_exchange = defaultdict(list)
    for q in queues:
        if isinstance(q, TopicExchangeQueue):
            by_exchange[q.exchange_name].append(q)

    for exchange_name, exchange_queues in by_exchange.items():
        existing = self._get_exchange_bindings(exchange_name)
        desired = {(q.name, q.routing_key) for q in exchange_queues}

        # Remove stale bindings
        for binding in existing:
            key = (binding["destination"], binding["routing_key"])
            if key not in desired:
                self._unbind(exchange_name, *key)

        # Create missing bindings
        existing_keys = {(b["destination"], b["routing_key"]) for b in existing}
        for q in exchange_queues:
            if (q.name, q.routing_key) not in existing_keys:
                if self._actor_exists(q.actor_path, q.actor_name):
                    self._bind(exchange_name, q.name, q.routing_key)
```

### Message Creation and Publishing

```python
def create_message(self, queue_name: str, actor_name: str,
                   args: tuple = (), kwargs: dict | None = None,
                   options: dict | None = None) -> dramatiq.Message:
    """Create a Dramatiq message for a given actor."""
    return dramatiq.Message(
        queue_name=queue_name,
        actor_name=actor_name,
        args=args,
        kwargs=kwargs or {},
        options=options or {}
    )

def publish(self, message: dramatiq.Message,
            exchange_name: str = "", routing_key: str = ""):
    """Publish message to RabbitMQ with persistent delivery."""
    channel = self.broker.connection.channel()
    channel.basic_publish(
        exchange=exchange_name,
        routing_key=routing_key,
        body=message.encode(),
        properties=pika.BasicProperties(delivery_mode=2)  # Persistent
    )
```

`delivery_mode=2` ensures messages survive RabbitMQ restarts.

## Topic-Based Pipeline Routing

Each document type maps to a multi-stage pipeline via routing keys. Routing keys follow the pattern `ocr.{stage}.{document_type}`:

```
Routing Key                    Queue                      Actor
───────────                    ─────                      ─────
ocr.new.eoglasna          →   dc_eoglasna_prepare    →   eoglasna_prepare
ocr.pdf.extract           →   dc_pdf_extract         →   process_pdf
ocr.extracted.eoglasna    →   dc_eoglasna_process    →   eoglasna_process
```

### Multi-Stage Pipeline Chaining

Each actor publishes to the next stage after completing its work. The next stage is determined by pre-defined constants in the topic modules or by database records that store the next actor's queue/routing info:

```
Stage 1: PREPARE               Stage 2: EXTRACT            Stage 3: PROCESS
(eoglasna_prepare)              (process_pdf)               (eoglasna_process)

  Extract PDFs from ZIP    →     OCR / text extraction  →    Parse entities (regex, NER)
  Normalize filenames            Save extracted text          Form structured datasets
  Write PdfExtractTasks          Write ExtractedText          Store to domain tables
  Publish to EXTRACT             Publish to PROCESS           Mark event complete
```

This provides isolation (failures in one stage don't affect others), independent scaling per stage, and clear pipeline observability.

## TaskDispatcher (Database-Driven Dispatch)

The TaskDispatcher is the central dispatch service that bridges database events to Dramatiq messages. It polls the `Events` table for unprocessed events and publishes messages to the correct exchange/queue based on the event's metadata.

### Why Database-Driven Dispatch

| Property | DB-Driven (TaskDispatcher) | Direct (watchdog → Dramatiq) |
|----------|--------------------------|------------------------------|
| Transactional | Event creation and dispatch tracking in same DB | File event and message publish are separate |
| Resumable | Unprocessed events survive any crash | Message lost if publish fails after file event |
| Auditable | `EventProcessingStatus` tracks exactly what was dispatched when | No dispatch tracking |
| Cross-platform | No OS-specific file system APIs | Platform-dependent observer behavior |
| Decoupled | Any process can write events (scrapers, API, watcher, manual) | Only watchdog produces events |

### TaskDispatcher Class

```python
class TaskDispatcher:
    """Polls Events table and dispatches to Dramatiq actors via RabbitMQ."""

    def __init__(self, database: Database, broker: DramatiqBroker,
                 batch_size: int = 100, poll_interval: int = 10):
        self.database = database
        self.broker = broker
        self.batch_size = batch_size
        self.poll_interval = poll_interval
        self.yield_per = max(1, batch_size // 10)

    def run(self):
        """Main dispatch loop — runs until stopped."""
        while not self._stop_event.is_set():
            dispatched = self._dispatch_batch()
            if dispatched == 0:
                self._stop_event.wait(timeout=self.poll_interval)

    def _dispatch_batch(self) -> int:
        """Find and dispatch unprocessed events. Returns count dispatched."""
        with self.database.create_session() as session:
            # COUNT first — avoid fetching rows when queue is empty
            count = session.execute(
                select(func.count())
                .select_from(Events)
                .outerjoin(EventProcessingStatus,
                    Events.id == EventProcessingStatus.event_id)
                .where(
                    Events.archive == None,
                    Events.app_path != None,
                    EventProcessingStatus.id == None    # Not yet dispatched
                )
            ).scalar()

            if count == 0:
                return 0

            # Fetch batch with yield_per for memory efficiency
            events = session.execute(
                select(Events)
                .outerjoin(EventProcessingStatus,
                    Events.id == EventProcessingStatus.event_id)
                .where(
                    Events.archive == None,
                    Events.app_path != None,
                    EventProcessingStatus.id == None
                )
                .limit(self.batch_size)
            ).yield_per(self.yield_per).scalars()

            dispatched = 0
            for event in events:
                if self._dispatch_event(event, session):
                    dispatched += 1
            return dispatched

    def _dispatch_event(self, event, session) -> bool:
        """Dispatch a single event to the correct Dramatiq actor."""
        # Dynamic import of topic module to get queue definition
        module = importlib.import_module(event.app_path)
        queue_def = module.MainExchangeQueue  # TopicExchangeQueue constant

        message = self.broker.create_message(
            queue_name=queue_def.name,
            actor_name=queue_def.actor_name,
            args=(event.id,)
        )
        self.broker.publish(
            message,
            exchange_name=queue_def.exchange_name,
            routing_key=queue_def.routing_key
        )

        # Track dispatch in EventProcessingStatus
        session.add(EventProcessingStatus(
            event_id=event.id,
            actor_name=queue_def.actor_name,
            dispatched_at=datetime.now()
        ))
        session.commit()
        return True
```

### Entry Point

```python
# Planned module path: data_collector/pipeline/task_dispatcher/main.py

def init(runtime, args=None):
    database = Database(MainDatabaseSettings())
    service = LoggingService("task_dispatcher", db_engine=database.engine)
    logger = service.configure_logger()

    app_id = get_app_info(__file__, only_id=True)
    logger = logger.bind(app_id=app_id, runtime=runtime)

    broker = DramatiqBroker(RabbitMQSettings(), load=True)
    dispatcher = TaskDispatcher(database, broker,
        batch_size=settings.dispatcher_batch_size,
        poll_interval=settings.dispatcher_poll_interval)

    dispatcher.run()
    service.stop()
```

## WatchService (Event Production)

For file system monitoring via directory watching, see [17.2. watchdog.md](17.2.%20watchdog.md). The WatchService monitors directories for new files and writes events to the `Events` table. The TaskDispatcher (above) polls that table and handles Dramatiq dispatch.

## Worker Patterns

The framework provides two categories of Dramatiq actors: **generic infrastructure actors** (document-type agnostic, reusable) and **domain-specific actors** (business logic per document type/country).

### Generic Infrastructure Actor

The `process_pdf` actor is document-type agnostic — it reads configuration from the database record to determine OCR settings and the next actor to chain to:

```python
@dramatiq.actor(queue_name="dc_pdf_extract", max_retries=3,
                on_retry_exhausted="log_dead_letter")
def process_pdf(event_id: int):
    """Generic PDF text extraction — config-driven, document-type agnostic."""
    database = Database(MainDatabaseSettings())

    with database.create_session() as session:
        task = session.execute(
            select(PdfExtractTasks)
            .where(PdfExtractTasks.event_id == event_id,
                   PdfExtractTasks.processed == None)
        ).scalar_one_or_none()

        if task is None:
            return  # Already processed (idempotent)

        # Extract text: pdfplumber if readable, OCR if 90%+ images
        for pdf_path in task.file_paths:
            text = extract_text(pdf_path, ocr_config=task.ocr_config)
            session.add(ExtractedText(
                event_id=event_id,
                file_path=pdf_path,
                content=text
            ))

        task.processed = datetime.now()
        session.commit()

    # Chain to next stage — actor info stored in the task record
    if task.next_actor_name:
        broker = DramatiqBroker(RabbitMQSettings())
        message = broker.create_message(
            queue_name=task.next_queue_name,
            actor_name=task.next_actor_name,
            args=(event_id,)
        )
        broker.publish(message,
            exchange_name=task.next_exchange_name,
            routing_key=task.next_routing_key)
```

### Domain-Specific Actors

Country/document-specific actors contain business logic:

```python
# Planned file path: data_collector/country/processing/document_ocr/workers/ocr.py

@dramatiq.actor(queue_name=EOGLASNA_PREPARE.name, max_retries=3,
                on_retry_exhausted="log_dead_letter")
def eoglasna_prepare(event_id: int):
    """Stage 1: Prepare eOglasna documents for OCR processing."""
    database = Database(MainDatabaseSettings())

    with database.create_session() as session:
        # Check if already processed (idempotent)
        existing = session.execute(
            select(EventProcessingStatus)
            .where(EventProcessingStatus.event_id == event_id,
                   EventProcessingStatus.actor_name == "eoglasna_prepare")
        ).scalar_one_or_none()
        if existing:
            return

        event = session.get(Events, event_id)

        # Extract PDFs from ZIP, normalize filenames
        pdf_paths = extract_and_normalize(event.file_path)

        # Write PdfExtractTasks for the generic PDF actor
        for pdf_path in pdf_paths:
            session.add(PdfExtractTasks(
                event_id=event_id,
                file_path=pdf_path,
                ocr_config={"lang": ["hr"], "dpi": 300},
                next_queue_name=EOGLASNA_PROCESS.name,
                next_actor_name=EOGLASNA_PROCESS.actor_name,
                next_exchange_name=EOGLASNA_PROCESS.exchange_name,
                next_routing_key=EOGLASNA_PROCESS.routing_key
            ))

        # Track processing status
        session.add(EventProcessingStatus(
            event_id=event_id,
            actor_name="eoglasna_prepare",
            dispatched_at=datetime.now()
        ))
        session.commit()

    # Publish to PDF extraction stage
    broker = DramatiqBroker(RabbitMQSettings())
    message = broker.create_message(
        queue_name=PDF_EXTRACT.name,
        actor_name=PDF_EXTRACT.actor_name,
        args=(event_id,)
    )
    broker.publish(message,
        exchange_name=PDF_EXTRACT.exchange_name,
        routing_key=PDF_EXTRACT.routing_key)


@dramatiq.actor(queue_name=EOGLASNA_PROCESS.name, max_retries=3,
                on_retry_exhausted="log_dead_letter")
def eoglasna_process(event_id: int):
    """Stage 3: Parse extracted text into structured data."""
    database = Database(MainDatabaseSettings())

    with database.create_session() as session:
        texts = session.execute(
            select(ExtractedText)
            .where(ExtractedText.event_id == event_id,
                   ExtractedText.parsed == None)
        ).scalars().all()

        for text_record in texts:
            # Regex extraction (fast, primary)
            data = regex_extract(text_record.content)

            # NER fallback (when regex misses entities)
            if not data.get("company_name"):
                data.update(ner_extract(text_record.content, model="company_hr"))

            # Store structured output
            records = build_orm_objects(data)
            for record in records:
                session.merge(record)

            text_record.parsed = datetime.now()
        session.commit()
```

### Dead-Letter Actor

Messages that exhaust all retry attempts are forwarded to the dead-letter actor for logging and alerting:

```python
@dramatiq.actor(queue_name="dc_dead_letters")
def log_dead_letter(message_data: dict):
    """Log failed messages for manual inspection and reprocessing."""
    database = Database(MainDatabaseSettings())

    with database.create_session() as session:
        session.add(DeadLetter(
            queue_name=message_data.get("queue_name"),
            actor_name=message_data.get("actor_name"),
            args=json.dumps(message_data.get("args", [])),
            error=message_data.get("error"),
            traceback=message_data.get("traceback"),
            created_at=datetime.now()
        ))
        session.commit()

    # Send notification alert
    logger.error("Dead letter received",
        queue=message_data.get("queue_name"),
        actor=message_data.get("actor_name"),
        error=message_data.get("error"))
```

## PipelineTask State Tracking

The `PipelineTask` table tracks every document through the pipeline (see [1.2. data-model.md](1.2.%20data-model.md) for the full data model):

| Column | Type | Description |
|--------|------|-------------|
| `id` | String (SHA hash) | Unique task identifier |
| `worker` | String | Dramatiq actor/queue name |
| `file_path` | String | Source document path |
| `stage` | Integer (FK) | Current pipeline stage (`PipelineStage` IntEnum) |
| `status` | Integer (FK) | Current status (`PipelineStatus` IntEnum) |
| `created_at` | DateTime | When the task was created |
| `started_at` | DateTime | When processing began |
| `completed_at` | DateTime | When processing finished |
| `error_message` | Text | Last error message (if failed) |
| `retry_count` | Integer | Number of retry attempts |

**PipelineStatus** and **PipelineStage** are defined as IntEnum in the planned module path: `data_collector/enums/pipeline.py` (see [1.3. enums.md](1.3.%20enums.md)).

## Retry and Dead-Letter Handling

### Retry Middleware

Dramatiq's built-in `Retries` middleware handles transient failures with configurable backoff:

```python
import dramatiq
from dramatiq.middleware import Retries

dramatiq.get_broker().add_middleware(Retries(
    max_retries=3,
    min_backoff=1_000,      # 1 second
    max_backoff=300_000,    # 5 minutes
    retry_when=should_retry  # Custom retry predicate
))
```

The `should_retry` predicate determines which exceptions are retryable:

| Exception Type | Retryable | Reasoning |
|---------------|-----------|-----------|
| `ConnectionError`, `TimeoutError` | Yes | Transient network issues |
| `OCRProcessingError` | Yes | OCR engine may recover on retry |
| `ValidationError` | No | Business rule failure — retry won't help |
| `FileNotFoundError` | No | Source file missing — permanent failure |

Per-actor retry configuration via decorator overrides the global setting:

```python
@dramatiq.actor(queue_name="dc_pdf_extract", max_retries=3,
                on_retry_exhausted="log_dead_letter")
```

### Dead-Letter Queues

Messages that exhaust all retry attempts are automatically routed to the dead-letter queue (`*.DQ`) or forwarded to a named dead-letter actor via `on_retry_exhausted`:

```
dc_pdf_extract              → log_dead_letter actor
dc_eoglasna_prepare         → log_dead_letter actor
dc_eoglasna_process         → log_dead_letter actor
```

Dead-letter messages retain the full payload (event_id, error trace, original queue/actor) for manual inspection and reprocessing. The `PipelineTask` record is updated to `FAILED` status with the final error message.

### Manual Reprocessing

Failed tasks can be reprocessed by:
1. Fixing the underlying issue (source file, configuration, service availability)
2. Resetting the `EventProcessingStatus` record for the failed actor
3. The TaskDispatcher will re-dispatch the event on its next poll cycle

## Configuration

### Dramatiq Workers

| Setting | Default | Description |
|---------|---------|-------------|
| `dramatiq_workers` | `4` | Number of worker threads per process |
| `dramatiq_processes` | `1` | Number of worker processes |
| `dramatiq_max_retries` | `3` | Maximum retry attempts per task |
| `dramatiq_min_backoff` | `1000` | Minimum retry backoff in milliseconds |
| `dramatiq_max_backoff` | `300000` | Maximum retry backoff in milliseconds |

### TaskDispatcher

| Setting | Default | Description |
|---------|---------|-------------|
| `dispatcher_batch_size` | `100` | Events to process per batch |
| `dispatcher_poll_interval` | `10` | Seconds between polls when queue is empty |

WatchService configuration is documented in [17.2. watchdog.md](17.2.%20watchdog.md).

Dramatiq workers are started as a separate service:

```bash
# Start workers for all pipeline queues
dramatiq data_collector.pipeline.actors -p 2 -t 4

# Start workers for specific queue only
dramatiq data_collector.pipeline.actors -Q dc_pdf_extract -p 1 -t 2
```

## Dependencies

- **dramatiq[rabbitmq]** — Task queue framework with RabbitMQ broker
- **pika** — Direct RabbitMQ management (exchange declaration, binding sync, Management API calls)
- RabbitMQ infrastructure ([17. rabbitmq.md](17.%20rabbitmq.md))
- WatchService event production ([17.2. watchdog.md](17.2.%20watchdog.md))
- PDF & OCR utilities ([14. pdf-ocr.md](14.%20pdf-ocr.md))
- NER utilities ([21. deep-learning-ner.md](21.%20deep-learning-ner.md))

