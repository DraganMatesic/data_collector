[← Back to index](1.%20index.md) | [← Back to utilities](4.%20utilities.md)

# Hashing & Data Comparison

## Overview

SHA-based change detection is the core data synchronization pattern in Data Collector. Every data record gets a deterministic hash. The `merge()` method compares hashes between incoming and existing data to compute inserts and removals — without scanning every column.

**Source:** `data_collector/utilities/functions/runtime.py`

## Supporting Types <a id="supporting-types"></a>

### UnicodeForm Enum

Defines the Unicode normalization form applied to string values before hashing. Lives in `data_collector/enums/hashing.py`.

```python
from enum import StrEnum

class UnicodeForm(StrEnum):
    NFC  = 'NFC'    # Canonical decomposition → canonical composition (default)
    NFD  = 'NFD'    # Canonical decomposition
    NFKC = 'NFKC'   # Compatibility decomposition → canonical composition
    NFKD = 'NFKD'   # Compatibility decomposition
```

| Form | Effect | Use when |
|------|--------|----------|
| `NFC` | Compose characters into single codepoints | Default — standard form for databases and web content |
| `NFD` | Decompose into base + combining characters | Rarely needed; some text processing libraries prefer this |
| `NFKC` | Like NFC but also resolves compatibility characters | Source data contains ligatures (`ﬁ` → `fi`), fullwidth chars (`Ａ` → `A`), or superscripts (`²` → `2`) |
| `NFKD` | Like NFD but also resolves compatibility characters | Same as NFKC but decomposed output |

**Why `StrEnum`?** `UnicodeForm` maps directly to `unicodedata.normalize()` form names — the string value _is_ the identity. No codebook table needed.

### UnicodeParams Dataclass

Controls Unicode normalization behavior. Passed as a single parameter to `make_hash()` instead of spreading Unicode concerns across multiple arguments.

```python
from dataclasses import dataclass
from typing import Literal
from data_collector.enums import UnicodeForm

@dataclass
class UnicodeParams:
    form: UnicodeForm = UnicodeForm.NFC
    errors: Literal['strict', 'ignore', 'replace', 'xmlcharrefreplace'] = 'strict'
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `form` | `UnicodeForm` | `NFC` | Normalization form passed to `unicodedata.normalize()` |
| `errors` | `Literal[...]` | `'strict'` | Error handling for encoding — passed to `str.encode(errors=...)` |

**Error handling options** (standard Python codec error handlers, constrained via `Literal`):

| Value | Behavior |
|-------|----------|
| `'strict'` | Raise `UnicodeEncodeError` on unencodable characters (default — surfaces data quality issues) |
| `'ignore'` | Silently drop unencodable characters |
| `'replace'` | Replace unencodable characters with `?` |
| `'xmlcharrefreplace'` | Replace with XML character references (`&#xxxx;`) |

**How it's used internally:**

```python
import unicodedata

# 1. Normalize form
value = unicodedata.normalize(unicode_params.form, value)

# 2. Encode with error handling, then decode back to clean string
value = value.encode('utf-8', errors=unicode_params.errors).decode('utf-8')
```

### Usage Examples

```python
from data_collector.utilities.functions.runtime import make_hash, UnicodeParams
from data_collector.enums import UnicodeForm

# Default — NFC normalization, strict errors (recommended)
make_hash(data)

# Explicit NFC (same as default)
make_hash(data, unicode=UnicodeParams())

# NFKC — resolve compatibility characters (ligatures, fullwidth, superscripts)
make_hash(data, unicode=UnicodeParams(form=UnicodeForm.NFKC))

# Tolerant mode — ignore unencodable characters from dirty sources
make_hash(data, unicode=UnicodeParams(errors='ignore'))

# Disable Unicode normalization entirely
make_hash(data, unicode=None)
```

**When to use `NFKC` over `NFC`:**

```python
# NFC keeps compatibility characters as-is
make_hash({"name": "ﬁnance"})           # "ﬁ" stays as ligature U+FB01

# NFKC decomposes them — "ﬁ" becomes "fi"
make_hash({"name": "ﬁnance"}, unicode=UnicodeParams(form=UnicodeForm.NFKC))
# Now "ﬁnance" and "finance" produce the same hash
```

**When to use `errors='ignore'`:**

```python
# Source has surrogate pairs or invalid sequences from broken encoding
data = {"name": "Company\ud800Name"}   # Invalid surrogate

# strict (default) — raises UnicodeEncodeError → surfaces the data quality issue
make_hash(data)  # raises

# ignore — drops the invalid character, hashes "CompanyName"
make_hash(data, unicode=UnicodeParams(errors='ignore'))
```

## make_hash() <a id="make-hash"></a>

Creates a hash value from any input data (dict, object, or string).

```python
from data_collector.utilities.functions.runtime import make_hash
```

### Arguments

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data` | dict \| str \| object | required | Input data to hash |
| `constructor` | str | `'sha3_256'` | Hash algorithm name (any from `hashlib`) |
| `on_keys` | list[str] | None | Hash only these keys/attributes |
| `exclude_keys` | list[str] | None | Exclude these keys from hash |
| `sort_keys` | bool | True | Sort dict keys before hashing |
| `inplace` | bool | False | Update the object's hash attribute in-place |
| `hash_col_name` | str | `'sha'` | Name of the hash column/attribute |
| `normalize_case` | bool | True | Convert all string values to lowercase |
| `no_spacing` | bool | True | Remove all whitespace from string values |
| `unicode` | UnicodeParams \| None | `UnicodeParams()` | Unicode normalization config. `None` to disable |
| `regex` | re.Pattern \| list[re.Pattern] | None | Compiled regex pattern(s) applied to string values before hashing |

### Hash Pipeline

```
Input (dict/object/string)
    │
    ▼
object_to_dict() — convert to plain dict
    │
    ▼
Remove existing hash column (prevents self-referential hash)
    │
    ▼
Apply on_keys / exclude_keys filters
    │
    ▼
Sort keys (if enabled)
    │
    ▼
Apply regex patterns (strip matched content from string values)
    │
    ▼
Normalize Unicode (UnicodeParams.form), case (lowercase), spacing (remove whitespace)
    │               ↳ applied recursively to nested dicts/lists
    ▼
JSON serialize → encode (UnicodeParams.errors) → hash_func.update() → hexdigest()
```

### Examples

**Hash a dictionary:**
```python
data = {"name": "John Doe", "company_id": 100}
hash_value = make_hash(data)
# Returns: "a1b2c3..." (64-char hex string)
```

**Hash with specific keys only:**
```python
data = {"name": "John", "surname": "Doe", "age": 35, "internal_id": 999}
hash_value = make_hash(data, on_keys=["name", "surname"])
# Only "name" and "surname" contribute to the hash
```

**Hash in-place (update object):**
```python
from data_collector.tables import ExampleTable

obj = ExampleTable(company_id=100, name="John")
make_hash(obj, inplace=True)
# obj.sha is now set to the computed hash
```

**Hash a string:**
```python
hash_value = make_hash("croatia|fina|companies")
# Hashes the normalized string directly
```

**Strip HTML tags before hashing:**
```python
import re

html_strip = re.compile(r'<[^>]+>')
data = {"content": "<b>Important</b> notice", "title": "Alert"}
hash_value = make_hash(data, regex=html_strip)
# Hashes {"content": "important notice", "title": "alert"} (tags stripped, then normalized)
```

**Multiple regex patterns:**
```python
import re

strip_html = re.compile(r'<[^>]+>')
strip_special = re.compile(r'[^\w\s]')
data = {"name": "Acme™ Corp.", "desc": "<p>Leading firm</p>"}
hash_value = make_hash(data, regex=[strip_html, strip_special])
# Patterns applied in order: HTML tags stripped first, then special characters
```

### Normalization

All normalization is applied **recursively** — nested dicts, lists, and deeply nested string values are all normalized consistently. This prevents false hash differences caused by formatting inconsistencies at any depth.

#### Unicode Normalization (`unicode=UnicodeParams()`)

Unicode has multiple byte representations for visually identical characters. Without normalization, the same name can produce different hashes depending on how the source encoded it.

```python
# NFC (composed) vs NFD (decomposed) — same visual character, different bytes
"č" == "\u010D"          # NFC: single codepoint
"č" == "c" + "\u030C"    # NFD: base + combining character

# Without unicode normalization: different hashes for "Matešić" depending on source encoding
# With unicode=UnicodeParams() (default NFC): both forms → same hash
```

Pass `unicode=None` to disable Unicode normalization entirely. See [UnicodeParams](#supporting-types) for form selection and error handling options.

#### Case Normalization (`normalize_case=True`)

`"John DOE"` → `"john doe"`

#### Whitespace Removal (`no_spacing=True`)

`"john doe"` → `"johndoe"`

#### Combined Effect

`"John Doe"`, `"john doe"`, `"JOHN  DOE"`, and `"John\u00A0Doe"` (non-breaking space) all produce the same hash. Disable individual options when exact matching is required.

### Regex Preprocessing

The `regex` parameter accepts one or more compiled `re.Pattern` objects. Each pattern is applied as `re.sub(pattern, '', value)` to all string values (dict values, list elements, nested strings) — **never to dictionary keys or object attribute names**.

Patterns are applied **before** Unicode/case/spacing normalization, so they operate on the original string content.

```python
import re

# Single pattern
phone_normalize = re.compile(r'[\s\-\(\)\+]')
make_hash({"phone": "+385 (1) 234-5678"}, regex=phone_normalize)
# phone value becomes "38512345678" before further normalization

# Multiple patterns — applied in list order
patterns = [
    re.compile(r'<[^>]+>'),      # 1. Strip HTML
    re.compile(r'\s{2,}'),        # 2. Collapse multiple spaces to empty
]
make_hash({"text": "<p>Hello   World</p>"}, regex=patterns)
# After pattern 1: "Hello   World"
# After pattern 2: "HelloWorld" (then case/spacing normalization continues)
```

**Use cases:**
- Strip HTML/XML tags from scraped content before comparison
- Remove special characters that vary between sources (`™`, `®`, `©`)
- Normalize phone numbers, registration numbers, postal codes
- Remove boilerplate text that doesn't represent actual data changes

## bulk_hash() <a id="bulk-hash"></a>

Hashes every element in a list. Defaults to `inplace=True`.

```python
from data_collector.utilities.functions.runtime import bulk_hash
```

### Arguments

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `data` | list[dict \| object \| str] | required | List of items to hash |
| `**kwargs` | | | All `make_hash()` arguments are supported |

### Example

```python
from data_collector.tables import ExampleTable
from data_collector.utilities.functions.runtime import bulk_hash

objects = [
    ExampleTable(company_id=100, name="John"),
    ExampleTable(company_id=100, name="Mary"),
]

# Hash all objects in-place (each object.sha is set)
bulk_hash(objects)

# Now each object has .sha populated
print(objects[0].sha)  # "a1b2c3..."
```

## obj_diff() <a id="obj-diff"></a>

Compares two lists of objects by a comparison key. Returns what needs to be inserted and what needs to be removed.

```python
from data_collector.utilities.functions.runtime import obj_diff
```

### Arguments

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `new_objs` | list | required | Objects from the current source |
| `existing_objs` | list | required | Objects currently in the database |
| `compare_key` | str \| list[str] \| tuple[str] | `'sha'` | Attribute(s) used as unique identifier |
| `logger` | Logger | None | Logs warnings about duplicate keys |

### Returns

```python
to_insert, to_remove = obj_diff(new_objs, existing_objs, compare_key='sha')
```

- `to_insert`: objects in `new_objs` not found in `existing_objs`
- `to_remove`: objects in `existing_objs` not found in `new_objs`

### How merge() Uses obj_diff

```python
# Simplified merge() flow:
to_insert, to_remove = obj_diff(new_objs=objs, existing_objs=db_data, compare_key='sha')
session.add_all(to_insert)                          # Insert new
for obj in to_remove: obj.archive = datetime.now()  # Archive removed
session.commit()
```

### Composite Keys

When a single attribute isn't unique enough, use composite keys:

```python
to_insert, to_remove = obj_diff(
    new_objs, existing_objs,
    compare_key=['company_id', 'person_id']
)
```

## SHAHashableMixin <a id="sha-mixin"></a>

**Source:** `data_collector/utilities/database/main.py`

A mixin for ORM models that adds automatic SHA computation on initialization.

### Features

- Adds a `sha` column (String(64), indexed)
- Computes the hash automatically on `__init__` (if `auto_sha=True`)
- Requires implementing `get_hash_keys()` in child classes

### Usage

```python
from data_collector.tables.shared import Base
from data_collector.utilities.database.main import auto_increment_column, SHAHashableMixin
from sqlalchemy import Column, String, Integer

class Company(SHAHashableMixin, Base):
    __tablename__ = 'companies'

    id = auto_increment_column()
    name = Column(String(256))
    country = Column(String(50))
    reg_number = Column(String(50))

    @staticmethod
    def get_hash_keys() -> list:
        return ['name', 'country', 'reg_number']

# SHA is computed automatically on creation:
c = Company(name="Acme Corp", country="Croatia", reg_number="123456")
print(c.sha)  # "f7a8b9c0..." — hash of name + country + reg_number
```

### Methods

| Method | Description |
|--------|-------------|
| `get_fields()` | Returns public attributes as dict (excludes `_` prefixed) |
| `get_hash_keys()` | **Must override.** Returns list of field names to include in hash |
| `compute_sha()` | Computes and returns the SHA value using `make_hash()` |

## Best Practices

1. **Choose hash keys carefully.** Use business keys (fields that identify a record from the source), not all columns. Example: for company data, hash `company_name + country + registration_number` — not `date_created` or `id`.

2. **Use `on_keys` over `exclude_keys`** when you know exactly which fields matter. It's more explicit and less prone to breaking when new columns are added.

3. **Always use `bulk_hash()` before `merge()`** unless your model uses `SHAHashableMixin` (which auto-hashes on creation).

4. **Composite `compare_key` for tables without SHA.** If a table doesn't have a `sha` column, pass `compare_key=['col1', 'col2']` to `merge()` and `obj_diff()`.

5. **Use `regex` for noisy sources.** When scraping HTML or processing documents with inconsistent formatting, use regex patterns to strip noise before hashing. This prevents false diffs caused by markup changes that don't affect the actual data content.

6. **Leave Unicode normalization enabled.** Sources encode diacritics differently (NFC vs NFD). Disabling this risks false hash mismatches for names like "Matešić" or "Müller". Use `NFKC` when sources contain compatibility characters (ligatures, fullwidth forms).

7. **Use `errors='strict'` (default) in production.** Invalid Unicode should surface as a data quality issue, not be silently dropped. Switch to `'ignore'` only for known-dirty sources where data loss is acceptable.
